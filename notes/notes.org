#+TITLE: Fast Algorithms and Integral Equation Methods
#+SUBTITLE: CS598APK
#+AUTHOR: Andreas Kloeckner
#+DATE: Fall 2024

* To-do                                                            :noexport:
** TODO Work out kernel-independent M2L in more detail
** TODO Add Krasny's barycentric-formula M2M
https://arxiv.org/abs/2012.06925, appendix B
** TODO Re-draw Gunnar's figures
* LaTeX header setup stuff                                         :noexport:

#+startup: beamer content indent

#+LATEX_CLASS: beamer
#+LaTeX_CLASS_OPTIONS: [aspectratio=149]

#+BEAMER_HEADER: \setbeamertemplate{navigation symbols}{}
#+BEAMER_HEADER: \setbeamertemplate{footline}{%
#+BEAMER_HEADER:     \raisebox{5pt}{\makebox[\paperwidth]{\hfill\makebox[20pt]{\color{gray}
#+BEAMER_HEADER:           \scriptsize\insertframenumber}}}\hspace*{5pt}}

#+BEAMER_HEADER: \usepackage{environ}
#+BEAMER_HEADER: \usepackage{tcolorbox}
#+BEAMER_HEADER: \newif\ifshowhidden
#+BEAMER_HEADER: \showhiddentrue
#+BEAMER_HEADER: \def\fillinbox#1{\begin{tcolorbox}[height=#1]\end{tcolorbox}}
#+BEAMER_HEADER: \NewEnviron{hidden}[1][1.5cm]{\ifshowhidden\begin{tcolorbox}\BODY\end{tcolorbox}\else\fillinbox{#1}\fi}

#+BEAMER_HEADER: \let\plainhref=\href
#+BEAMER_HEADER: \let\plainurl=\url
#+BEAMER_HEADER: \def\href#1#2{\plainhref{#1}{{\color{blue}\uline{#2}}}}
#+BEAMER_HEADER: \def\url#1{\href{#1}{\texttt{#1}}}

#+BEAMER_HEADER: \usepackage{pifont}
#+BEAMER_HEADER: \usepackage[normalem]{ulem}

#+BEAMER_HEADER: \def\classurl{https://relate.cs.illinois.edu/course/cs450-s19/}

#+BEAMER_HEADER: \def\activity#1{\href{\classurl/flow/#1/start}{Activity: #1}}
#+BEAMER_HEADER: \def\demonote#1{\ifshowhidden\medskip\par Demo Instructions: {\color{blue} #1}\fi}
#+BEAMER_HEADER: \newcommand{\inclass}[1]{\tmcolor{purple}{\textbf{In-class activity: }#1}}
#+BEAMER_HEADER: \newcommand{\demo}[1]{\textcolor{purple}{\textbf{Demo: }#1}}
#+BEAMER_HEADER: \newcommand{\demolink}[2]{\plainhref{https://mybinder.org/v2/gh/illinois-scicomp/cs450-s19-binder/master?filepath=#1/#2.ipynb}{\color{purple}\uline{\textbf{Demo: }#2}}}
#+BEAMER_HEADER: \newcommand{\inclasslink}[2]{\plainhref{\classurl/flow/inclass-#1/start}{\color{purple}\uline{\textbf{In-class activity: }#2}}}

#+BEAMER_HEADER: \let\tmop=\operatorname
#+BEAMER_HEADER: \let\tmtextbf=\textbf
#+BEAMER_HEADER: \let\tmtextit=\textit
#+BEAMER_HEADER: \let\tmtexttt=\texttt
#+BEAMER_HEADER: \let\tmcolor=\textcolor
#+BEAMER_HEADER: \let\tmmathbf=\mathbf
#+BEAMER_HEADER: \let\tmem=\emph
#+BEAMER_HEADER: \let\tmtt=\texttt
#+BEAMER_HEADER: \let\tmop=\operatorname
#+BEAMER_HEADER: \let\ds=\displaystyle


#+BEAMER_HEADER: \newcommand{\symball}[2]{
#+BEAMER_HEADER:   \begin{tikzpicture}[baseline=-0.7ex]
#+BEAMER_HEADER:     \shadedraw [shading=ball,ball color=#1,use as bounding box]
#+BEAMER_HEADER:       circle (1ex) node at (0.7ex,0) [minimum width=0.7ex] {};
#+BEAMER_HEADER:
#+BEAMER_HEADER:     \node [text=white,font=\bfseries] {#2};
#+BEAMER_HEADER:   \end{tikzpicture}}
#+BEAMER_HEADER: \newcommand{\plusball}{\symball{green}{{\small +}}}
#+BEAMER_HEADER: \newcommand{\okball}{\symball{orange}{o}}
#+BEAMER_HEADER: \newcommand{\minusball}{\symball{red}{-}}


# \vbar exists only because org gets grumpy if a line starts with a pipe character,
# getting confused about tables.
#+BEAMER_HEADER: \def\vbar{|}

#+BEAMER_HEADER: \let\B=\mathbf
#+BEAMER_HEADER: \let\op=\operatorname
#+BEAMER_HEADER: \newcommand{\nocomma}{}
#+BEAMER_HEADER: \newcommand{\Alpha}{A}

#+BEAMER_HEADER: \newcommand{\abs}[1]{\left| #1 \right|}
#+BEAMER_HEADER: \newcommand{\norm}[1]{\left\| #1 \right\|}
#+BEAMER_HEADER: \newcommand{\ip}[2]{\left\langle #1, #2 \right\rangle}
#+BEAMER_HEADER: \newcommand{\mathd}{\mathrm{d}}
#+BEAMER_HEADER: \newcommand{\assign}{:=}
#+BEAMER_HEADER: \newcommand{\fl}{\operatorname{fl}}

#+BEAMER_HEADER: \usepackage{tikz}
#+BEAMER_HEADER: \usetikzlibrary{calc}
#+BEAMER_HEADER: \usetikzlibrary{positioning}
#+BEAMER_HEADER: \usetikzlibrary{shapes.geometric}
#+BEAMER_HEADER: \usetikzlibrary{shapes.arrows}
#+BEAMER_HEADER: \usetikzlibrary{shapes.symbols}
#+BEAMER_HEADER: \usetikzlibrary{shadows}
#+BEAMER_HEADER: \usetikzlibrary{chains}
#+BEAMER_HEADER: \usetikzlibrary{fit}
#+BEAMER_HEADER: \usetikzlibrary{decorations}
#+BEAMER_HEADER: \usetikzlibrary{decorations.pathreplacing}
#+BEAMER_HEADER: \usetikzlibrary{3d}

#+BEAMER_HEADER: \tikzstyle{every picture}+=[remember picture]
#+BEAMER_HEADER: \pgfdeclarelayer{background}
#+BEAMER_HEADER: \pgfdeclarelayer{foreground}
#+BEAMER_HEADER: \pgfsetlayers{background,main,foreground}

#+BEAMER_HEADER: \newcommand{\cc}{\raisebox{-0.25ex}{\includegraphics[height=2ex]{cc.pdf}}}

#+BEAMER_HEADER: \AtBeginSection[] {
#+BEAMER_HEADER:   \begin{frame}[shrink]{Outline}
#+BEAMER_HEADER:     \linespread{0.8}
#+BEAMER_HEADER:     \tableofcontents[sectionstyle=show/shaded,subsectionstyle=show/show/hide]
#+BEAMER_HEADER:   \end{frame}
#+BEAMER_HEADER: }
#+BEAMER_HEADER: \AtBeginSubsection[] {
#+BEAMER_HEADER:   \begin{frame}[shrink]{Outline}
#+BEAMER_HEADER:     \linespread{0.8}
#+BEAMER_HEADER:     \tableofcontents[sectionstyle=show/shaded,subsectionstyle=show/shaded/hide]
#+BEAMER_HEADER:   \end{frame}
#+BEAMER_HEADER: }

# {{{ copied from QBX talk

#+BEAMER_HEADER: \def\intd{\,\mathrm{d}}

#+BEAMER_HEADER: \usepackage{booktabs}
#+BEAMER_HEADER: \usepackage{siunitx}
#+BEAMER_HEADER: \usepackage{colortbl}
#+BEAMER_HEADER: \usepackage{bm}
#+BEAMER_HEADER: \usepackage{algorithmic}
#+BEAMER_HEADER: \usepackage{mathtools}

#+BEAMER_HEADER: \DeclareUnicodeCharacter{2212}{-}

# {{{ from prior talks

#+BEAMER_HEADER: \colorlet{cubecolor}{gray}
#+BEAMER_HEADER: \definecolor{qbxcolor}{RGB}{43,131,186}
#+BEAMER_HEADER: \definecolor{srccolor}{RGB}{63,140,52}
#+BEAMER_HEADER: \definecolor{localcolor}{RGB}{215,25,28}

#+BEAMER_HEADER: \tikzset{%
#+BEAMER_HEADER:   mark coordinate/.style={inner sep=0pt,outer sep=0pt,minimum size=3pt,
#+BEAMER_HEADER:     fill=darkgray,circle,color=darkgray},
#+BEAMER_HEADER:   length/.style={color=darkgray}
#+BEAMER_HEADER: }

#+BEAMER_HEADER: \newcommand{\pqbx}{{p_\text{QBX}}}
#+BEAMER_HEADER: \newcommand{\pfmm}{{p_\text{FMM}}}
#+BEAMER_HEADER: \newcommand{\pquad}{{p_\text{quad}}}
#+BEAMER_HEADER: \newcommand{\nmax}{{n_{\text{max}}}}

#+BEAMER_HEADER: \newcommand{\converged}[1]{{\cellcolor[gray]{0.8}}#1}

#+BEAMER_HEADER: \newcommand{\nmpole}{n_\text{mpole}}

#+BEAMER_HEADER: \def\arxiv#1{\href{https://arxiv.org/abs/#1}{arxiv:#1}}

# }}}

# {{{ from 2019-qbx-bounds

#+BEAMER_HEADER: \newcommand{\lebesgueconst}[2]{\Lambda_{#1,#2}}
#+BEAMER_HEADER: \newcommand{\fourier}[1]{\mathcal{F}_{#1}}
#+BEAMER_HEADER: \newcommand{\bignorm}[1]{\left\lVert#1\right\rVert}
#+BEAMER_HEADER: \newtheorem{proposition}{Proposition}
#+BEAMER_HEADER: \newcommand{\proj}[1]{\mathcal{P}_{#1}}

#+BEAMER_HEADER: \definecolor{qbxcolor}{RGB}{43,131,186}
#+BEAMER_HEADER: \definecolor{srccolor}{RGB}{63,140,52}
#+BEAMER_HEADER: \definecolor{localcolor}{RGB}{215,25,28}

#+BEAMER_HEADER: \tikzset{%
#+BEAMER_HEADER:   >=latex,
#+BEAMER_HEADER:   disk/.style={draw, circle, inner sep=0},
#+BEAMER_HEADER:   mathlabel/.style={fill=lightgray,opacity=0.3,text opacity=1.0},
#+BEAMER_HEADER: }

#+BEAMER_HEADER: \newcommand{\ptpot}{\phi}
# args: source
#+BEAMER_HEADER: \newcommand{\pot}[1]{\mathcal{K}_{#1}}
# args: center, order
#+BEAMER_HEADER: \newcommand{\mpole}[2]{\mathcal{M}_{#1}^{#2}}
# args: center, order
#+BEAMER_HEADER: \newcommand{\local}[2]{\mathcal{L}_{#1}^{#2}}

# }}}

# {{{ 3D drawing primitives

# Arbitrary 3D drawing plane
# https://tex.stackexchange.com/a/353398
#+BEAMER_HEADER: \makeatletter
#+BEAMER_HEADER: \tikzoption{canvas is plane}[]{\@setOxy#1}
#+BEAMER_HEADER: \def\@setOxy O(#1,#2,#3)x(#4,#5,#6)y(#7,#8,#9)%
#+BEAMER_HEADER:   {\def\tikz@plane@origin{\pgfpointxyz{#1}{#2}{#3}}%
#+BEAMER_HEADER:    \def\tikz@plane@x{\pgfpointxyz{#4}{#5}{#6}}%
#+BEAMER_HEADER:    \def\tikz@plane@y{\pgfpointxyz{#7}{#8}{#9}}%
#+BEAMER_HEADER:    \tikz@canvas@is@plane
#+BEAMER_HEADER:   }
#+BEAMER_HEADER: \makeatother
# Args: style, ox, oy, oz, radius
#+BEAMER_HEADER: \newcommand\DrawSphere[5][black]{%
#+BEAMER_HEADER: \begin{scope}[canvas is plane={O(#2,#3,#4)x(#2+#5,#3,#4)y(#2,#3+#5,#4)}]
#+BEAMER_HEADER:   \draw[#1] (0,0) circle (1);
#+BEAMER_HEADER: \end{scope}
#+BEAMER_HEADER: \begin{scope}[canvas is plane={O(#2,#3,#4)x(#2,#3,#4+#5)y(#2,#3+#5,#4)}]
#+BEAMER_HEADER:   \draw[#1] (0,-1) arc (270:270+180:1);
#+BEAMER_HEADER:   \draw[#1,dotted] (0,1) arc (90:270:1);
#+BEAMER_HEADER: \end{scope}
#+BEAMER_HEADER: \begin{scope}[canvas is plane={O(#2,#3,#4)x(#2+#5,#3,#4)y(#2,#3,#4+#5)}]
#+BEAMER_HEADER:   \draw[#1] (1,0) arc (0:180:1);
#+BEAMER_HEADER:   \draw[#1,dotted] (-1,0) arc (180:360:1);
#+BEAMER_HEADER: \end{scope}
# longitudes at +/- 45 degrees
#+BEAMER_HEADER: \begin{scope}[canvas is plane={O(#2,#3,#4)x(#2+#5/sqrt 2,#3,#4+#5/sqrt 2)y(#2,#3+#5,#4)}]
#+BEAMER_HEADER:   \draw[#1] (0,-1) arc (270:270+180:1);
#+BEAMER_HEADER:   \draw[#1,dotted] (0,1) arc (90:270:1);
#+BEAMER_HEADER: \end{scope}
#+BEAMER_HEADER: \begin{scope}[canvas is plane={O(#2,#3,#4)x(#2-#5/sqrt 2,#3,#4+#5/sqrt 2)y(#2,#3+#5,#4)}]
#+BEAMER_HEADER:   \draw[#1] (0,-1) arc (270:270+180:1);
#+BEAMER_HEADER:   \draw[#1,dotted] (0,1) arc (90:270:1);
#+BEAMER_HEADER: \end{scope}
#+BEAMER_HEADER: }

# Args: style, ox, oy, oz, radius
#+BEAMER_HEADER: \newcommand\DrawCube[5][black]{%
#+BEAMER_HEADER: \begin{scope}[canvas is plane={O(#2,#3,#4+#5)x(#2+#5,#3,#4+#5)y(#2,#3+#5,#4+#5)}]
#+BEAMER_HEADER:   \draw[#1] (-1,-1) rectangle (1,1);
#+BEAMER_HEADER: \end{scope}
#+BEAMER_HEADER: \begin{scope}[canvas is plane={O(#2,#3,#4-#5)x(#2+#5,#3,#4-#5)y(#2,#3+#5,#4-#5)}]
#+BEAMER_HEADER:   \draw[#1] (-1,1) -- (1,1);
#+BEAMER_HEADER:   \draw[#1,dotted] (-1,-1) -- (1,-1);
#+BEAMER_HEADER: \end{scope}
#+BEAMER_HEADER: \begin{scope}[canvas is plane={O(#2+#5,#3,#4)x(#2+#5,#3,#4+#5)y(#2+#5,#3+#5,#4)}]
#+BEAMER_HEADER:   \draw[#1] (-1,-1) rectangle (1,1);
#+BEAMER_HEADER: \end{scope}
#+BEAMER_HEADER: \begin{scope}[canvas is plane={O(#2-#5,#3,#4)x(#2-#5,#3,#4+#5)y(#2-#5,#3+#5,#4)}]
#+BEAMER_HEADER:   \draw[#1] (-1,1) -- (1,1);
#+BEAMER_HEADER:   \draw[#1,dotted] (-1,-1) -- (1,-1);
#+BEAMER_HEADER:   \draw[#1,dotted] (-1,-1) -- (-1,1);
#+BEAMER_HEADER: \end{scope}
#+BEAMER_HEADER: }

# }}}

# }}}

#+BEAMER_HEADER: \usecolortheme{orchid}

#+LATEX_COMPILER: pdflatex
#+OPTIONS: H:3 toc:nil ':t tasks:t
#+BEAMER_THEME: default
#+COLUMNS: %45ITEM %10BEAMER_ENV(Env) %10BEAMER_ACT(Act) %4BEAMER_COL(Col) %8BEAMER_OPT(Opt)

* Introduction
  :PROPERTIES:
  :RELATE_TREE_SECTION_NAME: intro
  :RELATE_TREE_SECTION_OPENED: true
  :END:
** Notes
  :PROPERTIES:
  :RELATE_TREE_ICON: bi bi-book
  :RELATE_TREE_LINK: https://andreask.cs.illinois.edu/cs598apk-f24/notes.pdf
  :END:

** Notes (unfilled, with empty boxes)
  :PROPERTIES:
  :RELATE_TREE_ICON: bi bi-book
  :RELATE_TREE_LINK: https://andreask.cs.illinois.edu/cs598apk-f24/notes-folded.pdf
  :END:

*** What's the point of this class?

- Starting point: Large-scale scientific computing
- Many popular numerical algorithms: \(O (n^\alpha )\) for \(\alpha > 1\)

  (Think Matvec, Matmat, Gaussian Elimination, LU, ...)

- Build a set of tools that lets you cheat: Keep \(\alpha\) small

  (Generally: probably not--Special purpose: possible!)

- Final goal: Extend this technology to yield PDE solvers
- But: Technology applies in many other situations

  - Many-body simulation
  - Stochastic Modeling
  - Image Processing
  - `Data Science' (e.g. Graph Problems)

- This is class is about an even mix of math and computation

*** Survey

- Home dept
- Degree pursued
- Longest program ever written
  - in Python?
- Research area
- Interest in PDE solvers

*** Class web page

#+BEGIN_CENTER
[[https://bit.ly/fastalg-f24]]
#+END_CENTER

contains:

- Class outline
- Assignments
- Piazza
- Grading
- Video

*** Why study this at all?

- Finite difference/element methods are inherently
  - ill-conditioned
  - tricky to get high accuracy with

- Build up a toolset that does \emph{not} have these flaws
- Plus: An interesting/different analytical and computational point of
  view

  - If you're not going to use it to solve PDEs, it (or the ideas behind
    it) will still help you gain insight.

*** FD/FEM: Issues

Idea of these methods:

1. Take differential equations
2. Discretize derivatives
3. Make linear system
4. Solve

So what's wrong with doing that?

*** Discretizing Derivatives: Issues?
#+LATEX: \begin{hidden}[6cm]

\begin{columns}
\column{0.5\textwidth}
Differentiation is `unbounded'. Example:
\[(e^{i \alpha x})' = i \alpha e^{i \alpha x} \]
\column{0.5\textwidth}
\includegraphics[width=\textwidth]{media/diff-complex-exp.pdf}
\end{columns}

So a `small' function can become an arbitrarily big function.
Does that matter?

- \(\kappa (A) = \norm A \norm{ A^{- 1} } \), and this increases \(\norm{ A } \).

- Also, \(\norm{ A^{- 1} }\) doesn't exist for derivatives.
#+LATEX: \end{hidden}

*** Discretizing Derivatives: Issues?

*Result:* The better we discretize (the more points we use), the
worse the condition number gets.

\demo{Conditioning of Derivative Matrices}

*To be fair:* Multigrid works around that (by judiciously using *fewer* points!)

But there's another issue that's not fixable.

#+LATEX: \begin{hidden}[1.5cm]
Inherent tradeoff: FP accuracy \(\leftrightarrow\) Truncation error

\demo{Floating point vs Finite Differences}
#+LATEX: \end{hidden}

*Q:* Are these problems real?

#+LATEX: \begin{hidden}[1.5cm]
\(\rightarrow\) Try solving 3D Poisson with just FEM+CG.
#+LATEX: \end{hidden}

So this class is about starting fresh with methods that (rigorously!) don't
have these flaws!

*** Bonus Advertising Goodie

Both multigrid and fast/IE schemes ultimately are \(O (N)\) in the
number of degrees of freedom \(N\).
#+LATEX: \begin{hidden}[6cm]
But:

#+ATTR_LATEX: :height 4cm
[[./media/surface-vol-dofs.pdf]]

The number \(N\) is different! (And it's smaller for IEs.)

(Truth in advertising: Only for homogeneous problems.)
#+LATEX: \end{hidden}

*** Open Source <3
    
These notes (and the accompanying demos) are open-source!

\bigskip
Bug reports and pull requests welcome: https://github.com/inducer/fast-alg-ie-notes

\bigskip
Copyright (C) 2013 -- 24 Andreas Kloeckner

\bigskip
\scriptsize
Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

\medskip
The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

\medskip
THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.

* Dense Matrices and Computation
  :PROPERTIES:
  :RELATE_TREE_SECTION_NAME: dense_compute
  :RELATE_TREE_SECTION_OPENED: true
  :END:

*** Matvec: A Slow Algorithm

Matrix-vector multiplication: our first `slow' algorithm.

\(O (N^2)\) complexity.
\[\beta _i = \sum _{j = 1}^N A_{i j} \alpha _j \]
Assume \(A\) dense.

*** Matrices and Point Interactions

\[A_{i j} = G (x_i, y_j) \]
Does that actually change anything?
#+LATEX: \begin{hidden}[6cm]
\[\psi (x_i) = \sum _{j =
   1}^N G (x_i, y_j) \varphi (y_j) \]

Technically: *no difference*.

Can *translate back and forth* between both views.

But: Gain *terminology* and *intuition*:

- \(x_i\): ``Targets''

- \(y_i\): ``Sources''

- \(G\): ``Kernel''
#+LATEX: \end{hidden}

*** Matrices and Point Interactions

\[A_{i j} = G (x_i, y_j) \]
Graphically, too:
#+LATEX: \begin{hidden}[6cm]

#+ATTR_LATEX: :height 4cm
[[./media/point-interactions.pdf]]

Each arrow corresponds to a matrix entry.
#+LATEX: \end{hidden}

*** Matrices and point Interactions

\[\psi (x_i) = \sum _{j = 1}^N G (x_i, y_j) \varphi (y_j) \]
This /feels/ different.
#+LATEX: \begin{hidden}[4cm]
It's supposed to!

\(G (x, y)\) defined for all \(x \in \mathbb{R}^3 ?\) *Possibly!* (Maybe
also all \(y \in \mathbb{R}^3\)?)

In former case: \(\psi\) defined everywhere. (``Matrix infinitely tall'')

#+LATEX: \end{hidden}

*Q:* Are there enough matrices that come from globally defined \(G\)
to make this worth studying?

*** Point Interaction Matrices: Examples (I)

#+LATEX: \begin{hidden}[7cm]

- (Lagrange) Interpolation:
  \(\ds \psi (x) = \sum _{j = 1}^N \ell _j (x) \varphi (y_j) \)

  - (\(G (x, y_j) = \ell _j (x)\))
  - Also: Interpolation Error!
- Numerical Differentiation:
  \(\ds\psi (x) = \sum _{j = 1}^N \ell _j' (x) \varphi (y_j) \)
- Numerical Integration:
  \(\ds\psi (x) = \sum _{j = 1}^N \int _a^x \ell _j (\xi ) d \xi \varphi (y_j) \)
- Equivalents of the above for other bases: e.g. Fourier
#+LATEX: \end{hidden}

*** Point Interaction Matrices: Examples (II)
#+LATEX: \begin{hidden}[7cm]
- Potential Evaluation: Potential of an electron at the origin in 3D?
  \[U (x) = \frac{q_{\tmop{el}}}{4 \pi } \cdot \frac{1}{| x |} \]

  Potential of an electron at \(y\) in 3D?
  \(\ds U_y (x) = C \cdot \frac{1}{| x - y |   } \)

  Potential of a number of electrons at a \(y_1, \ldots , y_N\)?
  \[U (x) = \sum _{j = 1}^N \frac{1}{| x - y_j | } \varphi (y_j) \]
  You might feel like that sum wants to be an integral, to make things `fair'
  between sources and targets. Hold on to that feeling.
#+LATEX: \end{hidden}

*** Point Interaction Matrices: Examples (III)
#+LATEX: \begin{hidden}[6cm]

- Convolutions:
  \[\psi (x) = \sum _{j = 1}^N G (x - y_j) \varphi (y_j) \]
  *Quiz:* What do these do, visually?

  \smallskip
  *Notice:* Potential evaluation is actually an \emph{example}
  of convolution.

  \smallskip
  Once again, infinitely many sources is a possibility--just make the sum an
  integral.
#+LATEX: \end{hidden}

So yes, there are indeed lots of these things.

*** Integral Operators

Why did we go through the trouble of rephrasing matvecs as
\[\psi (x_i) = \sum _{j = 1}^N G (x_i, y_j) \varphi (y_j) ? \]
#+LATEX: \begin{hidden}[5cm]

- We're headed towards \emph{Integral Operators} (or `Integral
  Transforms') that look like this:
  \[\psi (x) = \int _\Omega  G (x, y) \varphi (y) \tmop{dy} \]
- We'll rely on \(\psi\) being defined everywhere to derive some important
  properties that we can't `see' if there are only finitely many targets.
#+LATEX: \end{hidden}

*** Cheaper Matvecs

\[\psi (x_i) = \sum _{j = 1}^N G (x_i, y_j) \varphi (y_j) \]
So what can we do to make evaluating this cheaper?
#+LATEX: \begin{hidden}[5cm]

- *Idea 1:* Make sure \(G\) evaluates to mostly zeros.

  (i.e. make it sparse) \(\rightarrow\) FEM/FD approach

  \smallskip
  /How?/ Limit `domain of influence' of each source, e.g. by using
  piecewise interpolation.

  \smallskip
  This is /not/ the approach in this class though.

- *Idea 2:* If the matrix is /very/ special (e.g.
  Toeplitz/circulant) or a DFT matrix, \(O (n \log n)\) FFTs help

- *Idea 3:* If the matrix has low rank
#+LATEX: \end{hidden}

*** Fast Dense Matvecs

Consider
\[A_{i j} = u_i v_j, \]
let \(\tmmathbf{u} = (u_i)\) and \(\tmmathbf{v} = (v_j)\).

Can we compute \(A \tmmathbf{x}\) quickly? (for a vector
\(\tmmathbf{x}\))
#+LATEX: \begin{hidden}[5cm]
\[A = \tmmathbf{u} \tmmathbf{v}^T, \text{so} \]
\[A \tmmathbf{x} = (\tmmathbf{u} \tmmathbf{v}^T) \tmmathbf{x} = \tmmathbf{u}
   (\tmmathbf{v}^T \tmmathbf{x}) \]
Cost: \(O (N)\).

*Q:* What is the \emph{row rank} of \(A\)? (#of lin.indep. rows)

*Q:* What is the \emph{column rank} of \(A\)? (#of lin.indep. columns)

*Remark:* Row and column rank are always equal, not just here.
#+LATEX: \end{hidden}

*** Fast Dense Matvecs (II)
\[A = \tmmathbf{u}_1 \tmmathbf{v}_1^T + \cdots + \tmmathbf{u}_K
   \tmmathbf{v}_K^T \]
Does this generalize? What is \(K\) here?

#+LATEX: \begin{hidden}[4cm]
- $k = \operatorname{rank} A$
- Sure does generalize. Cost: \(O (NK)\)
- What if matrix has `full' rank? Cost back to \(O (N^2)\)
#+LATEX: \end{hidden}

*** Low-Rank Point Interaction Matrices

Usable with low-rank complexity reduction?
\[\psi (x_i) = \sum _{j = 1}^N G (x_i, y_j) \varphi (y_j) \]

#+LATEX: \begin{hidden}[5cm]
\[\psi (x_i) = \sum _{j = 1}^N \underbrace{G_1 (x_i) G_2
   (y_j)}_{G (x_i, y_j)} \varphi (y_j) \]


- /Separation of variables/
- *Q:* Did any of our examples look like this? Nope.

  $\rightarrow$ Check computationally.

\demo{Rank of a Potential Evaluation Matrix (Attempt 1)}

- So it looks like the rank *does* decay, approximately
- Echelon form: good idea?
#+LATEX: \end{hidden}

*** Numerical Rank

What would a /numerical/ generalization of `rank' look like?
#+LATEX: \begin{hidden}[6cm]
First, what does \emph{exact} rank mean?
\[A = UV, \]
with \(U \in \mathbb{R}^{m \times k}\), \(V \in \mathbb{R}^{k \times n}\).

*Idea:* Let's loosen that definition to a precision \(\varepsilon\).

If \(A \in \mathbb{R}^{m \times n}\):
\[\tmop{numrank} (A, \varepsilon ) = \min \{k : \exists U \in \mathbb{R}^{m
   \times k}, V \in \mathbb{R}^{k \times n} : | A - UV |_2 \leqslant
   \varepsilon \}.   \]
*Q:* That's great, but how do we find those matrices?
#+LATEX: \end{hidden}

*** Eckart-Young-Mirsky Theorem

#+LATEX: \begin{theorem}[Eckart-Young-Mirsky]
 SVD \(A = U \Sigma V^T\). If \(k < r = \tmop{rank} (A)\)
  and
  \[A_k = \sum _{i = 1}^k \sigma _i u_i v_i^T, \]
  then
  \[\min _{\tmop{rank} (B) = k} | A - B |_2 = | A - A_k |_2 = \sigma _{k + 1} . \]
#+LATEX: \end{theorem}

*Q:* What's that error in the Frobenius norm?

So in principle that's good news:

- We can find the numerical rank.
- We can also find a factorization that reveals that rank (!)

\demo{Rank of a Potential Evaluation Matrix (Attempt 2)}

*** Constructing a tool

There is still a slight downside, though.
#+LATEX: \begin{hidden}[6cm]
Suppose we wanted to use this to make the matvec cheaper.

\smallskip
That wouldn't quite work:

\smallskip
We would need to build the entire matrix (\(O (N^2)\)), factorize it
(\(O (N^3)\)), and then apply the low-rank-approximation (\(O
(N)\)).

\smallskip
So we would need to make the /factorization/ cheaper as well.

\smallskip
*Big Q:* Possible?
#+LATEX: \end{hidden}

*** Representation

What does all this have to do with (right-)preconditioning?
#+LATEX: \begin{hidden}[6cm]
Idea behind Right Preconditioning:
Instead of
\[Ax = b \]
solve
\[AMy = b, \]
then find \(x = My\).

What this does is change the \emph{meaning} of the degrees of freedom in
the linear system.

You could say: We change how we \emph{represent} the solution.

#+LATEX: \end{hidden}

*** Representation (in context)
#+LATEX: \begin{hidden}[6cm]
Connection with what we've been doing:

- Assume \(Ax = b\) is a big, bad problem
- Assume we can apply \(M\) cheaply

  (with the help of low-rank machinery)

- Or, even better, apply all of \(AM\) cheaply
- Assume \(M\) is tall and skinny
- Then solving \(AMy = b\) is as good as solving \(Ax = b\), but (ideally)
  lots cheaper
- These `point interaction' matrices we've been discussing are the
  prototypes of such \(M\) matrices

  (Go from few points to all of \(\mathbb{R}^3\))
#+LATEX: \end{hidden}

* Tools for Low-Rank Linear Algebra
  :PROPERTIES:
  :RELATE_TREE_SECTION_NAME: low_rank
  :RELATE_TREE_SECTION_OPENED: true
  :END:

** Low-Rank Approximation: Basics
*** Rephrasing Low-Rank Approximations

SVD answers low-rank-approximation (`LRA') question. But: too
expensive. First, rephrase the LRA problem:
#+LATEX: \begin{hidden}[6cm]
Instead of the /factorization form/ \(A \approx BC^T \),

we will ask for the /projection form/ of LRA: \(A \approx QQ^T A, \)
i.e. \(A\) being approximated by an orthogonal projection of its columns.

(\(Q\) has orthogonal columns, i.e. \(Q^T Q = I\), and fewer than \(A\))

\medskip
Call the columns of \(Q\) the /LRA basis/.

\medskip
If we have the projection form, can we find the factorization form?

Sure: Set \(B = Q\) and \(C = Q^T A\).
#+LATEX: \end{hidden}

*** Using LRA bases

If we have an LRA basis \(Q\), can we compute an SVD?
#+LATEX: \begin{hidden}[6cm]
1. \(B = Q^T A\)
2. Compute an SVD of \(B\): \(B = \bar {U} \Sigma V^T\)
3. Set \(U = Q \bar {U}\)

Then: \(A \approx QQ^T A = Q \bar {U} \Sigma V^T = U \Sigma V^T . \)
Cost:

- Assume \(A\) is \(N \times N\), \(Q\) has \(k\) columns.
- Step 1: \(kN^2\)
- Step 2: \(k^2 N\)
- Step 3: \(k^2 N\)

Can we hope to do better overall? (#entries?)
#+LATEX: \end{hidden}

*** Finding an LRA basis

How would we /find/ an LRA basis?
#+LATEX: \begin{hidden}[6cm]
Goal: Find \(Q\) columns so that
\[| A - QQ^T A |_2 \leqslant \varepsilon .   \]
*Question:* Do we know the number of columns \(k\) of \(Q\) ahead of
time?

- Yes: /`Fixed-rank approximation'/

- No: /`Adaptive LRA'/

*Idea 1:* SVD \(\rightarrow\) First \(k\) columns of \(U\) in \(A = U \Sigma  V^T\) provide the \emph{optimal answer}.

\(\rightarrow\) You've got to be joking.

\(\rightarrow\) Give up optimality, allow `slack' in accuracy and column
count.\medskip

*Idea 2:* Use a randomized algorithm, based on the same intuition as
the power method.
#+LATEX: \end{hidden}

*** Giving up optimality

What problem should we actually solve then?
#+LATEX: \begin{hidden}[6cm]
Instead
of
\[| A - QQ^T A |_2 = \min _{\tmop{rank} (X) \leqslant k} | A - X |_2 =
   \sigma _{k + 1} \]
with \(Q\) having \(k\) columns,\medskip

we'll only go for
\[| A - QQ^T A |_2 \approx \min _{\tmop{rank} (X) \leqslant k} | A - X |_2 \]
with \(Q\) having \(k + p\) columns.
#+LATEX: \end{hidden}

*** Recap: The Power Method

How did the power method work again?
#+LATEX: \begin{hidden}[6cm]
\(A\) square,
eigenvalues
\[| \lambda _1 | \geqslant | \lambda _2 | \geqslant \cdots \geqslant |
   \lambda _n | \geqslant 0.   \]
with eigenvectors \(\tmmathbf{v}_i\).

\medskip
*Goal:* Find eigenvector to largest (by-magnitude) eigenvalue.

Start with random vector \(\tmmathbf{x}\):
\(\tmmathbf{x} = \alpha _1 \tmmathbf{v}_1 + \cdots + \alpha _n \tmmathbf{v}_n .
\)

Then
\(A \tmmathbf{x} = \alpha _1 \lambda _1 \tmmathbf{v}_1 + \cdots + \alpha _n
   \lambda _n \tmmathbf{v}_n . \)

*Important observation:* Matvecs with random vectors `kill' the
`unimportant' bits of the range.
#+LATEX: \end{hidden}

*** How do we construct the LRA basis?

Put randomness to work:
#+LATEX: \begin{hidden}[6cm]
Design a /randomized
range finder/:

1. Draw an \(n \times \ell\) /Gaussian/ (iid) random matrix \(\Omega\)

1. \(Y = A \Omega\)

1. Orthogonalize columns of \(Y\), e.g. by QR factorization:
  \[Y = \tmop{QR} \]
  \(\rightarrow\) \(Q\) has \(\ell\) orthogonal columns
#+LATEX: \end{hidden}

*** Tweaking the Range Finder (I)
Can we accelerate convergence?

#+LATEX: \begin{hidden}[6cm]

*Possible tweak:* Kill the unimportant bits of the range faster, by
inserting a few iterations of the power method into Step 2:
\[Y = (AA^T)^q A \Omega . \]
*Q:* Why multiply by \((AA^T)\) and not just \(A\)?

\(\rightarrow\)Retains singular vectors!
\[AA^T A = (U \Sigma V^T) (V \Sigma U^T) (U \Sigma V^T) . \]
But: singular values decay much faster:
\[\sigma _i (AA^T A) = \sigma _i (A)^3 ! \]
#+LATEX: \end{hidden}

*** Tweaking the Range Finder (II)
What is one possible issue with the power method?

#+LATEX: \begin{hidden}[5cm]
- Overflow/FP problems

- Normalization, orthogonalization

- If FP is a concern, apply QR after every application of \(A\) or \(A^T\).

#+LATEX: \end{hidden}

*** Even Faster Matvecs for Range Finding

Assumptions on \(\Omega\) are pretty weak--can use more or less
anything we want.

\(\rightarrow\) Make it so that we can apply the matvec \(A \Omega\) in \(O (n \log \ell )\) time.

How? Pick \(\Omega\) as a carefully-chosen subsampling of the Fourier transform.

** Low-Rank Approximation: Error Control
*** Errors in Random Approximations

If we use the randomized range finder, how close do we get to the optimal answer?
#+LATEX: \begin{theorem}
For an \(m \times n\) matrix \(A\), a target rank \(k \geqslant 2\) and an
  oversampling parameter \(p \geqslant 2\) with \(k + p \leqslant \min (m, n)\),
  with probability \(1 - 6 \cdot p^{- p}\),
  \[\left | A - QQ^T A \left |_2 \leqslant \left ( 1 + 11 \sqrt{k + p}
     \sqrt{\min (m, n)} \right ) \sigma _{k + 1} . \right . \right . \]
  (given a few more very mild assumptions on \(p\))
#+LATEX: \end{theorem}

[Halko/Tropp/Martinsson `10, 10.3]\medskip

*Message:* We can \emph{probably} (!) get away with oversampling
parameters as small as \(p = 5\).\medskip

*** A-posteriori and Adaptivity

The result on the previous slide was /a-priori/. Once we're
done, can we find out `how well it turned out'?
#+LATEX: \begin{hidden}[5cm]
Sure: Just consider the error:
\[A - QQ^T A \]
Realize that what this does is instead of projecting onto the columns of \(Q\),
it projects onto their orthogonal complement:
\[E = (I - QQ^T) A \]
*Idea:* Use a randomized technique as well.

- We are interested in \(| E |_2 = \sigma _1 (E)  \)

- If the previous techniques work,

  \(| E \tmmathbf{\omega } |_2  \) for a randomly drawn
  Gaussian vector \(\tmmathbf{\omega }\) should give us a pretty good idea of \(|     E |_2  \).
#+LATEX: \end{hidden}

*** Adaptive Range Finding: Algorithm

#+LATEX: \begin{hidden}[6cm]
- Compute small-ish fixed rank LRA

- Check error

- Too big? Throw in a few more vectors, repeat

Next, realize that the error estimator relies on the same thing as the range
finder, multiplication by random vector: Not hard to modify algorithm to make
both use the same data!\medskip

\demo{Randomized SVD}
#+LATEX: \end{hidden}

** Reducing Complexity
*** Rank-revealing/pivoted QR

Sometimes the SVD is too /good/ (aka expensive)--we may need less
accuracy/weaker promises, for a significant decrease in cost.
#+LATEX: \begin{hidden}[6cm]
This is where /RRQR/ or /pivoted QR/ comes in.

For \(A \in \mathbb{R}^{m \times n}\),
\[A \Pi = \tmop{QR} = Q
\begin{bmatrix}
  R_{11} & R_{12}\\
  & R_{22}
\end{bmatrix}, \]
where

- \(R_{11} \in \mathbb{R}^{k \times k}\),

- \(| R_{22} |_2  \) is (hopefully) `small'.

- \(Q \in \mathbb{R}^{m \times n}\) with \(Q^T Q = I\)

- It is possible to skip computing the bottom half of \(R\)

  (and the corresponding bits of \(Q\))

- \(\Pi\) is an \(n \times n\) (column) permutation matrix
#+LATEX: \end{hidden}

*** Using RRQR for LRA

#+LATEX: \begin{hidden}[6cm]
Given a RRQR factorization, we know

- \(\sigma _{k + 1} \leqslant | R_{22} |_2  \) (i.e. it
  can't do better than an SVD)

- To precision \(| R_{22} |_2  \), \(A\) has
  \emph{at most} numerical rank \(k\).

(see e.g. Golub and Van Loan, ch. 5)\medskip

\demo{Rank-revealing QR}\medskip

Stop and think:

- RRQR delivers essentially the same service as what we've been
  developing: Find an orthogonal basis of the range.

- But: an \(O (N^3)\) factorization.
#+LATEX: \end{hidden}

*** Interpolative Decomposition (ID): Definition

Would be helpful to know /columns of \(A\)/ that contribute `the most' to the rank.

(orthogonal transformation like in QR 'muddies the waters')
#+LATEX: \begin{hidden}[5cm]
For a rank-\(k\) matrix \(A\), the /Interpolative
Decomposition/ provides this:
\[A_{m \times n} = A_{(:, J)} P_{k \times n}, \]
where

- \(J\) is an index set of length \(k\) representing column selection,

- \(k\) columns of \(P\) contain only a single entry of \(1\), and

- \(P\) is well-conditioned.

  In particular, the magnitude of its entries is bounded by 2.

#+LATEX: \end{hidden}

*** ID: Computation

*How* do we construct this (from RRQR): (short/fat case)

#+LATEX: \begin{hidden}
\(A \Pi = Q \begin{bmatrix} R_{11} & R_{12} \end{bmatrix} \)
Set \(B = QR_{11} =(A\Pi)_{(:,J)}\).
#+LATEX: \end{hidden}

*Q:* What is \(P\), in terms of the RRQR?
#+LATEX: \begin{hidden}[4cm]
Next, set
\(P = \begin{bmatrix} \tmop{Id} & R_{11}^{- 1} R_{12} \end{bmatrix} \Pi ^T \),
then
\begin{eqnarray*}
  BP & = & QR_{11}
  \begin{bmatrix}
    \tmop{Id} & R_{11}^{- 1} R_{12}
  \end{bmatrix} \Pi ^T\\
  & = & Q
  \begin{bmatrix}
    R_{11} & R_{12}
  \end{bmatrix} \Pi ^T\\
  BP \Pi & = & Q
  \begin{bmatrix}
    R_{11} & R_{12}
  \end{bmatrix}\\
  A \Pi & = & Q
  \begin{bmatrix}
    R_{11} & R_{12}
  \end{bmatrix}.
\end{eqnarray*}

#+LATEX: \end{hidden}

*** ID $Q$ vs ID $A$

What does row selection mean for the LRA?
#+LATEX: \begin{hidden}[5cm]
Starting point: At end of stage 1, have LRA \(A \approx QQ^T A. \)

Run an ID on the /rows/ (i.e. a /transpose ID/) of \(Q\):
\(Q \approx PQ_{(J, :)} \)

(Recall: \(Q\) is tall and skinny. \(Q_{(J, :)}\) is a square subset.)

\[A \approx PQ_{(J, :)} Q^T A. \]
Now consider:
\(A_{(J, :)} \approx \underbrace{P_{(J, :)}}_{\tmop{Id}} Q_{(J, :)} Q^T A \)

So \(PA_{(J, :)} \approx P Q_{(J, :)} Q^T A  \approx A\).

I.e. \(P\) for \(Q\) and \(A\) are essentially interchangeable!
#+LATEX: \end{hidden}

[Martinsson, Rokhlin, Tygert `06]

*** ID: Remarks

Slight tradeoff here: what?
#+LATEX: \begin{hidden}
Accuracy (two $\approx$ on previous slide) vs. expense
#+LATEX: \end{hidden}

How would we use the ID in the context of the range finder?
#+LATEX: \begin{hidden}
- Can simply use ID on the sample matrix \(Y\)
- Because of (essentially) the same argument, \(P\) made from \(Y\)
  should transfer to \(A\).
#+LATEX: \end{hidden}

\demo{Interpolative Decomposition}

*** What does the ID buy us?

Name a property that the ID has over other factorizations.
#+LATEX: \begin{hidden}

It preserves (a subset of) matrix entries exactly.

Copmosition with other transforms without (expensive!) matmats.
#+LATEX: \end{hidden}

All our randomized tools have two stages:

1. Find ONB of approximate range

1. Do actual work only on approximate range

Complexity?

#+LATEX: \begin{hidden}
First step of this: \(C = Q^T A\) \(\rightarrow\) \(O (N^2 k)\).

For now, both stages are \(O (N^2 k)\).
#+LATEX: \end{hidden}

What is the impact of the ID?

#+LATEX: \begin{hidden}
- We avoid the need to form/work with \(C = Q^T A\).
- Row subset \(A_{(J,:)}\) assumes role of \(Q^T A\)
#+LATEX: \end{hidden}

*** Leveraging the ID for SVD (I)

Build a low-rank SVD with row extraction.
#+LATEX: \begin{hidden}[6cm]

1. Obtain the row subset \(J\) and upsampler \(\underset{N \times k}{P}\).

  (via \(Q\) or directly from \(Y\))

1. Compute row QR of remaining rows:
  \[\underset{N \times k}{(A_{(J, :)})^T} = \underset{N \times k}{\bar {Q}}
       \underset{k \times k}{\bar {R}} \]
1. Upsample the row coefficients \(\bar {R}^T\):
  \[\underset{N \times k}{Z} = \underset{N \times k}{P} \underset{k \times
       k}{\bar {R}^T} \]
1. SVD the result:
  \[Z = U \Sigma \tilde {V}^T \]
#+LATEX: \end{hidden}

*** Leveraging the ID for SVD (II)

In what way does this give us an SVD of \(A\)?
#+LATEX: \begin{hidden}[5cm]
\begin{eqnarray*}
  &  & \underset{N \times k}{U}  \underset{k \times k}{\Sigma } \left (
      \underset{N \times k}{\bar {Q}}  \underset{k \times k}{\tilde {V}}
      \right )^T\\
  & = & U \Sigma \tilde {V}^T \bar {Q} ^T\\
  & = & Z \bar {Q}^T\\
  & = & P \bar {R}^T \bar {Q}^T\\
  & = & PA_{(J, :)}\\
  & \approx & A.
\end{eqnarray*}
#+LATEX: \end{hidden}

*** Leveraging the ID for SVD (III)

*Q:* Why did we need to do the row QR?
#+LATEX: \begin{hidden}[5cm]
Because otherwise we wouldn't have gotten a `real' SVD:

\begin{eqnarray*}
  A_{(J, :)} & = & U \Sigma V^T\\
  PA_{(J, :)} & = & \underbrace{PU}_{\tmop{orth} ?} \Sigma V^T
\end{eqnarray*}
\(\rightarrow\) So `hide' \(P\) in matrix being SVD'd--but: can't do full
reconstruction. Use \((\tmop{small}) R\) in QR as a proxy!\medskip

*Cost:* Finally \(O (Nk^2)\)!\medskip

Putting all this together in one code: \(\rightarrow\) HW exercise :)
#+LATEX: \end{hidden}

*** TODO Perspectives on Low-Rank Approximation

- [[https://arxiv.org/abs/2009.11392][Generalized Nyström]]
- [[https://arxiv.org/abs/1908.06059][CUR]]

*** Where are we now?

- We have observed that we can make matvecs faster if the matrix has
  low-ish numerical rank

- In particular, it seems as though if a matrix has low rank, there is
  no end to the shenanigans we can play.

- We have observed that some matrices we are interested in (in some
  cases) have low numerical rank (cf. the point potential example)

- We have developed a toolset that lets us obtain LRAs and do useful work
  (using SVD as a proxy for "useful work") in \(O(N\cdot K^\alpha)\) time
  (assuming availability of a cheap matvec).

*Next stop:* Get some insight into \emph{why} these matrices have
low rank in the first place, to perhaps help improve our machinery even
further.

* Rank and Smoothness
  :PROPERTIES:
  :RELATE_TREE_SECTION_NAME: rank_smoothness
  :RELATE_TREE_SECTION_OPENED: true
  :END:

** Local Expansions
*** Punchline

What do (numerical) rank and smoothness have to do with each other?
#+LATEX: \begin{hidden}[4cm]

If the result of a (continuous) operation is smooth, its result can be
represented with a short expansion in a function basis.

Types of basis:

- Polynomials (orthogonal, or monomials if you must),
- Sines/Cosines,
- Eigenfunctions of Sturm-Liouville operators, ...
- It mostly doesn't matter.
#+LATEX: \end{hidden}

Even shorter punchline?

#+LATEX: \begin{hidden}
Smooth functions are boring. (But useful!)
#+LATEX: \end{hidden}

*** Smoothing Operators

If the operations you are considering are \emph{smoothing}, you can expect
to get a lot of mileage out of low-rank machinery.\medskip

What types of operations are smoothing?

#+LATEX: \begin{hidden}[3.5cm]

- *Derivatives*: nope. Make a function `rougher'.

  (Consider the idea of a function `having \(n\) derivatives' as a measure of
  how smooth it is, i.e. the \(C^n\) function spaces.)

- *Integrals:* yep.

This provides a good computational justification to try and use integral
operators as a tool to construct numerical methods.

#+LATEX: \end{hidden}

Now: Consider some examples of smoothness, with justification.

\smallskip
How do we judge smoothness?
#+LATEX: \begin{hidden}
Decay of Taylor remainders.
#+LATEX: \end{hidden}

*** Recap: Multivariate Taylor

#+LATEX: \begin{hidden}[6cm]
*1D Taylor:*
\(\ds f (c + h) \approx \sum _{p = 0}^k \frac{f^{(p)} (c)}{p!} h^p \)

*Notational tool:* /Multi-Index/ in \(n\) dimensions
\begin{eqnarray*}
  p & = & (p_1, p_2, \ldots , p_n), \quad \left ( \text{all \(\geqslant 0\)}
    \right )\\
  \vbar p |   & = & p_1 + \cdots + p_n,\\
  p! & = & p_1 ! \cdot \cdots \cdot p_n !,\\
  \tmmathbf{x}^p & = & x_1^{p_1} \cdot \cdots \cdot x_n^{p_n}\\
  D^p f & = & \frac{\partial ^{| p |   } f}{\partial
    x_1^{p_1} \cdot \cdots \cdot \partial x_n^{p_n}} .
\end{eqnarray*}
*With that:* For \(f\) scalar,
\(\ds f (\tmmathbf{c} + \tmmathbf{h}) \approx \sum _{| p | \leqslant k
    } \frac{D^p f (\tmmathbf{c})}{p!} \tmmathbf{h}^p \)
#+LATEX: \end{hidden}

*** Taylor and Error (I)

How can we estimate the error in a Taylor expansion?
#+LATEX: \begin{hidden}[5cm]
Back to 1D:
(\(n\)D is analogous)

\begin{eqnarray*}
  &  & \abs{f (c + h) - \sum _{p = 0}^k \frac{f^{(p)} (c)}{p!} h^p}\\
  & = & \abs{\sum _{p = k + 1}^\infty  \frac{f^{(p)} (c)}{p!} h^p},
\end{eqnarray*}
assuming that the function is identical to its infinite Taylor expansion.
#+LATEX: \end{hidden}

*** Taylor and Error (II)
Now suppose that we had an estimate that
\(\ds\abs{\frac{f^{(p)} (c)}{p!} h^p} \leqslant \alpha ^p . \)
#+LATEX: \begin{hidden}[5cm]
\vspace*{-2ex}
\begin{eqnarray*}
  &  & \abs{\sum _{p = k + 1}^\infty  \frac{f^{(p)} (c)}{p!} h^p}\\
  & \leqslant & \sum _{p = k + 1}^\infty  \abs{\frac{f^{(p)} (c)}{p!} h^p}\\
  & \leqslant & \sum _{p = k + 1}^\infty  \alpha ^p = \frac{1}{1 - \alpha }
    \cdot \alpha ^{k + 1}
\end{eqnarray*}
If \(\alpha < 1\), then this gives a viable bound.

- Slightly different technique from 'textbook' calculus technique.
- 'Textbook' like mean value theorem: not what we'll use.
#+LATEX: \end{hidden}

*** Connect Taylor and Low Rank

Can Taylor help us establish low rank of an interaction?
#+LATEX: \begin{hidden}[6cm]

#+ATTR_LATEX: :height 2cm
[[./media/taylor-schematic.pdf]]
Taylor makes a statement about evaluating a function in a vicinity:
\begin{eqnarray*}
  f (\tmmathbf{x}) = f (\tmmathbf{c} + \tmmathbf{h}) & = & \sum _{| p |
    \leqslant k   } \frac{D^p f (\tmmathbf{c})}{p!}
    \tmmathbf{h}^p\\
  & = & \sum _{| p | \leqslant k   } (\tmop{coeff}_p) G
    (\tmmathbf{x}, p)
\end{eqnarray*}
So if we can Taylor expand with a small remainder and a short
expansion, then /low rank/!
#+LATEX: \end{hidden}

*** Taylor on Potentials (I)

Compute a Taylor expansion of a 2D Laplace point potential.
#+LATEX: \begin{hidden}[6cm]
\begin{eqnarray*}
  \psi (x) & = & \sum _{i = 1}^n G (x, y_j) \varphi (y_j)\\
  & = & \sum _{i = 1}^n \log \left ( \norm{x - y}_2 \right ) \varphi (y_j)
\end{eqnarray*}
Since this is a superposition anyway: Just consider a single source.
\[\psi (x) = \log \left ( \norm{x - y}_2 \right ) \]
Pick an expansion center \(\tmmathbf{c}\). WLOG, \(\tmmathbf{c} = \tmmathbf{0}\).
\(\ds\psi (\tmmathbf{h}) \approx \sum _{| p | \leqslant k   }
   \frac{D^p \psi (\tmmathbf{0})}{p!} \tmmathbf{h}^p \)
#+LATEX: \end{hidden}

*** Taylor on Potentials (Ia)

Why is it interesting to consider Taylor expansions of Laplace point potentials?

#+LATEX: \begin{hidden}[4cm]
- Fairly non-smooth (singular)
  - What works for them will also work for smoother functions
- Important application in its own right
  - \(N\)-body simulation
  - Integral equation solvers
#+LATEX: \end{hidden}

*** Taylor on Potentials (II)

\footnotesize
\verbatiminput{maxima-kernel-derivatives.txt}

*** Taylor on Potentials (III)
Which of these is the most dangerous (largest) term?

\(\rightarrow\) Hard to say. They all contain the same number of powers of
components of \(\tmmathbf{y}\).\medskip

What's a bound on it? Let \(R = \sqrt{y_1^2 + y_2^2}\).
\[\left | \frac{5040 y_1}{(y_2^2 + y_1^2)^4}
   \right | \leqslant C \left | \frac{y_1}{R^8} \right | \leqslant C
   \frac{1}{R^7} . \]
`Generalize' this bound:
\[| D^p \psi | \leqslant C_p
\begin{cases}
  \log (R) & | p | = 0\\
  R^{- | p |} & | p | > 0
\end{cases} . \]
Appears true at least from the few \(p\) we tried. (Actually is true.)

\(C_p\) is a `generic constant'--its value could change from one time it's
written to the next.

*** Taylor on Potentials (IV)

What does this mean for the convergence of the Taylor series as a whole?
#+LATEX: \begin{hidden}[5cm]

\(\rightarrow\) Need to estimate each term. Recall that \(\tmmathbf{h}\) is the
vector from \(\tmmathbf{c}\) to the target (aka point where we evaluate)
\(\tmmathbf{x}\). (Assume \(| p | > 0\) to keep it simple.)
\[\left | \frac{D^p \psi (\tmmathbf{0})}{p!} \tmmathbf{h}^p \right |_2
   \leqslant C_p | D^p \psi (\tmmathbf{0}) \tmmathbf{h}^p |_2 \leqslant C_p
   \left ( \frac{| \tmmathbf{h} |}{R} \right )^p . \]
#+ATTR_LATEX: :height 3cm
[[./media/local-expansion.pdf]]
#+LATEX: \end{hidden}

*** Taylor on Potentials (V)

Lesson: As long as
\begin{equation*}
\frac{\max _i  | \tmmathbf{x}_i - \tmmathbf{c} |_2}{\min _j | \tmmathbf{y}_j
   - \tmmathbf{c} |_2   } = \frac{r}{R} < 1,
\end{equation*}
the Taylor series converges.

*** Taylor on Potentials (VI)

A few remarks:

- We have just invented one specific example of what we will call a
  /local expansion/ (of a potential \(\psi\)).

- The abstract idea of a /local expansion/ is that:

  - it converges on the interior of a ball as long as the closest source
    is outside that ball,

  - The error in approximating the potential by a truncated (at order
    \(k\)) local expansion is
    \[C_p \left ( \frac{r}{R} \right )^{k + 1} = \left ( \frac{\left .
           \text{dist(\tmtextbf{c}, furthest target} \right )}{\left .
           \text{dist(\tmtextbf{c}, closest source} \right )} \right )^{k + 1} \]

*** Local expansions as a Computational Tool

Low rank makes evaluating interactions cheap(er). Do local
expansions help with that
goal?
#+LATEX: \begin{hidden}[6cm]
#+ATTR_LATEX: :height 4cm
[[./media/local-expansion.pdf]]

No, not really. In a roughly uniform target distribution with \(O (N)\) targets,
we need \(O (N)\) local expansion \(\rightarrow\) nothing saved really.
#+LATEX: \end{hidden}

** Multipole Expansions
*** Taylor on Potentials, Again

Stare at that Taylor formula again.
#+LATEX: \begin{hidden}[6cm]

\[\psi (\tmmathbf{x} - \tmmathbf{y}) \approx \sum _{| p | \leqslant k
     } \underbrace{\frac{D^p_{\tmmathbf{x}} \psi
   (\tmmathbf{ x - \tmmathbf{y}) |_{\tmmathbf{x = \tmmathbf{c}}}
    }  }{p!}}_{\text{depends on src/ctr}}
   \underbrace{(\tmmathbf{x} - \tmmathbf{c})^p}_{\text{dep. on ctr/tgt}} \]
Recall: \(\tmmathbf{x}\): targets, \(\tmmathbf{y}\): sources.

At least formally, nothing goes wrong if I swap the roles of \(\tmmathbf{x}\)
and \(\tmmathbf{y}\) in the Taylor expansion:
\[\psi (\tmmathbf{x} - \tmmathbf{y}) \approx \sum _{| p | \leqslant k
     } \underbrace{\frac{D^p_{\tmmathbf{y}} \psi
   (\tmmathbf{ x - \tmmathbf{y}) |_{\tmmathbf{y = \tmmathbf{c}}}
    }  }{p!}}_{\text{depends on ctr/tgt}}
   \underbrace{(\tmmathbf{y} - \tmmathbf{c})^p}_{\text{dep. on src/ctr}} . \]
In comparison to the local expansion above, we will call this (and other
expansions like it) a /multipole expansion/.
#+LATEX: \end{hidden}

*** Multipole Expansions (I)
At first sight, it doesn't look like much happened, but
mathematically/geometrically, this is a very different animal.

*First Q:* When does this expansion converge?
#+LATEX: \begin{hidden}[5cm]

The analysis is the same as earlier:
\[(\ast ) = \abs{\frac{D^p_{\tmmathbf{y}} \psi (\tmmathbf{ x -
   \tmmathbf{y}) |_{\tmmathbf{y = \tmmathbf{c}}}  }  }{p!}
   (\tmmathbf{y} - \tmmathbf{c})^p} \leqslant C_p \frac{\norm{\tmmathbf{y} -
   \tmmathbf{c}}_2^p}{\norm{\tmmathbf{x} - \tmmathbf{c}}_2^p} = C_p \left (
   \frac{\norm{\tmmathbf{y} - \tmmathbf{c}}_2}{\norm{\tmmathbf{x} -
   \tmmathbf{c}}_2} \right )^p \]
(just with the roles of \(\tmmathbf{x}\) and \(\tmmathbf{y}\) reversed). If we
admit multiple sources/targets, we get
\[(\ast ) \leqslant C_p \left ( \frac{\max _j \norm{\tmmathbf{y}_i -
   \tmmathbf{c}}_2}{\min _i \norm{\tmmathbf{x}_i - \tmmathbf{c}}_2} \right )^p .
\]
#+LATEX: \end{hidden}

*** Multipole Expansions (II)

The abstract idea of a \emph{multipole expansion} is that:

- it converges on the *exterior* of a ball as long as the
  furthest source is closer to the center than the closest target,

- The error in approximating the potential by a truncated (at order \(k\))
  local expansion is
  \[\left ( \frac{\left . \text{dist(\tmtextbf{c}, furthest source}
       \right )}{\left . \text{dist(\tmtextbf{c}, closest target} \right )}
       \right )^{k + 1} . \]

#+ATTR_LATEX: :height 3cm
[[./media/multipole.pdf]]

The multipole expansion converges everywhere outside the circle!
(Possibly: slowly, if the targets are too close--but it does!)

*** Multipole Expansions (III)

If our particle distribution is like in the figure, then a multipole expansion
is a computationally useful thing. If we set

- \(S\) = #sources,
- \(T\) = #targets,
- \(K\) = #terms in expansion,

then the cost /without/ the expansion is \(O (ST)\),
whereas the cost /with/ the expansion is \(O (SK + KT)\).

If \(K \ll S, T\), then that's going from \(O (N^2)\) to \(O (N)\).\medskip

The rank (#terms) of the multipole expansion is the same as above for the
local expansion.\medskip

\demo{Multipole/local expansions}

** Rank Estimates

*** Taylor on Potentials: Low Rank?
Connect this to the numerical rank observations:

#+LATEX: \begin{hidden}[5cm]
We have just shown that point\(\rightarrow\)point potential interactions
have low numerical rank!

Specifically, to precision
\(\ds C_p \left ( \frac{r}{R} \right )^{k + 1} \)
the interaction from sources to targets has a numerical rank of at most
(#terms in Taylor series)
aka
\[\frac{(k + 1) (k + 2)}{2} = O (k^2) \]
in 2D, and
\[\frac{(k + 1) (k + 2) (k + 3)}{2 \cdot 3} = O (k^3) \]
in 3D.
#+LATEX: \end{hidden}

*** Taylor on Potentials: Low Rank :noexport:

Low numerical rank is no longer a numerically observed oddity, it's
mathematical fact.

Away from the sources, point potentials are smooth enough that their
Taylor series (`local expansions') decay quickly. As a result, the
potential is well-approximated by truncating those expansions, leading to
low rank.

*** On Rank Estimates

So how many terms do we need for a given precision \(\varepsilon\)?
#+LATEX: \begin{hidden}[6cm]
\begin{equation*}
  \varepsilon \approx \left ( \frac{\left .
   \text{dist(\tmtextbf{c}, furthest target} \right )}{\left .
   \text{dist(\tmtextbf{c}, closest source} \right )} \right )^{k + 1} = \rho ^{k
   + 1}
\end{equation*}
Want to relate this to \(K\) (#terms = rank). Take (2D) \(K \approx k^2\), i.e.
\(k \approx \sqrt{K}\), so \(\varepsilon \approx \rho ^{\sqrt{K} + 1}\) or

\begin{eqnarray*}
  \log \varepsilon & \approx & \left ( \sqrt{K} + 1 \right ) \log \rho \\
  \sqrt{K} + 1 & \approx & \frac{\log \varepsilon }{\log \rho }\\
  K & \approx & \left ( \frac{\log \varepsilon }{\log \rho } - 1 \right )^2 .
\end{eqnarray*}
#+LATEX: \end{hidden}

\demo{Checking rank estimates}

*** Estimated vs Actual Rank

Our rank estimate was off by a power of \(\log \varepsilon\). What gives?
#+LATEX: \begin{hidden}[6cm]
Possible reasons:

- Maybe by some happy accident some of the Taylor coefficients are zero?
  \(\rightarrow\) No, doesn't look like it.

- The Taylor basis uses \(O (\log (\varepsilon )^2)\) terms.

  - That's just an existence proof of an expansion with that error.

  - Maybe a better basis exists?
#+LATEX: \end{hidden}

*** Taylor and PDEs

Look at \(\partial _x^2 G\) and \(\partial _y^2 G\) in the multipole demo again.
Notice anything?

#+LATEX: \begin{hidden}[5cm]

How does that relate to \(\Delta G = 0\)?

- \(\partial _x G = - \partial _y^2 G\) means that we can reduce from \(O     (p^2)\) to \(O (p)\) \emph{actually} distinct terms \(\rightarrow\) problem
  solved: same value of expansion (i.e. same accuracy), many fewer terms

- Alternatively: be clever
#+LATEX: \end{hidden}

*** Being Clever about Expansions

How could one be clever about expansions? (i.e. give examples)
#+LATEX: \begin{hidden}[6cm]

- Realize that in 2D, harmonic functions (\(\Delta u = 0\)) map one-to-one
  to complex-analytic ones.

  Then, use complex-valued Taylor, reduces number of terms from \(O (p^2)\) to
  \(O (p)\)

- Use DLMF:

  Example: Helmholtz kernel \((\Delta + \kappa ^2) u = 0\)

  Fundamental solutions:

  - Bessel functions \(J_\ell  (\kappa r)\)

  - Hankel functions of the first kind \(H^{(1)}_\ell  (\kappa r)\)

- 3D: Spherical harmonics, ...
#+LATEX: \end{hidden}

*** Expansions for Helmholtz

How do expansions for other PDEs arise?
#+LATEX: \begin{hidden}[2cm]
- Transform Helmholtz PDE to polar coordinates
- Obtain the Bessel ODE (in \(r\))
- Solve resulting 1D BVP (in \(r\))
#+LATEX: \end{hidden}

DLMF 10.23.6 shows `Graf's addition theorem':

\begin{gather*}
H^{(1)}_0 \left ( \kappa \norm{x - y}_2 \right ) =\\
\sum _{\ell = - \infty }^\infty  \underbrace{H_\ell ^{(1)} \left ( \kappa
    \norm{y - c}_2 \right ) e^{i \ell \theta '}}_{\text{singular}}
    \underbrace{J_\ell  \left ( \kappa \norm{x - c}_2 \right ) e^{- i \ell
    \theta }}_{\text{nonsingular}}
\end{gather*}

where \(\theta = \angle (x - c)\) and \(\theta ' = \angle (x' - c\)).

\medskip
Can apply same family of tricks as with Taylor to derive multipole/local expansions.

** Proxy Expansions
*** Making Multipole/Local Expansions using Linear Algebra

Actual expansions cheaper than LA approaches. Can this be fixed?
\smallskip

Compare costs for this situation:
#+LATEX: \begin{hidden}[5cm]

- \(S\) sources
- \(T\) targets
- Actual interaction rank: \(K \ll \min (S, T)\).

*Cost for expansions:*

- Compute expansion coefficients: \(O (KS)\)
- Evaluate expansion coefficients: \(O (KT)\)

Overall: \(O (k (S + T))\): Cheap!\medskip

*Cost for linear algebra:*

- Build matrix: \(O (ST)\)
- ...

Oops. Can't be competitive, can it?
#+LATEX: \end{hidden}

*** The Proxy Trick

*Idea:* /Skeletonization using Proxies/

\demo{Skeletonization using Proxies}\medskip

*Q:* What error do we expect from the proxy-based multipole/local
`expansions'?

#+LATEX: \begin{hidden}[5cm]

- Function expansions give an indication of what is doable at a certain
  rank

- SVD-based linear algebra should match or beat that
- Proxy-based linear algebra... may or may not?

*Investigation of this:* \(\rightarrow\) HW
#+LATEX: \end{hidden}

*** Why Does the Proxy Trick Work?

In particular, how general is this? Does this work for any
kernel?
#+LATEX: \begin{hidden}[5cm]
*No.* There are two (kernel-specific) miracles here:

- We can /represent/ the far field of many sources in terms of
  the far field of a few--and that apparently regardless of what the targets
  are.

  (`plausible', rigorously due to Green's formula \(\rightarrow\) later)

- We only get a /surface/ of sources because `surface data' is
  enough to reconstruct /volume/ data.

  This works because an (interior or exterior) Laplace potential is fully
  determined by its values on a boundary. (This is a fact that we will prove
  later, but if you believe that Laplace boundary value problems are solvable,
  you already believe it.)

*Remark:* In both cases, it's the PDE that
provides the cost reduction from \(O (k^d)\) (`volume') to \(O (k^{d - 1})\)
(`surface')!
#+LATEX: \end{hidden}

*** Where are we now? (I)

Summarize what we know about interaction ranks.

- We know that far interactions with a smooth kernel have low rank.
  (Because: short Taylor expansion suffices)

- If
  \[\psi (\tmmathbf{x}) = \sum _j G (\tmmathbf{x}, \tmmathbf{y}_j) \varphi
       (\tmmathbf{y}_j) \]
  satisfies a PDE (e.g. Laplace), i.e. if
  \(G (\tmmathbf{x}, \tmmathbf{y}_j) \)
  satisfies a PDE, then that low rank is /even/ lower.

- Can construct interior (`local') and exterior (`multipole') expansions
  (using Taylor or other tools).

- Can lower the number of terms using the PDE.

- Can construct LinAlg-workalikes for interior (`local') and exterior
  (`multipole') expansions.

- Can make those cheap using proxy points.

*** Where are we now? (II)
So we can compute interactions where sources are distant from targets (i.e.
where the interaction is low rank) quite quickly.

*Problem:* In general, that's not the situation that we're in.

#+ATTR_LATEX: :height 3cm
[[./media/pile-of-particles.pdf]]

*But:* /Most/ of the targets are far away from /most/ of the sources.

(\(\Leftrightarrow\) Only a few sources are close to a chosen `close-knit' group
of targets.)

So maybe we can do business yet--we just need to split out the near
interactions to get a hold of the far ones (which (a) constitute the bulk of
the work and (b) can be made cheap as we saw.)

* Near and Far: Separating out High-Rank Interactions
  :PROPERTIES:
  :RELATE_TREE_SECTION_NAME: near_and_far
  :RELATE_TREE_SECTION_OPENED: true
  :END:

** Ewald Summation
*** Preliminaries: Convolution

\[(f \ast g) (x) = \int _{\mathbb{R}} f (\xi ) g (x - \xi ) d \xi . \]
- Convolution with shifted $\delta$ is the same as shifting the function;
  \[[f \ast (\xi \mapsto \delta (\xi - a))] (x) = f (x - a)\]

- Convolution is linear (in both arguments) and commutative.


*** Preliminaries: Fourier Transform
\[\mathcal F(f)(\omega) = \int_{\mathbb R} f(x) e^{-2\pi i \omega x} dx \]
- Convolution turns into multiplication: \(\mathcal{F} \{f \ast g \}=\mathcal{F}f \cdot \mathcal{F}g, \)
- A single $\delta$ turns into: \(\mathcal F \{\delta(x-a)\}(\omega)=e^{-i a\omega}  \)

- And a "train" of  \(\delta\)s turns into:
  \[\mathcal{F} \left \{\sum _{\ell \in \mathbb{Z}} \delta (x - \ell) \right \}
  (\omega ) = \sum _{k \in \mathbb{Z}} \delta (\omega - 2\pi k) . \]

What is \(\mathcal F\{f(x-a)\}\)?
#+LATEX: \begin{hidden}
\[\mathcal F\{x\mapsto f(x-a)\} =\mathcal F\{f\}\mathcal F\{\delta(x-a)\} =\mathcal F\{f\}e^{-i a\omega}\]
#+LATEX: \end{hidden}

See e.g. [[[http://maverick.inria.fr/~Xavier.Decoret/resources/maths/impulsion-train.pdf][Décoret `04]]].

*** Simple and Periodic: Ewald Summation

Want to evaluate potential from an infinite periodic grid of
sources:
\[\psi (\tmmathbf{x}) = \sum _{\tmmathbf{m} \in \mathbb{Z}^d} \sum _{j =
   1}^{N_{\tmop{src}}} G (\tmmathbf{x}, \tmmathbf{y}_j + \tmmathbf{m}) \varphi
   (\tmmathbf{y}_j) \]
#+LATEX: \begin{hidden}[5cm]
`Potential' \(\psi\) is periodic as well (\(\rightarrow\) just need values in
one unit cell).

#+ATTR_LATEX: :height 3cm
[[./media/boxes.pdf]]

Clear: Expressible as a convolution.
#+LATEX: \end{hidden}


*** Lattice Sums: Convergence
*Q:* When does this have a right to converge?

#+LATEX: \begin{hidden}[6cm]

- \(G = O (1)\) throghout obviously won't work

  \(\rightarrow\) there must be some sort of fall-off

- \(G = O \left ( \norm{\tmmathbf{x}}_2^{- p} \right )\). Now think in
  spherical shells:
  \[\psi (\tmmathbf{0}) = \sum _{i = 0}^\infty  \sum _{\text{cells@\(\ell ^2\)
       dist \([i, i + 1)\) to \(\tmmathbf{0}\)}} \underbrace{O (i^{d -
       1})}_{\text{surface of shell} \sim \#\tmop{cells}} O (i^{- p}) \]
  where \(d\) is space dimension. Have:
  \[d - 1 - p < - 1 \quad \Leftrightarrow \quad p > d. \]
  (because \(\sum 1 / n\) is divergent)
#+LATEX: \end{hidden}

*** Ewald Summation: Dealing with Smoothness


\[\psi (\tmmathbf{x}) = \sum _{\tmmathbf{i} \in \mathbb{Z}^d} \sum _{j =
   1}^{N_{\tmop{src}}} G (\tmmathbf{x}, \tmmathbf{y}_j + \tmmathbf{i}) \varphi
   (\tmmathbf{y}_j) \]

Clear: a discrete convolution. Would like to make use of the fact that
the Fourier transform turns convolutions into products. How?

#+LATEX: \begin{hidden}[4cm]
- $G$ is nonsmooth, it will have a Fourier transform with a long tail, hard to compute.
- Idea: separate near (singular) and far part in such a way that
  far kernel is smooth enough for Fourier, and near is close enough
  to allow for direct summation.
#+LATEX: \end{hidden}

*** Ewald Summation: Screens

#+LATEX: \begin{hidden}[7cm]
Split \(G\) into two parts with a /screen/ \(\sigma\) that
`bleeps out' the singularity:
\[G (\tmmathbf{x}) = \sigma (\tmmathbf{x}) G (\tmmathbf{x}) + (1 - \sigma
   (\tmmathbf{x})) G (\tmmathbf{x}) . \]
How does that help? Consider \(G = 1 / r^4\).
\[G (\tmmathbf{x}) = \underbrace{\sigma (\tmmathbf{x})
   \frac{1}{\norm{\tmmathbf{x}}_2^4}}_{G_{\tmop{LR}}} + \underbrace{(1 -
   \sigma (\tmmathbf{x})) \frac{1}{\norm{\tmmathbf{x}}_2^4}}_{G_{\tmop{SR}}}
\]
Then, suppose

- \(\sigma\) is smooth
- \(\sigma (\tmmathbf{x}) = O \left ( \norm{\tmmathbf{x}}_2^4 \right )\)
- \(1 - \sigma\) has bounded support (i.e. \(\sigma (\tmmathbf{x}) = 1\) if
  \(\norm{\tmmathbf{x}}_2 > R\) for some \(R\))
#+LATEX: \end{hidden}

*** Ewald Summation: Field Splitting

We can split the computation (from the perspective of a unit cell target) as
follows:

#+LATEX: \begin{hidden}[5cm]
|--------------+--------------------+-------------------------|
|              | \(G_{\tmop{SR}}\)  | \(G_{\tmop{LR}}\)       |
|--------------+--------------------+-------------------------|
| Close source | \(A\): singular    | \(B\): smooth           |
|              | sum directly (few) | use Fourier \((\ast )\) |
|--------------+--------------------+-------------------------|
| Far source   | 0                  | \(C\): smooth           |
|              |                    | use Fourier \((\ast )\) |
|--------------+--------------------+-------------------------|

(where `close' means `s \(\rightarrow\) t distance \(< R\)' and `far' the opposite)
#+LATEX: \end{hidden}

*** Ewald Summation: Summation (1D for simplicity)

Interesting bit: How to sum \(G_{\tmop{LR}}\).
#+LATEX: \begin{hidden}[6cm]
\vspace*{-3ex}
\begin{eqnarray*}
  &  & \mathcal{F} \{\psi \}-\mathcal{F} \{\psi _{\tmop{SR}} \}
    =\mathcal{F} \{\psi _{\tmop{LR}} \}\\
  & = & \mathcal{F} \{G_{\tmop{LR}} \}\mathcal{F} \left \{x \mapsto \sum _{m
    \in \mathbb{Z}} \sum _{j = 1}^{N_{\tmop{src}}} \delta (x - y_j - m)
    \right \}\\
  & = & \mathcal{F} \{G_{\tmop{LR}} \}\left ( \sum _{j = 1}^{N_{\tmop{src}}}
    e^{- iy_j \omega } \cdot \mathcal{F} \left \{x \mapsto \sum _{m \in
    \mathbb{Z}} \delta (x - m) \right \}\right )\\
  & = & \mathcal{F} \{G_{\tmop{LR}} \}\left ( \sum _{j = 1}^{N_{\tmop{src}}}
    e^{- iy_j \omega } \cdot \left ( \omega \mapsto \sum _{n \in \mathbb{Z}}
    \delta (\omega - 2\pi n) \right ) \right )
\end{eqnarray*}
Now, since \(G_{\tmop{LR}}\) is smooth, \(\mathcal{F} \{G_{\tmop{LR}} \} (\omega )\) should fall off quickly as \(| \omega |  \)
increases. \(\rightarrow\) Well-approximated with finitely many terms of the sum
over \(m\). (Again: Smooth function leads to low rank!)
#+LATEX: \end{hidden}

*** Ewald Summation: Remarks

*In practice:* Fourier transforms carried out discretely, using FFT.

- Additional error contributions from interpolation

  (small if screen smooth enough to be well-sampled by mesh)
- \(O (N \log N)\) cost (from FFT)
- Need to choose evaluation grid (`mesh')
- Resulting method called Particle-Mesh-Ewald (`PME')

** Barnes-Hut
*** Barnes-Hut: Putting Multipole Expansions to Work

#+BEGIN_CENTER
# FIXME Add missing figure
# #+ATTR_LATEX: :height 7cm
# [[./media/bhut-01-particles.pdf]]
#+END_CENTER

*** Barnes-Hut: The Task At Hand

Want: All-pairs interaction.

*Caution:*

- In these (stolen) figures: \textcolor{blue}{targets} \textcolor{red}{sources}
- Here: \textcolor{red}{targets and sources}

#+LATEX: \begin{hidden}[5cm]
Specifically, want
\[\tmmathbf{u} = A \tmmathbf{q} \]
where
\[A_{i j} = \log (\tmmathbf{x_i - \tmmathbf{x}_j}) . \]
*Idea:* We have all this multipole technology, but no way to use
it:

No targets are cleanly separated from other sources.

\medskip
*Lesson from PME:* If you can't compute the entire interaction,
compute parts of it. To help do so, put down a grid.
#+LATEX: \end{hidden}

*** Barnes-Hut: Putting Multipole Expansions to Work

#+BEGIN_CENTER
# FIXME Add missing figure
# #+ATTR_LATEX: :height 5cm
# [[./media/bhut-02-boxes.pdf]]
#+END_CENTER

*** Barnes-Hut: Putting Multipole Expansions to Work

#+BEGIN_CENTER
# FIXME Add missing figure
# #+ATTR_LATEX: :height 5cm
# [[./media/bhut-03-boxes-tgt.pdf]]
#+END_CENTER

*** Barnes-Hut: Box Targets
For sake of discussion, choose one `box' as targets.

*Q:* For which boxes can we then use multipole expansions?
#+LATEX: \begin{hidden}
*A:* Depends on the wanted accuracy (via the expansion order)!
#+LATEX: \end{hidden}

*** Barnes-Hut: Putting Multipole Expansions to Work

#+BEGIN_CENTER
# FIXME Add missing figure
# #+ATTR_LATEX: :height 5cm
# [[./media/bhut-04-boxes-mpole.pdf]
#+END_CENTER

*** Barnes-Hut: Accuracy

With this computational outline, what's the accuracy?
#+LATEX: \begin{hidden}[4cm]
\begin{eqnarray*}
  \varepsilon & \sim & \left ( \frac{d \left ( \text{box ctr}, \tmop{furthest}
    \tmop{src} \right )}{d \left ( \text{box ctr}, \text{closest tgt} \right )}
    \right )^{k + 1}\\
  & = & \left ( \frac{\text{box `radius'} \cdot \sqrt{2}}{\text{box `radius'}
    \cdot 3} \right )^{k + 1}\\
  & = & \left ( \frac{\sqrt{2}}{3} \right )^{k + 1}
\end{eqnarray*}
*Observation:* Dependent on space dimension!

#+LATEX: \end{hidden}
*Q:* Does this get better or worse as dimension increases?

*** Barnes-Hut (Single-Level): Computational Cost

What's the cost of this algorithm?
#+LATEX: \begin{hidden}[6cm]
Let:

- \(N\) be #particles
- \(K\) be #terms in expansion
- \(m\) be #particles/box.

|-----------------+------------------------------------------+----------+---------------|
| *What*          | *How often*                              | *Cost*   | *Total*       |
|-----------------+------------------------------------------+----------+---------------|
| Compute mpoles  | \(N / m\) boxes                          | \(Km\)   | \(KN\)        |
| Evaluate mpoles | \(N\) tgts \(\cdot\) \(N / m\) src boxes | \(K\)    | \(N^2 K / m\) |
| 9 close boxes   | \(9 \cdot\)(\(N / m\) boxes)             | \(m^2\)  | \(Nm\)        |
|-----------------+------------------------------------------+----------+---------------|

- Assume \(m \sim \sqrt{N}\) or \(N \sim m^2\).

  *Q:* Where does this assumption come from?
#+LATEX: \end{hidden}

*** Barnes-Hut Single Level Cost: Observations

#+LATEX: \begin{hidden}[6cm]
Forget \(K\) (small, constant). Only mpole eval matters:
\[\tmop{cost} \sim \frac{N^2}{m} \sim N^{1.5} . \]
*Observations:* There are very many (very) far box-box
interactions.

*Idea:* Summarize further \(\rightarrow\) bigger boxes \(\rightarrow\)
`larger' multipoles representing more sources.\medskip

*Idea:* To facilitate this `clumping', don't use a \emph{grid} of
boxes, instead make a /tree/.
#+LATEX: \end{hidden}

*** Box Splitting

#+BEGIN_CENTER
# FIXME Add missing figure
# #+ATTR_LATEX: :height 5cm
# [[./media/bhut-05-levels.pdf]]
#+END_CENTER

*** Level Count
How many levels?
#+LATEX: \begin{hidden}[6cm]
Options:

- Keep refining until the number of sources in each leaf box is below a
  certain given constant

- /Obvious tweak:/ Only do that for boxes that
  /actually/ have too many sources (\(\rightarrow\) /adaptive/
  tree, vs. the above /non-adaptive/ strategy.

  /Downside of adaptive tree:/ More bookkeeping
#+LATEX: \end{hidden}

*** Box Sizes

#+BEGIN_CENTER
# FIXME Add missing figure
# #+ATTR_LATEX: :height 5cm
# [[./media/bhut-06-particles.pdf]]
#+END_CENTER

Want to evaluate all the \textcolor{red}{source} interactions with the
\textcolor{blue}{targets} in the box.

\medskip
*Q:* What would be good sizes for source boxes? What's the
requirement?

*** Multipole Sources

#+BEGIN_CENTER
# FIXME Add missing figure
# #+ATTR_LATEX: :height 5cm
# [[./media/bhut-07-particle-tree.pdf]]
#+END_CENTER

Data from which of these boxes could we bring in using multipole
expansions? Does that depend on the type of expansion? (Taylor/special
function vs skeletons)

*** Barnes-Hut: Box Properties

#+BEGIN_CENTER
# FIXME Add missing figure
# #+ATTR_LATEX: :height 5cm
# [[./media/bhut-08-mpole-tree.pdf]]
#+END_CENTER

What properties do these boxes have?

*Simple observation:* The further, the bigger.

*** Barnes-Hut: Box Properties

#+LATEX: \begin{hidden}[7cm]

*More complete:* We can put a uniform bound on the error in the
(function) expansion at the target:\medskip

\(r_s :\) Source box `radius` (center to vertical/horizontal edge)

\(R\): source center \(\rightarrow\) target center distance

\(r_t :\) Target box `radius` (center to vertical/horizontal edge)
\[\left ( \frac{d \left ( \text{src ctr}, \text{furthest src} \right )}{d \left (
   \text{src ctr}, \text{closest tgt} \right )} \right )^{k + 1} = \left (
   \frac{r_s \sqrt{2}}{R - r_t} \right )^{k + 1} \]
#+LATEX: \end{hidden}

*** Barnes-Hut: Well-separated-ness

Which boxes in the tree should be allowed to contribute via multipole?

#+LATEX: \begin{hidden}[7cm]
#+BEGIN_CENTER
\begin{tikzpicture}[scale=0.7]
  \draw [thick] (-1,-1) rectangle (1,1) ;
  \draw [thick,dashed] (0,0) coordinate (sctr) -- +(0,1) node [pos=0.5,anchor=west] {$r_s$};
  \draw [thick] (3-1,-1) rectangle (3+1,1) ;
  \draw [thick,dashed] (3,0) coordinate (tctr) -- +(0,1) node [pos=0.5,anchor=west] {$r_t$};
  \draw (3,0) circle (1.4145);
  \draw [thick,dotted] (3,0) (sctr) -- (tctr) node [pos=0.4,anchor=south] {$R$};
\end{tikzpicture}
#+END_CENTER

Convergent iff
\(r_s \sqrt{2} < R - r_t . \qquad (\ast ) \)

Convergent /if/ (picture) \(R \geqslant 3 \cdot \max (r_t, r_s) \qquad (\ast \ast ) \)

because \((\ast ) \Leftrightarrow \left ( r_t + \sqrt{2} r_s \right ) < R. \)

\medskip
We'll make a new word for that: A pair of boxes satisfying the condition
\((\ast \ast )\) is called /well-separated/. *Observations:*

- This is just /one/ choice. (the one we'll use anyway)
- One can play games here, based on a target accuracy.

  \(\rightarrow\) /Multipole Acceptance Criterion/ (`MAC')
#+LATEX: \end{hidden}

*** Barnes-Hut: Revised Cost Estimate

Which of these boxes are well-separated from one another?
#+BEGIN_CENTER
# FIXME Add missing figure
# #+ATTR_LATEX: :height 5cm
# [[./media/bhut-08-mpole-tree.pdf]]
#+END_CENTER

What is the cost of evaluating the \textcolor{blue}{target}
potentials, assuming that we know the multipole expansions already?

*** Barnes-Hut: Revised Cost Estimate
#+LATEX: \begin{hidden}[7cm]
- \(L\) be the number of levels
- \(N\) be #particles
- \(K\) be #terms in expansion
- \(m\) be #particles/box.
  Assume bounded (say, \(m \leqslant 100\))

- Then \(L \sim \log (N)\)

What do we need to do?

- 9 boxes of direct evaluation (self and touching neighbors)

  \(\rightarrow\) \(O (m) = O (1)\)

- \(L\) levels of multipoles, each of which contains:

  - \(\leqslant\) 27 source boxes (!) (in 2D)

  \(\rightarrow\) \(O (LK) = O (\log N)\)

There are \(O (N)\) target boxes (because \(m\) is fixed), so we do the above \(O (N)\) times \(\rightarrow\) \(O (N \log N)\) total work to evaluate.
#+LATEX: \end{hidden}

*** Barnes-Hut: Next Revised Cost Estimate

#+BEGIN_CENTER
# FIXME Add missing figure
# #+ATTR_LATEX: :width \textwidth
# [[./media/bhut-09-all-mpoles.pdf]]
#+END_CENTER

Summarize the algorithm (so far) and the associated cost.

*** Barnes-Hut: Next Revised Cost Estimate

Summarize the algorithm (so far) and the associated cost.
#+LATEX: \begin{hidden}[7cm]

|                 | *How often*                             | *Cost*  | *Total cost*  |
|-----------------+-----------------------------------------+---------+---------------|
| Compute mpoles  | \(N\) srcs                              | \(LK\)  | \(KN \log N\) |
| Evaluate mpoles | \(N\) tgts \(\cdot\) \(27 L\) src boxes | \(K\)   | \(NK \log N\) |
| 9 close boxes   | \(9 \cdot\)(\(N / m\) boxes)            | \(m^2\) | \(N / m\)     |

So even with the forming of the multipoles, the overall algorithm is \[O (N \log N).\]

Also, if we wanted to get the whole thing down to \(O (N)\), we would need to
speed up both computing and evaluating the multipoles.

Let's start with the former.
#+LATEX: \end{hidden}

*** Barnes-Hut: Putting Multipole Expansions to Work

#+BEGIN_CENTER
# FIXME Add missing figure
# #+ATTR_LATEX: :width \textwidth
# [[./media/bhut-10-mpoles-sources.pdf]
#+END_CENTER

How could this process be sped up?

*** Barnes-Hut: Clumps of Boxes?

*Observation:* The amount of work does not really
decrease as we go up the tree: Fewer boxes, but more particles in each of
them.

But we already compute multipoles to summarize lower-level boxes...

*** Barnes-Hut: Putting Multipole Expansions to Work

#+BEGIN_CENTER
# FIXME Add missing figure
# #+ATTR_LATEX: :width \textwidth
# [[./media/bhut-11-mpoles-translate.pdf]]
#+END_CENTER

To get a new `big' multipole from a `small' multipole, we need a
new mathematical tool.

*** Barnes-Hut: Translations

#+LATEX: \begin{hidden}[7cm]
Nominally, all the tool needs to accomplish is to

- take in a multipole expansion at one center

- and `translate it' so that it now serves as an expansion about a
  different center.

The transformation that accomplishes this is called a /translation
operator/, and this particular one is called /multipole-to-multipole
translation/.

\medskip
Questions:

- How do you do it?

- Where is the resulting expansion valid?

\(\rightarrow\) HW
#+LATEX: \end{hidden}

*** Cost of Multi-Level Barnes-Hut

#+LATEX: \begin{hidden}[7cm]
*Just the new construction phase:*

| *Level*                | *What*                      | *Cost*  | *How Many*     |
|------------------------+-----------------------------+---------+----------------|
| \(L\) (lowest, leaves) | src \(\rightarrow\) mpoles  | \(mK\)  | \((N / m)\)    |
| \(L - 1\)              | mpole \(\rightarrow\) mpole | \(K^2\) | \((N / m)/4\)  |
| \(L - 2\)              | mpole \(\rightarrow\) mpole | \(K^2\) | \((N / m)/16\) |
|                        |                             |         | \(\vdots\)     |

/Altogether:/ \(O (KN) + O (K^2 N) \sim O (N)\)

| *What*          | *Total Cost*   |
|-----------------+----------------|
| Compute mpoles  | \(KN + K^2 N\) |
| Evaluate mpoles | \(NK \log N\)  |
| 9 close boxes   | \(NK / m\)     |

*Altogether:* Still \(O (N \log N)\), but the first stage is now \(O (N)\).\medskip

#+LATEX: \end{hidden}

*** Cost of Multi-Level Barnes-Hut: Observations

*Observation:* Multipole evaluation remains as the single most costly
bit of this algorithm. \emph{Fix?}\medskip

*Idea:* Exploit the tree structure also in performing this
step.

If `upward' translation of multipoles helped earlier,
maybe `downward' translation of /local/ expansions can help now.

** Fast Mutipole
*** Using Multipole-to-Local

#+BEGIN_CENTER
# FIXME Add missing figure
# #+ATTR_LATEX: :width \textwidth
# [[./media/fmm-mtol-1.pdf]]
#+END_CENTER

Come up with an algorithm that computes the interaction in the figure.

*** Using Multipole-to-Local
Come up with an algorithm that computes the interaction in the figure.

#+LATEX: \begin{hidden}[6cm]

1. Form \textcolor{red}{multipoles}
2. Translate \textcolor{red}{multipole} to \textcolor{blue}{local}
3. Evaluate \color{blue}{local}

*But:*

- \textcolor{green}{Box} has children. What about them?
- And there are a number of closer sources that we've neglected.

Let's consider the situation from the next level down.
#+LATEX: \end{hidden}

*** Using Multipole-to-Local: Next Level

#+BEGIN_CENTER
# FIXME Add missing figure
# #+ATTR_LATEX: :width \textwidth
# [[./media/fmm-mtol-2.pdf]]
#+END_CENTER

Assuming we retain information from the previous level, how can we
obtain a valid local expansion on the \textcolor{blue}{target}
box?

*** Using Multipole-to-Local: Next Level

Assuming we retain information from the previous level, how can we
obtain a valid local expansion on the \textcolor{blue}{target}
box?

#+LATEX: \begin{hidden}[5cm]

1. Obtain contribution from well-separated boxes on previous level by
  \textcolor{blue}{local}\(\rightarrow\)\textcolor{blue}{local} translation.

1. Obtain contribution from \textcolor{red}{well-separated boxes} on this
  level by

  \textcolor{red}{multipole}\(\rightarrow\)\textcolor{blue}{local} translation. For
  our target box \textcolor{green}{\(b\)}, call this list of boxes the
  /interaction list/ \(I_b\).

1. Keep recursing until only touching boxes remain, compute interaction
  from those directly.
#+LATEX: \end{hidden}

*** Define `Interaction List'

For a box \(b\), the interaction list \(I_b\) consists of all boxes
\(b'\) so that
#+LATEX: \begin{hidden}[3cm]

- \(b\) and \(b'\) are on the same level,

- \(b\) and \(b'\) are well-separated, and

- the parents of \(b\) and \(b'\) touch.
#+LATEX: \end{hidden}

*** The Fast Multipole Method (`FMM')

\small

**** Upward pass                                                   :B_column:
     :PROPERTIES:
     :BEAMER_col: 0.5
     :END:

*Upward pass*

1. Build tree

1. Compute interaction lists
1. Compute lowest-level multipoles from sources
1. Loop over levels \(\ell = L - 1, \ldots , 2\):

   1. Compute multipoles at level \(\ell\) by \(\tmop{mp} \rightarrow \tmop{mp}\)

**** Downward pass                                                 :B_column:
     :PROPERTIES:
     :BEAMER_col: 0.5
     :END:
*Downward pass*

1. Loop over levels \(\ell = 2, 3, \ldots , L - 1\):

   1. Loop over boxes \(b\) on level \(\ell :\)

      1. Add contrib from \(I_b\) to local expansion by \(\tmop{mp} \rightarrow \tmop{loc}\)

      2. Add contrib from parent to local exp by \(\tmop{loc} \rightarrow \tmop{loc}\)
1. Evaluate local expansion and direct contrib from 9 neighbors.

**** Below columns                                          :B_ignoreheading:
     :PROPERTIES:
     :BEAMER_env: ignoreheading
     :END:

\medskip
*Overall algorithm:* Now \(O (N)\) complexity.

\medskip
*Note:* \(L\) levels, numbered \(0, \ldots , L - 1\). Loop indices above /inclusive./

*** What about adaptivity?

#+BEGIN_CENTER

  #+ATTR_LATEX: :height 6cm
  [[./media/fmm-tree-no-numbers.png]]

  Figure credit: Carrier et al. (`88)
#+END_CENTER

*** What about adaptivity?

#+BEGIN_CENTER

  #+ATTR_LATEX: :height 7cm
  [[./media/fmm-tree.pdf]]

  Figure credit: Carrier et al. (`88)
#+END_CENTER

*** Adaptivity: what changes?
#+LATEX: \begin{hidden}[4cm]

- Boxes interacting with a target box \(b\) can be at many levels
- Both higher and lower
#+LATEX: \end{hidden}

*** FMM: List of Interaction Lists
Make a list of cases:
#+LATEX: \begin{hidden}[5cm]

1. Near/touching neighbor: direct
2. Well-separated, same level: \(\tmop{mp} \rightarrow \tmop{loc}\)
3. Well-separated, lower level: \(\tmop{mp} \rightarrow \tmop{tgt}\)
4. Not well-separated, higher level: \(\tmop{src} \rightarrow \tmop{loc}\)
5. Well-separated, higher level: nothing to do

In the FMM literature, the resulting interaction lists are `insightfully'
often called `List 1', `List 2', ... (with the case numbers above).
Alternatively: `List U', `List V', ...
#+LATEX: \end{hidden}

** Direct Solvers
*** What about solving?

Likely computational goal: Solve a linear system \(Ax = b\). How do
our methods help with that?
#+LATEX: \begin{hidden}[4cm]

- Barnes-Hut/FMM/Ewald provide a matvec.

  If iterative solvers (e.g. GMRES) work (i.e. converge quickly), we're done
  here.

- If not, how would we construct the equivalent of a direct solver (e.g.
  LU)?
#+LATEX: \end{hidden}

*** A Matrix View of Low-Rank Interaction

Only /parts of the matrix are low-rank/! What does this look like from a matrix perspective?
#+LATEX: \begin{hidden}[6cm]

#+BEGIN_CENTER
  \begin{tikzpicture}[x=1cm,y=-1cm, lr/.style={fill=black!20},scale=0.75]
    \draw (0,0) rectangle (4,4);
    \draw (0,0) -- (4,4);
    \draw [lr] (0,2) rectangle ++(2,2);
    \draw [lr] (2,0) rectangle ++(2,2);

    \draw [lr] (0,0) ++(0,1) rectangle ++(1,1);
    \draw [lr] (0,0) ++(1,0) rectangle ++(1,1);
    \draw [lr] (2,2) ++(0,1) rectangle ++(1,1);
    \draw [lr] (2,2) ++(1,0) rectangle ++(1,1);

    \draw [lr] (0,0) ++(0,0.5) rectangle ++(0.5,0.5);
    \draw [lr] (0,0) ++(0.5,0) rectangle ++(0.5,0.5);
    \draw [lr] (1,1) ++(0,0.5) rectangle ++(0.5,0.5);
    \draw [lr] (1,1) ++(0.5,0) rectangle ++(0.5,0.5);
    \draw [lr] (2,2) ++(0,0.5) rectangle ++(0.5,0.5);
    \draw [lr] (2,2) ++(0.5,0) rectangle ++(0.5,0.5);
    \draw [lr] (3,3) ++(0,0.5) rectangle ++(0.5,0.5);
    \draw [lr] (3,3) ++(0.5,0) rectangle ++(0.5,0.5);
  \end{tikzpicture}
#+END_CENTER

where shaded blocks have low rank.

Remarks about this matrix form:

- This structure is obviously dependent on ordering.

  Realize that finding tree boxes constitutes an ordering.
#+LATEX: \end{hidden}

*** (Recursive) Coordinate Bisection (RCB)

  #+ATTR_LATEX: :height 3cm
[[./media/coordinate-bisection.pdf]]

\vspace{-5ex}
#+LATEX: \begin{hidden}[5cm]
Then, assuming the right ordering, the matrix captures the following
interactions:
\[
\begin{bmatrix}
  \tmop{Left} \rightarrow \tmop{Left} & \tmop{Right} \rightarrow
         \tmop{Left}\\\tmop{Left} \rightarrow \tmop{Right} & \tmop{Right} \rightarrow
         \tmop{Right}
\end{bmatrix} \]
The off-diagonal blocks have low-ish rank.
#+LATEX: \end{hidden}

*** Block-separable matrices

\[A =
\begin{bmatrix}
  D_1 & A_{12} & A_{13} & A_{14}\\A_{21} & D_2 & A_{23} & A_{24}\\A_{31} & A_{32} & D_3 & A_{34}\\A_{41} & A_{42} & A_{43} & D_4
\end{bmatrix} \]
where \(A_{i j}\) has low rank: How to capture rank structure?
#+LATEX: \begin{hidden}[4cm]
Recall column ID:
\[A_{i j} \approx (A_{i j})_{(:, J)} \Pi _{\tmop{col}} \]
Recall row ID:
\[A_{i j} \approx P_{\tmop{row}} (A_{i j})_{(I, :)} \]
Both together:
\[A_{i j} \approx P_{\tmop{row}} \underbrace{(A_{i j})_{(I,
   J)}}_{\tilde {A}_{i j}} \Pi _{\tmop{col}} . \]
#+LATEX: \end{hidden}


*** Proxy Recap
/Saw:/ If \(A\) comes from a kernel for which Green's formula holds,
then the same skeleton will work for all of space, for a given set of
sources/targets.

What would the resulting matrix look like?

*** Rank and Proxies

Unlike FMMs, partitions here do not include "buffer" zones of near elements.
What are the consequences?

#+LATEX: \begin{hidden}[5cm]
- IDs will be built using proxies
- If near-neighbor particles inside the proxy circle, simply include them as 'additional proxies'
  to enlarge the space enough to get accuracy for those interactions
- Will have higher rank as a result
- It's possible to use buffering in the sovler, but the math gets more involved
#+LATEX: \end{hidden}

*** Block-Separable Matrices

A /block-separable matrix/ looks like this:
\[A =
\begin{bmatrix}
  D_1 & \tmcolor{red}{P_1} \tilde {A}_{12} \Pi _2 & \tmcolor{red}{P_1}
       \tilde {A}_{13} \Pi _3 & \tmcolor{red}{P_1} \tilde {A}_{14} \Pi _4\\P_2 \tilde {A}_{21} \Pi _1 & D_2 & P_2 \tilde {A}_{23} \Pi _3 & P_2
       \tilde {A}_{24} \Pi _4\\P_3 \tilde {A}_{31} \Pi _1 & P_3 \tilde {A}_{32} \Pi _2 & D_3 & P_3
       \tilde {A}_{34} \Pi _4\\P_4 \tilde {A}_{41} \Pi _1 & P_4 \tilde {A}_{42} \Pi _2 & P_4 \tilde {A}_{43}
       \Pi _3 & D_4
\end{bmatrix} \]
Here:

- \(\tilde {A}_{i j}\) smaller than \(A_{i j}\)

- \(D_i\) has full rank (not necessarily diagonal)

- \(P_i\) shared for entire row

- \(\Pi _i\) shared for entire column

\medskip
*Q:* Why is it called that?

*** Block-Separable Matrix: Questions

*Q:* Why is it called that?
#+LATEX: \begin{hidden}
The word /separable/ arises because what low-rank representations do is (effectively) apply
`separation of variables', i.e. \(u (x, y) = v (x) w (y)\), just in the row/column indices.
#+LATEX: \end{hidden}

*Q:* How expensive is a matvec?
#+LATEX: \begin{hidden}
A matvec with a block-separable matrix costs \(O (N^{3 / 2})\) like the
single-level Barnes-Hut scheme.
#+LATEX: \end{hidden}

*Q:* How about a solve?
#+LATEX: \begin{hidden}
\(\rightarrow\) To do a solve, we need some more technology.
#+LATEX: \end{hidden}

*** BSS Solve (I)

Use the following notation:
\begin{equation*}
    B =
    \begin{bmatrix}
    0 & P_1 \tilde {A}_{12} & P_1 \tilde {A}_{13} & P_1 \tilde {A}_{14}\\
    P_2 \tilde {A}_{21} & 0 & P_2 \tilde {A}_{23} & P_2 \tilde {A}_{24}\\
    P_3 \tilde {A}_{31} & P_3 \tilde {A}_{32} & 0 & P_3 \tilde {A}_{34}\\
    P_4 \tilde {A}_{41} & P_4 \tilde {A}_{42} & P_4 \tilde {A}_{43} & 0
    \end{bmatrix}
\end{equation*}
and
\begin{equation*}
    D =
    \begin{bmatrix}
    D_1 &  &  & \\& D_2 &  & \\&  & D_3 & \\&  &  & D_4
    \end{bmatrix}, \quad \Pi =
    \begin{bmatrix}
    \Pi _1 &  &  & \\& \Pi _2 &  & \\&  & \Pi _3 & \\&  &  & \Pi _4
    \end{bmatrix} .
\end{equation*}
Then \(A = D + B \Pi\) and
\begin{equation*}
    \begin{bmatrix}
    D & B\\- \Pi & \tmop{Id}
    \end{bmatrix}
    \begin{bmatrix}
    \tmmathbf{x}\\\widetilde{\tmmathbf{x}}
    \end{bmatrix} =
    \begin{bmatrix}
    \tmmathbf{b}\\
    \tmmathbf{0}
    \end{bmatrix}
\end{equation*}

is equivalent to \(A \tmmathbf{x} = \tmmathbf{b}\).

*** BSS Solve (II)

*Q:* What are the matrix sizes? The vector lengths of \(\tmmathbf{x}\)
and \(\tmmathbf{\tilde {x}}\)?

#+LATEX: \begin{hidden}
(\( \Pi : \tmop{small} \times \tmop{large})\)
#+LATEX: \end{hidden}

Now work towards doing /just/ a `coarse' solve on \(\tmmathbf{\tilde
{x}}\), using, effectively, the \ Schur complement. Multiply first row
by \(\Pi D^{- 1}\), add to second:

#+LATEX: \begin{hidden}[4cm]
\begin{eqnarray*}
  \begin{bmatrix}
    \Pi \tmcolor{red}{D^{- 1} D} & \Pi D^{- 1} B\\- \Pi & \tmop{Id}
  \end{bmatrix}
  \begin{bmatrix}
    \tmmathbf{x}\\\widetilde{\tmmathbf{x}}
  \end{bmatrix} & = &
  \begin{bmatrix}
    \Pi D^{- 1} \tmmathbf{b}\\\tmmathbf{0}
  \end{bmatrix}\\
  \begin{bmatrix}
    \Pi D^{- 1} D & \Pi D^{- 1} B\\0 & \tmop{Id} + \Pi D^{- 1} B
  \end{bmatrix}
  \begin{bmatrix}
    \tmmathbf{x}\\
    \widetilde{\tmmathbf{x}}
  \end{bmatrix} & = &
  \begin{bmatrix}
    \tmmathbf{0}\\
    \Pi D^{- 1} \tmmathbf{b}
  \end{bmatrix}
\end{eqnarray*}
#+LATEX: \end{hidden}

*** BSS Solve (III)
Focus in on the second row:
\[(\tmop{Id} + \Pi D^{- 1} B) \widetilde{\tmmathbf{x}} = \Pi D^{- 1}
   \tmmathbf{b} \]
Every non-zero entry in \(\Pi D^{- 1} B\) looks like
#+LATEX: \begin{hidden}
\[\Pi _i D_i^{- 1} P_i \tilde {A}_{i j} . \]
#+LATEX: \end{hidden}
Define a diagonal entry:
#+LATEX: \begin{hidden}
\[\tilde {A}_{i i} = (\Pi _i D_i^{- 1} P_i)^{- 1} \]

The nomenclature makes (some) sense, because \(\widetilde{A_{}}_{i i}\) is a
`downsampled' version of \(D_i\) (with two inverses thrown in for good measure).
#+LATEX: \end{hidden}

*** BSS Solve (IV)
Next, left-multiply \((\tmop{Id} + \Pi D^{- 1} B)\) by \(\operatorname{diag}(\tilde A_{ii})\):
#+LATEX: \begin{hidden}[5cm]
\begin{equation*}
    \begin{bmatrix}
    \tilde {A}_{11} &  &  & \\
    & \tilde {A}_{22} &  & \\
    &  & \tilde {A}_{33} & \\
    &  &  & \tilde {A}_{44}
    \end{bmatrix} (\tmop{Id} + \Pi D^{- 1} B) =
    \underbrace{
    \begin{bmatrix}
    \tilde {A}_{11} & \tilde {A}_{12} & \tilde {A}_{13} & \tilde {A}_{14}\\
    \tilde {A}_{21} & \tilde {A}_{22} & \tilde {A}_{23} & \tilde {A}_{24}\\
    \tilde {A}_{31} & \tilde {A}_{32} & \tilde {A}_{33} & \tilde {A}_{33}\\
    \tilde {A}_{41} & \tilde {A}_{42} & \tilde {A}_{43} & \tilde {A}_{44}
    \end{bmatrix}}_{\tilde {A}} .
\end{equation*}
Summary: Need to solve
\[\tilde {A} \tmmathbf{\tilde {x}} = (\Pi _i D_i^{- 1} P_i)^{- 1} \tmmathbf{}
   \Pi D^{- 1} \tmmathbf{b} . \]
   #+LATEX: \end{hidden}

*** BSS Solve: Summary
**** Summary                                                       :B_column:
     :PROPERTIES:
     :BEAMER_col: 0.7
     :END:
 What have we achieved?

 - Instead of solving a linear system of size
   \[(N_{L 0 \tmop{boxes}} \cdot m) \times (N_{L 0 \tmop{boxes}} \cdot m) \]
   we solve a linear system of size
   \[(N_{L 0 \tmop{boxes}} \cdot K) \times (N_{L 0 \tmop{boxes}} \cdot K), \]
   which is cheaper by a factor of \((K / m)^3\).

 - We are now only solving on the skeletons.

**** Solving on the Skeletons                                      :B_column:
     :PROPERTIES:
     :BEAMER_col: 0.3
     :END:

# FIXME Add missing figure: find citable source
# #+ATTR_LATEX: :height 4cm
# [[./media/cluster-compress-single.pdf]]
(Figure credit: G. Martinsson)

*** Hierarchically Block-Separable

To get to \(O (N)\), realize we can \emph{recursively}

- group skeletons
- eliminate more variables.

Where does this process start?
#+LATEX: \begin{hidden}[5cm]
Start from the `most refined' level:

  #+ATTR_LATEX: :height 4cm
  [[./media/box-skeleton.pdf]]

\textcolor{red}{Coarser level skeletons} \(\cdot\) \textcolor{blue}{Finest skeletons}
#+LATEX: \end{hidden}

*** Hierarchically Block-Separable

In order to get \(O (N)\) complexity, could we apply this procedure recursively?
#+BEGIN_CENTER

# FIXME Add missing figure: find citable source
# #+ATTR_LATEX: :height 6cm
# [[./media/cluster-compress.pdf]

(Figure credit: G. Martinsson)
#+END_CENTER

*** Hierarchically Block-Separable
- Using this hierarchical grouping gives us

  /Hierarchically Block-Separable/ (/HBS/) matrices.

- If you have heard the word /\(\mathcal{H}\)-matrix/ and
  /\(\mathcal{H}^2\)-matrix/, the ideas are very similar. Differences:

  - \(\mathcal{H}\)-family matrices don't typically use the ID

    (instead often use /Adaptive Cross Approximation/ or /ACA/)

  - \(\mathcal{H}^2\) does target clustering (like FMM), \(\mathcal{H}\)
    does not (like Barnes-Hut)

*** Telescoping Factorization

#+BEGIN_CENTER
# FIXME Add missing figure: find citable source
# #+ATTR_LATEX: :width \textwidth
# [[./media/telescoping-factorization.pdf]]

(Figure credit: G. Martinsson)
#+END_CENTER

- The most decrease in `volume' happens in the off-diagonal part of the
  matrix. \(\rightarrow\) Rightfully so!

- All matrices are block-diagonal, except for the highest-level
  matrix--but that is small!

** The Butterfly Factorization
*** Recap: Fast Fourier Transform

The /Discrete Fourier Transform (DFT)/ is given by:
\[X_k = \sum_{n=0}^{N-1} x_n e^{-\frac{2\pi i}{N} nk}\quad(k=0,\dots,N-1)\]

The foundation of the /Fast Fourier Transform (FFT)/ is the factorization:
\begin{equation*}
 X_k= \underbrace{
  \sum \limits_{m=0}^{N/2-1} x_{2m}   e^{-\frac{2\pi i}{N/2} mk}}_{\mathrm{DFT\;of\;even-indexed\;part\;of\;} x_n}
 +  e^{-\frac{2\pi i}{N}k} \underbrace{
   \sum \limits_{m=0}^{N/2-1} x_{2m+1} e^{-\frac{2\pi i}{N/2} mk}}_{\mathrm{DFT\;of\;odd-indexed\;part\;of\;} x_n}.
\end{equation*}

*** FFT: Data Flow

#+ATTR_LATEX: :height 6cm
[[./media/fft-butterfly.png]]

Perhaps a little bit like a butterfly?

*** Fourier Transforms: A Different View

Claim:

#+BEGIN_QUOTE
The [numerical] rank of the normalized Fourier transform with kernel
$e^{i\gamma x t}$ is bounded by a constant times $\gamma$, at any
fixed precision $\epsilon$.
#+END_QUOTE

(i.e. rank is bounded by the area of the rectangle swept out by \(x\) and \(t\))

[[[https://doi.org/10.1016/j.acha.2009.08.005][O'Neil et al. `10]]]

\bigskip
\demo{Butterfly Factorization} (Part I)

*** Recompression: Making use of Area-Bounded Rank

How do rectangular submatrices get expressed so as to reveal their constant rank?
#+LATEX: \begin{hidden}[6cm]
#+BEGIN_CENTER
\begin{tikzpicture}[scale=1.25,thick]
  \draw (0,0) coordinate (orig-a) rectangle ++(0.25,1.5);
  \draw (orig-a)  ++ (0.5,0) coordinate (orig-b) rectangle ++(0.25,1.5);

  \draw [|-|] (orig-a) ++(0,-0.125) -- ++(0.25,0) node [pos=0.5,anchor=north] {$r$};
  \draw [|-|] (orig-b) ++(0,-0.125)-- ++(0.25,0) node [pos=0.5,anchor=north] {$r$};

  \node at (1,0.75) {$\to$};

  \draw (2,0) coordinate (merged) rectangle ++(0.5,1.5);
  \draw [dotted,gray] (merged) ++(0.25,0) -- ++(0,1.5);

  \node at (3,0.75) {$\to$};

  \draw (4,0) ++ (0,-0.25) coordinate (hacked-a) rectangle ++(0.5,0.75);
  \draw (hacked-a) ++(0,0.5+0.75) coordinate (hacked-b) rectangle ++(0.5,0.75);

  \draw [dotted,gray] (hacked-a) ++(0.25,0) -- ++(0,0.75);
  \draw [dotted,gray] (hacked-b) ++(0.25,0) -- ++(0,0.75);

  \node [anchor=west] at ($ (hacked-a) + (0.6,0.375) $) {$=$};
  \node [anchor=west] at ($ (hacked-b) + (0.6,0.375) $) {$=$};

  \draw (hacked-a) ++(1.25, 0) rectangle ++(0.25,0.75) ++(0.05, 0.05) rectangle ++(0.5, 0.25);
  \draw (hacked-b) ++(1.25, 0) rectangle ++(0.25,0.75) ++(0.05, 0.05) rectangle ++(0.5, 0.25);
\end{tikzpicture}

#+END_CENTER
#+LATEX: \end{hidden}
*** Observations

\demo{Butterfly Factorization} (Part II)

For which types of matrices is the Butterfly factorization guaranteed accurate?
#+LATEX: \begin{hidden}
All of them.
#+LATEX: \end{hidden}
For which types of $n\times n$ matrices does the butterfly lead to a reduction in cost?
#+LATEX: \begin{hidden}
"Matrices for which \(p\times q\) subblocks have rank proportional to \(p q/n\)."
[[[https://doi.org/10.1016/j.acha.2009.08.005][O'Neil et al. `10]]]
#+LATEX: \end{hidden}
Explore the limit cases of the characterization.
#+LATEX: \begin{hidden}
Reducing \(p\) to 1 leads to a rank of one, which doesn't make sense. Instead, the
claim needs to be viewed as \(n\to \infty\).
#+LATEX: \end{hidden}
*** Observations: Cost

What is the cost (in the reduced-cost case) of the matvec?
#+LATEX: \begin{hidden}[4cm]
- Level 0: \(P_{0,k}\) have size \(r\times n/2^L\)
- Level \(\ell\): \(P_{\ell,i,j}\) have size \(r\times 2r\)
- Postprocess: \(B_{L,j}\) have size \(n/(2^L)\times r\)
#+LATEX: \end{hidden}

Comments?
#+LATEX: \begin{hidden}
In the typical case, even Level 0 will not be so oversampled that the
cost of applying is substantially less than \(O(n^2)\).
#+LATEX: \end{hidden}

* Outlook: Building a Fast PDE Solver
  :PROPERTIES:
  :RELATE_TREE_SECTION_NAME: outlook_pde
  :RELATE_TREE_SECTION_OPENED: true
  :END:

*** PDEs: Simple Ones First, More Complicated Ones Later

\begin{tabular}{p{6.0cm}p{6.0cm}}
  \tmtextbf{Laplace} & \tmtextbf{Helmholtz}\\\(\triangle u = 0\) & \(\triangle u + k^2 u = 0\)\\
  \begin{itemize}
    \item Steady-state \(\partial _t u = 0\) of wave propagation, heat conduction
  \end{itemize}

  \begin{itemize}
    \item Electric potential \(u\) for applied voltage

        \item Minimal surfaces/``soap films''

        \item \(\nabla u\) as velocity of incompressible flow
  \end{itemize} &
  \begin{itemize}
    \item Assume time-harmonic behavior \(\tilde {u} = e^{\pm i \omega t} u (x)\)
        in time-domain wave equation:
        \[\partial _t^2  \tilde {u} = \triangle \tilde {u} \]
        \item Sign in \(\tilde {u}\) determines direction of wave: Incoming/outgoing
        if free-space problem

        \item \tmtextit{Applications:} Propagation of sound, electromagnetic waves
  \end{itemize}
\end{tabular}

*** Fundamental Solutions


\begin{tabular}{p{6.0cm}p{6.0cm}c}
  \tmtextbf{Laplace} & \tmtextbf{Helmholtz} & \\
  \(- \triangle u = \delta\)  & \(\triangle u + k^2 u = \delta\) & \\
  \includegraphics[height=1.5cm]{media/laplace-monopole.png} &
    \includegraphics[height=1.5cm]{media/helmholtz-monopole.png}  &
    Monopole\\
  \includegraphics[height=1.5cm]{media/laplace-dipole.png} &
    \includegraphics[height=1.5cm]{media/helmholtz-dipole.png}  & Dipole\\
  & \includegraphics[height=1.5cm]{media/helmholtz-quadrupole.png}  &
    Quadrupole
\end{tabular}

aka. /Free space Green's Functions/

How do you assign a precise meaning to the statement with the \(\delta\)-function?
#+LATEX: \begin{hidden}[1cm]
Multiply by a test function, integrate by parts.
#+LATEX: \end{hidden}

*** Green's Functions

Why care about Green's functions?
#+LATEX: \begin{hidden}[3cm]
If you know them, they make
solving the PDE simple:
\[\triangle G = \delta \quad \Rightarrow \quad \triangle (G \ast f) = (f \ast
   \delta ) = f, \]
i.e. \(G \ast f\) is the solution to free-space Poisson \(\triangle u = f\).
#+LATEX: \end{hidden}

What is a non-free-space Green's function? I.e. one for a specific domain?
#+LATEX: \begin{hidden}[3cm]
One that satisfies \(\Delta G = \delta\) \emph{and} a boundary
condition.
#+LATEX: \end{hidden}

*** Green's Functions (II)

Why not just use domain Green's functions?
#+LATEX: \begin{hidden}[3cm]
We don't know them! (for general domains)
#+LATEX: \end{hidden}

What if we don't know a Green's function for our PDE... at all?
#+LATEX: \begin{hidden}[3cm]
Use a known one that works for the highest-order derivative parts of the PDE.
#+LATEX: \end{hidden}

*** Fundamental Solutions

\begin{tabular}{p{6.0cm}p{6.0cm}c}
  \tmtextbf{Laplace} & \tmtextbf{Helmholtz} & \\\[G (x) =
  \begin{cases}
    \frac{1}{- 2 \pi } \log |x| & \text{2D}\\
    \frac{1}{4 \pi }  \frac{1}{|x|} & \text{3D}
  \end{cases} \]& \[G (x) =
  \begin{cases}
    \frac{i}{4} H^1_0 (k|x|) & \text{2D}\\
    \frac{1}{4 \pi }  \frac{e^{ik |x|}}{|x|} & \text{3D}
  \end{cases} \]& Monopole\\
  \[\frac{\partial }{\partial _x} G (x) \]& \[\frac{\partial }{\partial _x} G
       (x) \]& Dipole
\end{tabular}

*** Layer Potentials (I)

\begin{align*}
  (S_k \sigma ) (x) & \assign \int _\Gamma  G_k  (x - y) \sigma (y) ds_y\\(S_k' \sigma ) (x) & \assign n \cdot \nabla _x PV \int _\Gamma  G_k  (x - y)
    \sigma (y) ds_y\\(D_k \sigma ) (x) & \assign PV \int _\Gamma  n \cdot \nabla _y G_k  (x - y)
    \sigma (y) ds_y\\(D_k' \sigma ) (x) & \assign n \cdot \nabla _x f.p. \int _\Gamma  n \cdot
    \nabla _y G_k  (x - y) \sigma (y) ds_y
\end{align*}

- \(G_k\) is the Helmholtz kernel (\(k = 0\) \(\rightarrow\) Laplace)

- Operators--map function \(\sigma\) on \(\Gamma\) to...

  - ...function on \(\mathbb{R}^n\)

  - ...function on \(\Gamma\) (in particular)

*** Layer Potentials (II)

- Alternate (``standard'') nomenclature:

  | Ours   | Theirs |
  |--------+--------|
  | \(S\)  | \(V\)  |
  | \(D\)  | \(K\)  |
  | \(S'\) | \(K'\) |
  | \(D'\) | \(T\)  |

- \(S''\) (and higher) analogously

- Called /layer potentials/:

  - \(S\) is called the /single-layer potential/

  - \(D\) is called the /double-layer potential/

- (Show pictures using =pytential/examples/layerpot.py=, observe
  continuity properties.)

*** How does this actually solve a PDE?

Solve a (interior Laplace Dirichlet) BVP, \(\partial \Omega = \Gamma\)
\[\triangle u = 0 \quad \text{in } \Omega , \qquad u|_\Gamma  = f|_\Gamma  .
\]

1. Pick /representation/:
  \[u (x) \assign (S \sigma ) (x) \]
1. Take (interior) limit onto \(\Gamma\):
  \[u|_\Gamma  = S \sigma \]
1. Enforce BC:
  \[u|_\Gamma  = f \]
1. Solve resulting linear system:
  \[S \sigma = f \]
  (quickly--using the methods we've developed: It is precisely of the form
  that suits our fast algorithms!)

1. Obtain PDE solution in \(\Omega\) by evaluating representation

*** IE BVP Solve: Observations (I)

*Observations:*

- One can choose representations relatively freely. Only constraints:

  - Can I get to the solution with this representation?

    I.e. is the solution I'm looking for represented?

  - Is the resulting integral equation solvable?

  *Q:* How would we know?

*** IE BVP Solve: Observations (II)

- Some representations lead to better integral equations than others.
  The one above is actually terrible (both theoretically and
  practically).

  Fix above: Use \(u (x) = D \sigma (x)\) instead of \(u (x) = S \sigma      (x)\).

  *Q:* How do you tell a good representation from a bad one?

- Need to actually \emph{evaluate} \(S \sigma (x)\) or \(D \sigma      (x)\)...

  *Q:* How?

\(\rightarrow\) Need some theory

* Going Infinite: Integral Operators and Functional Analysis
  :PROPERTIES:
  :RELATE_TREE_SECTION_NAME: func_ana
  :RELATE_TREE_SECTION_OPENED: true
  :END:

** Norms and Operators

*** Norms

#+LATEX: \begin{definition}
(Norm) A /norm/ \(\|\cdot \|\) maps an element of a /vector space/ into \([0, \infty )\). It satisfies:

- \(\|x\|= 0 \Leftrightarrow x = 0\)

- \(\|\lambda x\|= | \lambda | \|x\|\)

- \(\|x + y\|\le \|x\|+ \|y\|\) (triangle inequality)

#+LATEX: \end{definition}

Can create norm from /inner product/: \(\|x\|= \sqrt{\langle x, x \rangle }\)

*** Function Spaces

Name some function spaces with their norms.
#+LATEX: \begin{hidden}[5cm]

\begin{tabular}{l|p{12.0cm}}
  \(C (\Omega )\) & \(f\) continuous, \(\|f\|_\infty  \assign \sup _{x \in \Omega }     |f (x) |\)\\\(C^k (\Omega )\) & \(f\) \(k\)-times continuously differentiable\\\(C^{0, \alpha } (\Omega )\) & \(\|f\|_\alpha  \assign \|f\|_\infty  + \sup _{x     \ne y}  \frac{|f (x) - f (y) |}{|x - y|^\alpha }\) (\(\alpha \in (0, 1)\))\\\(C_L (\Omega )\) & \(|f (x) - (y) | \le L \|x - y\|\)\\\(L^p (\Omega )\) & \(\|f\|_p \assign \sqrt[p]{\int _D |f (x) |^p dx} < \infty\)

    \(L^2 \tmop{special} \tmop{because} ?\)
\end{tabular}
#+LATEX: \end{hidden}

*** Convergence

Name some ways in which a sequence can `converge'.
#+LATEX: \begin{hidden}[6cm]

#+LATEX: \begin{definition}[Convergent sequence]

    \(x_n \to x : \Leftrightarrow \|x_n - x\|\to 0\) \(\qquad\) ``convergence in
    norm''
#+LATEX: \end{definition}

#+LATEX: \begin{definition}[Cauchy sequence]

    For all \(\epsilon > 0\) there exists an \(n\) for which \(\|x_\nu  - x_\mu      \|\le \epsilon\) for \(\mu , \nu \ge n\)
#+LATEX: \end{definition}

(Convergence without known limit!)

#+LATEX: \begin{definition}[Complete/``Banach'' space]

    Cauchy \(\Rightarrow\) Convergent
#+LATEX: \end{definition}

*Q:* Counterexample?
#+LATEX: \end{hidden}

*** Operators

\(X, Y\): Banach spaces, \(A : X \to Y\) linear operator

#+LATEX: \begin{definition}[Operator norm]
\(\|A\|\assign \sup \{\|Ax\|: x \in X, \norm{x} = 1\}\)
#+LATEX: \end{definition}

#+LATEX: \begin{theorem}
\(\|A\|\) bounded \(\Leftrightarrow\) \(A\) continuous
#+LATEX: \end{theorem}

Other facts?
#+LATEX: \begin{hidden}
- The set of bounded linear operators is itself a Banach space: \(L (X,     Y)\)
- \(\|Ax\|\le \|A\|\|x\|\)
- \(\|BA\|\le \|B\|\|A\|\)
#+LATEX: \end{hidden}

- What does `linear' mean here?
- Is there a notion of `continuous at \(x\)' for linear operators?

*** Operators: Examples

Which of these is bounded as an operator on functions on the real
line?

- Multiplication by a scalar
- ``Left shift''
- Fourier transform
- Differentiation
- Integration
- Integral operators

#+LATEX: \begin{hidden}
Need to know spaces (norms really) to answer that!
#+LATEX: \end{hidden}

*** Integral Equations: Zoology

| Volterra                                | Fredholm                              |
|-----------------------------------------+---------------------------------------|
| \(\int _a^x k (x, y) f (y) dy = g (x)\) | \(\int _G k (x, y) f (y) dy = g (x)\) |

| First kind                            | Second Kind                                  |
|---------------------------------------+----------------------------------------------|
| \(\int _G k (x, y) f (y) dy = g (x)\) | \(f (x) + \int _G k (x, y) f (y) dy = g(x)\) |

Questions:

- First row: First or second kind?

- Second row: Volterra or Fredholm?

- Matrix (i.e. finite-dimensional) analogs?

- What can happen in 2D/3D?

- Factor allowable in front of the identity?

- Why even talk about `second-kind operators'?

  - Throw a \(+ \delta (x - y)\) into the kernel, back to looking like
    first kind. So?

  - Is the identity in \((I + K)\) crucial?

*** Connections to Complex Variables

Complex analysis is /full/ of integral operators:

- Cauchy's integral formula:
  \[f (a) = \frac{1}{2 \pi i}  \oint _\gamma  \frac{1}{z - a} f (z)
       \hspace{0.17em} dz \]
- Cauchy's differentiation formula:
  \[f^{(n)} (a) = \frac{n!}{2 \pi i}  \oint _\gamma  \frac{1}{(z - a)^{n +
       1}} f (z)  \hspace{0.17em} dz \]

*** Integral Operators: Boundedness (=Continuity)

#+LATEX: \begin{theorem}[Continuous kernel $\Rightarrow$  bounded]

    \(G \subset \mathbb{R}^n\) closed, bounded (``compact''), \(K \in C (G^2)\).
    Let
    \[(A \phi ) (x) \assign \int _G K (x, y) \phi (y) dy. \]
    Then
    \[\|A\|_\infty  = \max _{x \in G}  \int _G |K (x, y) | dy. \]
#+LATEX: \end{theorem}

Show `\(\leqslant\)'.

*** Solving Integral Equations

Given
\[(A \phi ) (x) \assign \int _G K (x, y) \varphi (y) dy, \]
are we allowed to ask for a solution of
\[(\tmop{Id} + A) \varphi = g ? \]
#+LATEX: \begin{hidden}[3cm]
Will see three attempts to answer that,
in roughly historical order:

- Neumann

- Riesz

- Fredholm
#+LATEX: \end{hidden}

*** Attempt 1: The Neumann series

Want to solve
\[\varphi - A \varphi = (I - A) \varphi = g. \]
Formally:
\[\varphi = (I - A)^{- 1} g. \]
What does that remind you of?
#+LATEX: \begin{hidden}[3cm]

\[\sum _{k = 0}^\infty  \alpha ^k = \frac{1}{1 - \alpha } \]
Only works if \(| \alpha | < 1  \)!
#+LATEX: \end{hidden}

*** Attempt 1: The Neumann series (II)

#+LATEX: \begin{theorem}
\(A : X \to X\) Banach, \(\|A\|< 1\)
\(\ds(I - A)^{- 1} = \sum _{k = 0}^\infty  A^k \)
with \(\|(I - A)^{- 1} \|\le 1 / (1 -\|A\|)\).
#+LATEX: \end{theorem}

- How does this rely on completeness/Banach-ness?

- There's an iterative procedure hidden in this.

  (Called /Picard Iteration/. Cf: Picard-Lindelöf theorem.)

  /Hint:/ How would you compute \(\sum _k A^k f\)?

*Q:* Why does this fall short?

#+LATEX: \begin{hidden}[3cm]

- \(\norm{A} \leqslant 1\) is /way/ to restrictive a condition.

- \(\rightarrow\) We'll need better technology.

- *Biggest Q:* If Cauchy sequences are too weak a tool to deliver a
  limit, where else are we going to get one?
#+LATEX: \end{hidden}

** Compactness

*** Compact Sets

#+LATEX: \begin{definition}[Precompact/Relatively compact]

  \(M \subseteq X\) precompact\(: \Leftrightarrow\) all sequences \((x_k) \subset    M\) contain a subsequence converging in \(X\)
#+LATEX: \end{definition}

#+LATEX: \begin{definition}[Compact/`Sequentially complete']

  \(M \subseteq X\) compact\(: \Leftrightarrow\) all sequences \((x_k) \subset M\)
  contain a subsequence converging in \(M\)
#+LATEX: \end{definition}

- Precompact \(\Rightarrow\) bounded
- Precompact \(\Leftrightarrow\) bounded (finite dim. only!)

*** Compact Sets (II)

Counterexample to `precompact \(\Leftrightarrow\) bounded'? (\(\infty\) dim)
#+LATEX: \begin{hidden}[4cm]
Looking for a bounded set where not
every sequence contains a convergent subsequence. \(\rightarrow\) Make use of
the fact that there are infinitely many `directions' (dimensions).\medskip

Precompactness `replaces' boundedness in \(\infty\) dim (because boundedness is
`not strong enough')
#+LATEX: \end{hidden}

*** Compact Operators

\(X, Y\): Banach spaces

#+LATEX: \begin{definition}[Compact operator]

    \(T : X \to Y\) is /compact/ \(: \Leftrightarrow\) \(T (\text{bounded     set})\) is precompact.
#+LATEX: \end{definition}

#+LATEX: \begin{theorem}

- \(T, S\) compact \(\Rightarrow\) \(\alpha T + \beta S\) compact

- /One of/ \(T, S\) compact \(\Rightarrow\) \(S \circ T\) compact

- \(T_n\) all compact, \(T_n \to T\) in operator norm \(\Rightarrow\) \(T\)
      compact

#+LATEX: \end{theorem}

Questions:

- Let \(\mathrm{dim} T (X) < \infty\). Is \(T\) compact?

- Is the identity operator compact?

*** Intuition about Compact Operators

- Compact operator: As finite-dimensional as you're going to get in
  infinite dimensions.

- Not clear yet--but they are moral (\(\infty\)-dim) equivalent of a
  matrix having /low numerical rank/.

- Are compact operators continuous (=bounded)?

- What do they do to high-frequency data?

- What do they do to low-frequency data?

*** Arzelà-Ascoli

Let \(G \subset \mathbb{R}^n\) be compact.

#+LATEX: \begin{theorem}[Arzel{\`a}-Ascoli]

    \(U \subset C (G)\) is precompact iff it is bounded and
    /equicontinuous/.
#+LATEX: \end{theorem}

*Equicontinuous* means

#+LATEX: \begin{hidden}[2cm]
For all \(x, y \in G\)

for all \(\epsilon > 0\) there exists a \(\delta > 0\) such that for all \(f \in U\)

if \(|x - y| < \delta\), then \(|f (x) - f (y) | < \epsilon\).\medskip
#+LATEX: \end{hidden}

*Continuous* means:

#+LATEX: \begin{hidden}[2cm]
For all \(x, y \in G\)

for all \(\epsilon > 0\) there exists a \(\delta > 0\) such that

if \(|x - y| < \delta\), then \(|f (x) - f (y) | < \epsilon\).\medskip
#+LATEX: \end{hidden}
*** Arzelà-Ascoli: Proof Sketch

#+LATEX: \begin{hidden}[6cm]
- Pick a dense seuqence in \(G\)
- Use a diagonal argument to find pointwise convergent subsequence
- Use equicontinuity to show uniform convergence
#+LATEX: \end{hidden}

*** Arzelà-Ascoli (II)

Intuition?
#+LATEX: \begin{hidden}
Equicontinuity prevents the functions from `running
away'.
#+LATEX: \end{hidden}

``Uniformly continuous''?
#+LATEX: \begin{hidden}
One \(\delta\) works for all \(x\).
#+LATEX: \end{hidden}

When does \emph{uniform continuity} happen?
#+LATEX: \begin{hidden}
Continuous on a closed and bounded (`compact') set.
#+LATEX: \end{hidden}

(Note: Kress LIE 2nd ed. defines `uniform equicontinuity' in one go.)

** Integral Operators

*** Integral Operators are Compact

#+LATEX: \begin{theorem}[Continuous kernel $\Rightarrow$  compact {\scriptsize [Kress LIE 2nd ed. Thm. 2.20]}]

    \(G \subset \mathbb{R}^m\) compact, \(K \in C (G^2)\). Then
    \[(A \phi ) (x) \assign \int _G K (x, y) \phi (y) dy. \]
    is compact on \(C (G)\).
#+LATEX: \end{theorem}

Use A-A. (a statement about compact /sets/) What is there to show?

Pick \(U \subset C (G)\). \(A (U)\) bounded?
#+LATEX: \begin{hidden}[1cm]
Yes, because the operator is bounded.
#+LATEX: \end{hidden}

\(A (U)\) equicontinuous?
#+LATEX: \begin{hidden}[1cm]
Yes: \(K\) /uniformly/
continuous on \(G \times G\) because \(G \times G\) compact.
#+LATEX: \end{hidden}

*** Weakly singular

\(G \subset \mathbb{R}^n\) compact

#+LATEX: \begin{definition}[Weakly singular kernel]

- \(K\) defined, continuous everywhere except at \(x = y\)

- There exist \(C > 0\), \(\alpha \in (0, n]\) such that
        \[|K (x, y) | \le C |x - y|^{\alpha - n}  \qquad (x \ne y) \]

#+LATEX: \end{definition}

#+LATEX: \begin{theorem}[Weakly singular kernel $\Rightarrow$  compact {\scriptsize [Kress LIE 2nd ed. Thm. 2.22]}]

\(K\) weakly singular. Then
\[(A \phi ) (x) \assign \int _G K (x, y) \phi (y) dy. \]
is compact on \(C (G)\), where \(\operatorname{cl}(G^\circ)=G\).
#+LATEX: \end{theorem}

*** Weakly singular: Proof Outline
Outline the proof of `Weakly singular kernel $\Rightarrow$  compact'.
#+LATEX: \begin{hidden}[6cm]

- \[\int|x-y|^{\alpha-n} \le \omega_n \int_0^d \rho^{\alpha-n}\rho^{n-1}d\rho=d^\alpha/\alpha \omega_n\]
- Show boundedness/existence as improper integral.

  (polar coordinates)

- Bleep out the singularity with a \(C^0\) PoU that shrinks with \(n\):
  \(A_n\)

- Each \(A_n\) compact by previous thm.

- Shrink singularity with \(n\). \(A_n\) converge uniformly (because of weak
  singularity).

- \(A\) is limit of compact operators.
#+LATEX: \end{hidden}

*** Weakly singular (on surfaces)

\(\Omega \subset \mathbb{R}^n\) bounded, open, \(\partial\Omega\) is \(C^1\) (what does that mean?)

#+LATEX: \begin{definition}[Weakly singular kernel (on a surface)]

- \(K\) defined, continuous everywhere except at \(x = y\)

- There exist \(C > 0\), \(\alpha \in (0, n - 1]\) such that
        \[|K (x, y) | \le C |x - y|^{\alpha - n + 1}  \qquad (x, y \in \partial
           \Omega , \hspace{0.27em} x \ne y) \]

#+LATEX: \end{definition}

#+LATEX: \begin{theorem}[Weakly singular kernel $\Rightarrow$  compact {\scriptsize [Kress LIE 2nd ed. Thm. 2.23]}]

\(K\) weakly singular on \(\partial \Omega\). Then
\(\ds(A \phi ) (x) \assign \int _{\partial \Omega } K (x, y) \phi (y) dy \)
is compact on \(C (\partial \Omega )\).
#+LATEX: \end{theorem}

*Q:* Has this estimate gotten worse or better?

** Riesz and Fredholm

*** Riesz Theory (I)

Still trying to solve
\[L \phi \assign (I - A) \phi = \phi - A \phi = f \]
with \(A\) compact.

#+LATEX: \begin{theorem}[First Riesz Theorem {\scriptsize [Kress, Thm. 3.1]}]

    \(N (L)\) is finite-dimensional.
#+LATEX: \end{theorem}

Questions:

- What is \(N (L)\) again?
- Why is this good news?

*** Riesz First Theorem: Proof Outline
Show it.
#+LATEX: \begin{hidden}[6cm]
Good news because each dimension in \(N (L)\) is an obstacle to
invertibility. Now we know that there's only `finitely many obstacles'.

Proof:

- \(N (L)\) closed. (Why?)

- \(L \phi = 0\) means what for \(A\)?

- When is the identity compact again?
#+LATEX: \end{hidden}

*** Riesz Theory (II)

#+LATEX: \begin{theorem}[Riesz theory {\scriptsize [Kress, Thm. 3.4]}]

    \(A\) compact. *Then:*

- \((I - A)\) injective \(\Leftrightarrow\) \((I - A)\) surjective

  - It's either bijective or neither s nor i.

- If \((I - A)\) is bijective, \((I - A)^{- 1}\) is bounded.

#+LATEX: \end{theorem}

Rephrase for solvability:
#+LATEX: \begin{hidden}
Sol. \((I - A) \varphi = 0\)
unique (\(\varphi = 0\)) $\Rightarrow$ \((I - A) \varphi = f\) unique sol.

A real existence result!
#+LATEX: \end{hidden}

Key shortcoming?
#+LATEX: \begin{hidden}
Gives out completely if there happens to be a nullspace.
#+LATEX: \end{hidden}
*** Riesz Theory: Boundedness Proof Outline

Assuming \((I - A)\) is bijective, show that \((I - A)^{- 1}\) is bounded.
#+LATEX: \begin{hidden}[6cm]
- Assume \(L^{-1}\) unbounded, so there exists a sequence \((f_n)\) with \(\norm{f_n}=1\) and \(\norm{L^{-1}f_n}\ge n\).
- Define \(g_n=f_n/\norm{L^{-1}f_n}\to 1\) and \(\phi_n=L^{-1} f_n/\norm{L^{-1}f_n}\)
- \(\phi_n-A\phi_n=g_n\)
- Pick a convergent subsequence out of \((A\phi_n)\), let \(A\phi_n\to \phi\)
- \(A\phi=\phi\), so \(\phi\in N(L)\).
#+LATEX: \end{hidden}

*** Hilbert spaces

Hilbert space: Banach space with a norm coming from an /inner product/:
\[(\alpha x + \beta y, z) = ? \]
\[(x, \alpha y + \beta z) = ? \]
\[(x, x) ? \]
\[(y, x) = ? \]
Is \(C^0 (G)\) a Hilbert space?
#+LATEX: \begin{hidden}
No, no inner product generates
\(\norm{\cdot }_\infty\).
#+LATEX: \end{hidden}

Name a Hilbert space of functions.
#+LATEX: \begin{hidden}
\(L^2 (\Omega )\) with
\[(f, g) = \int _\Omega  f \cdot g. \]
#+LATEX: \end{hidden}

*** Continuous and Square-Integrable
Can we carry over \(C^0 (G)\) boundedness/compactness results to \(L^2 (G)\)?

\medskip
\(X\), \(Y\) normed spaces with a scalar product
so that \(|(\phi,\psi)|\le\norm\phi \norm \psi\) for \(\phi,\psi\in X\).
#+LATEX: \begin{theorem}[Lax dual system {\scriptsize [Kress LIE 3rd ed. Thm. 4.13]}]
Let \(U\subseteq X\) be a subspace and let \(A:X\to Y\) and \(B:Y\to
X\) be bounded linear operators with
\[A\phi,\psi)=(\phi,B\psi)\qquad (\phi\in U, \psi\in Y).\]
Then \(A:U\to Y\) is bounded with respect to \(\norm{\cdot}_s\)
induced by the scalar product and \(\norm{A}_s^2\le \norm{A}\norm{B}\).
#+LATEX: \end{theorem}

Based on this, it is also possible to carry over compactness results.
*** Adjoint Operators

#+LATEX: \begin{definition}[Adjoint oeprator]

    \(A^\ast\) called adjoint to \(A\) if
    \[(Ax, y) = (x, A^\ast  y) \]
    for all \(x, y\).
#+LATEX: \end{definition}

Facts:

#+LATEX: \begin{tcolorbox}

- \(A^\ast\) unique

- \(A^\ast\) exists

- \(A^\ast\) linear

- \(A\) bounded \(\Rightarrow\) \(A^\ast\) bounded

- \(A\) compact \(\Rightarrow\) \(A^\ast\) compact

#+LATEX: \end{tcolorbox}

*** Adjoint Operator: Observations?
What is the adjoint operator in finite dimensions? (in matrix
representation)
#+LATEX: \begin{hidden}[1cm]
The transpose.
#+LATEX: \end{hidden}

What do you expect to happen with integral operators?
#+LATEX: \begin{hidden}[1cm]
Sources and targets swap roles.
#+LATEX: \end{hidden}

Adjoint of the single-layer?
#+LATEX: \begin{hidden}[1cm]
Itself. (`self-adjoint')
#+LATEX: \end{hidden}

Adjoint of the double-layer?
#+LATEX: \begin{hidden}[1cm]
\(S'\)
#+LATEX: \end{hidden}

*** Fredholm Alternative

#+LATEX: \begin{theorem}[Fredholm Alternative {\scriptsize [Kress LIE 2nd ed. Thm. 4.14]}]

    \(A : X \to X\) compact. *Then either:*

- \(I - A\) and \(I - A^\ast\) are bijective

    *or:*

- \(\mathrm{dim} N (I - A) = \mathrm{dim} N (I - A^\ast )\)

- \((I - A) (X) = N (I - A^\ast )^\perp\)

- \((I - A^\ast ) (X) = N (I - A)^\perp\)

#+LATEX: \end{theorem}

Seen these statements before?
#+LATEX: \begin{hidden}[1cm]
Fundamental thm of linear algebra \(\rightarrow\) next slide
#+LATEX: \end{hidden}

*** Fundamental Theorem of Linear Algebra

#+BEGIN_CENTER

  #+ATTR_LATEX: :height 7cm
  [[./media/fundamental-thm-of-la.pdf]]
#+END_CENTER

*** Fredholm Alternative in IE terms

Translate to language of integral equation solvability:
#+LATEX: \begin{hidden}[6cm]

- *Either* \(\varphi (x) - \int K (x, y) \varphi (y) = 0\) and
  \(\psi (x) - \int K (y, x) \psi (y) = 0\) have only the trivial solution and
  their inhom counterparts are uniquely solvable,

- *or* the homogeneous and inhomogeneous int.eqs. have the same
  finite number of lin.indep. solutions. In particular, the inhom equations
  \begin{eqnarray*}
    \varphi (x) - \int K (x, y) \varphi (y) & = & f (x)\\\psi (x) - \int K (y, x) \psi (y) & = & g (x)
  \end{eqnarray*}
  are solvable iff

  - \(f \perp \psi\) for all solutions \(\psi\) of \((I - A^\ast ) \psi =         0\),

  - \emph{or} \(g \perp \varphi\) for all solutions \(\varphi\) of \((I -         A) \varphi = 0\).

#+LATEX: \end{hidden}

*** Fredholm Alternative: Further Thoughts

What about symmetric kernels (\(K (x, y) = K (y, x)\))?
#+LATEX: \begin{hidden}
\(A = A^\ast\).
#+LATEX: \end{hidden}

Where to get uniqueness?
#+LATEX: \begin{hidden}
Will work on that.
#+LATEX: \end{hidden}

** A Tiny Bit of Spectral Theory

*** Spectral Theory: Terminology

\(A : X \to X\) bounded, \(\lambda\) is a \dots value:

#+LATEX: \begin{definition}[Eigenvalue]

    There exists an element \(\phi \in X\), \(\phi \ne 0\) with \(A \phi = \lambda      \phi\).
#+LATEX: \end{definition}

#+LATEX: \begin{definition}[Regular value]

    The ``resolvent'' \((\lambda I - A)^{- 1}\) exists and is bounded.
#+LATEX: \end{definition}

Can a value be regular and ``eigen'' at the same time?
#+LATEX: \begin{hidden}[1cm]
No: eigen means that \((\lambda I - A)\) has a nullspace, so there isn't an inverse.
#+LATEX: \end{hidden}

What's special about \(\infty\)-dim here?
#+LATEX: \begin{hidden}[1cm]
Not all non-regular values are eigen.
#+LATEX: \end{hidden}

*** Resolvent Set and Spectrum

#+LATEX: \begin{definition}[Resolvent set]
\(\rho (A) \assign \{\lambda \text{ is regular} \}\)
#+LATEX: \end{definition}

#+LATEX: \begin{definition}[Spectrum]
\(\sigma (A) \assign \mathbb{C} \setminus \rho (A)\)
#+LATEX: \end{definition}

*** Spectral Theory of Compact Operators

#+LATEX: \begin{theorem}
\(A : X \to X\) compact linear operator, \(X\) \(\infty\)-dim.

*Then:*

- \(0 \in \sigma (A)\) (show! )

- \(\sigma (A) \setminus \{0\}\) consists /only/ of eigenvalues

- \(\sigma (A) \setminus \{0\}\) is at most countable

- \(\sigma (A)\) has no accumulation point except for 0

#+LATEX: \end{theorem}

*** Spectral Theory of Compact Operators: Proofs

Show the first part.
#+LATEX: \begin{hidden}
If \(0 \not \in  \sigma (A)\), then, \(A^{- 1}\) exists and is bounded. Then \(I = AA^{- 1}\) is compact.
#+LATEX: \end{hidden}

Show second part.
#+LATEX: \begin{hidden}
By Riesz, nullspaces and non-invertibility (of \(\lambda I - A\)) coincide.
#+LATEX: \end{hidden}

*** Spectral Theory of Compact Operators: Implications

Rephrase last two: how many eigenvalues with \(| \cdot | \ge  R\)?
#+LATEX: \begin{hidden}[1cm]
Finitely many
#+LATEX: \end{hidden}

*Recap:* What do compact operators do to high-frequency data?
#+LATEX: \begin{hidden}[1cm]
Dampen it.
#+LATEX: \end{hidden}

Don't confuse \(I - A\) with \(A\) itself!
#+LATEX: \begin{hidden}[1cm]
For example: \(\mathrm{dim} N (A)\) vs \(\mathrm{dim} N (I - A)\)
#+LATEX: \end{hidden}

* Singular Integrals and Potential Theory
  :PROPERTIES:
  :RELATE_TREE_SECTION_NAME: potential_theory
  :RELATE_TREE_SECTION_OPENED: true
  :END:

** Singular Integrals
*** Recap: Layer potentials

\begin{align*}
  (S \sigma ) (x) & \assign \int _\Gamma  G (x - y) \sigma (y) ds_y\\(S' \sigma ) (x) & \assign PV \hspace{0.27em} \hat {n} \cdot \nabla _x
    \int _\Gamma  G (x - y) \sigma (y) ds_y\\(D \sigma ) (x) & \assign PV \int _\Gamma  \hat {n} \cdot \nabla _y G (x - y)
    \sigma (y) ds_y\\(D' \sigma ) (x) & \assign f.p. \hspace{0.27em} \hat {n} \cdot \nabla _x
    \int _\Gamma  \hat {n} \cdot \nabla _y G (x - y) \sigma (y) ds_y
\end{align*}

#+LATEX: \begin{definition}[Harmonic function]

    \(\triangle u = 0\)
#+LATEX: \end{definition}

Where are layer potentials harmonic?
#+LATEX: \begin{hidden}[1cm]
Away from the boundary.
#+LATEX: \end{hidden}

*** On the double layer again

Is the double layer \emph{actually} weakly singular? *Recap:*

#+LATEX: \begin{definition}[Weakly singular kernel]

- \(K\) defined, continuous everywhere except at \(x = y\)

- There exist \(C > 0\), \(\alpha \in (0, n - 1]\) such that
        \[|K (x, y) | \le C |x - y|^{\alpha - n + 1}  \qquad (x, y \in \partial
           \Omega , \hspace{0.27em} x \ne y) \]

#+LATEX: \end{definition}

*** Actual Singularity in the Double Layer

\[\frac{\partial }{\partial _x} \log (|0 - x|) = \frac{x}{x^2 + y^2} \]
#+BEGIN_CENTER
  #+ATTR_LATEX: :height 3cm
  [[./media/laplace-dipole.png]]
#+END_CENTER

- Singularity with approach on \(y = 0\)?
- Singularity with approach on \(x = 0\)?

#+LATEX: \begin{hidden}[2cm]
So life is simultaneously worse and better than discussed.

How about 3D? (\(- x / |x|^3\))

Would like an analytical tool that requires `less' fanciness.
#+LATEX: \end{hidden}

*** Cauchy Principal Value

But I don't *want* to integrate across a
singularity! \(\rightarrow\) punch it out.\medskip

*Problem:* Make sure that what's left over is well-defined

\[\int _{- 1}^1 \frac{1}{x} dx \text{?} \]
#+LATEX: \begin{hidden}[4cm]
Not defined really.
\[PV \int _{- 1}^1 \frac{1}{x} dx \assign \lim _{\epsilon \to 0 +} \left (
   \int _{- 1}^{- \epsilon } \frac{1}{x} + \int _\epsilon ^1 \frac{1}{x} \right )
\]
*Q:* Slight wrinkle--Symmetry matters!
\[\tmcolor{red}{\tmop{NOPE} :} \int _{- 1}^{- 2 \epsilon } \frac{1}{x} +
   \int _\epsilon ^1 \frac{1}{x} \]
#+LATEX: \end{hidden}

*** Principal Value in $n$ dimensions

#+BEGIN_CENTER
\begin{tikzpicture}[scale=0.8]
  \draw [->] (0,-0.5) -- (0,2.5) node [right] {$y$} ;
  \draw [->] (-2.5,0) -- (2.5, 0) node [right] {$x$} ;

  \draw [thick] (-2,1) -- ++(4,0) node [right] {$\Gamma$};
  \draw [->] (-1,1) -- ++(0,0.5) node [above] {$n$};

  %\draw [blue ] (-2,1.05) node [above left,font=\small] {$(+)$ Integration Contour}
  %  -- (1-0.3,1.05) arc (180:0:0.3) -- ++(2-1-0.3,0) ;
  \fill (1,1) coordinate (x0) circle (0.05) node [above ] {$x_0$};

  \draw [blue] (-2,0.95) node [below left,font=\small,xshift=2cm]
    {Integration Contour}
    -- (1-0.3,0.95) arc (-180:0:0.3) -- ++(2-1-0.3,0) ;

  \draw (x0) -- +(180+70:0.35) ;
  \node at (x0) [anchor=north,xshift=1mm,font=\footnotesize] {$\varepsilon$};

\end{tikzpicture}
#+END_CENTER

Again: Symmetry matters!
#+LATEX: \begin{hidden}
Not an ellipse, not a potato, a circle. Sphere in 3D.
#+LATEX: \end{hidden}

What about even worse singularities?
#+LATEX: \begin{hidden}
``Hadamard finite part''

- HFP integrals: /hypersingular/
- CPV integrals: /singular/
#+LATEX: \end{hidden}

*** Recap: Layer potentials


\begin{align*}
  (S \sigma ) (x) & \assign \int _\Gamma  G (x - y) \sigma (y) ds_y\\
  (S' \sigma ) (x) & \assign \textcolor{red}{\tmop{PV}}  \hspace{0.27em} \hat {n}
    \cdot \nabla _x  \int _\Gamma  G (x - y) \sigma (y) ds_y\\
  (D \sigma ) (x) & \assign \textcolor{red}{\tmop{PV}}  \int _\Gamma  \hat {n}
    \cdot \nabla _y G (x - y) \sigma (y) ds_y\\
  (D' \sigma ) (x) & \assign \textcolor{red}{f.p.}  \hspace{0.27em} \hat {n} \cdot
    \nabla _x  \int _\Gamma  \hat {n} \cdot \nabla _y G (x - y) \sigma (y) ds_y
\end{align*}

*Important for us:* Recover `average' of interior and exterior limit
without having to refer to off-surface values.

** Green's Formula and Its Consequences
*** Green's Theorem

\(\Omega\) bounded
#+LATEX: \begin{theorem}[Green's Theorem {\footnotesize [Kress LIE 2nd ed. Thm 6.3]}]

    \[\int _\Omega  u \triangle v + \nabla u \cdot \nabla v = \int _{\partial
       \Omega } u (\hat {n} \cdot \nabla v) ds \]
    \[\int _\Omega  u \triangle v - v \triangle u = \int _{\partial \Omega } u
       (\hat {n} \cdot \nabla v) - v (\hat {n} \cdot \nabla u) ds \]
#+LATEX: \end{theorem}

If \(\triangle v = 0\) and \(u = 1\), then
\[\int _{\partial \Omega } \hat {n} \cdot \nabla v = ? \]
#+LATEX: \begin{hidden}

\[\int _\Omega  1 \underbrace{\triangle v}_0 - v \underbrace{\triangle 1}_0 =
   \int _{\partial \Omega } 1 (\hat {n} \cdot \nabla v) - v
   (\underbrace{\hat {n} \cdot \nabla 1}_0) ds \]
#+LATEX: \end{hidden}

*** Green's Formula

What if \(\triangle v = 0\) and \(u = G (|y - x|)\) in Green's second
identity?
\[\int _\Omega  u \triangle v - v \triangle u = \int _{\partial \Omega } u
       (\hat {n} \cdot \nabla v) - v (\hat {n} \cdot \nabla u) ds \]

Can you write that more briefly?
#+LATEX: \begin{hidden}
\[(S (\hat {n} \cdot \nabla u) - Du) (x) =u(x)\]
#+LATEX: \end{hidden}

*** Green's Formula (Full Version)
\(\Omega\) bounded
#+LATEX: \begin{theorem}[Green's Formula {\footnotesize [Kress LIE 2nd ed. Thm 6.5]}]

If \(\triangle u = 0\), then
\[(S (\hat {n} \cdot \nabla u) - Du) (x) =
\begin{cases}
  u (x) & x \in \Omega, \\
  \frac{u (x)}{2} & x \in \partial \Omega, \\
  0 & x \not\in \Omega.
\end{cases}  \]
#+LATEX: \end{theorem}

*** Green's Formula and Cauchy Data

Suppose I know `Cauchy data' (\(u|_{\partial \Omega } \),
\(\hat {n} \cdot \nabla u|_{\partial \Omega } \)) of \(u\). What can I
do?
#+LATEX: \begin{hidden}
Compute \(u\) anywhere.
#+LATEX: \end{hidden}

What if \(\Omega\) is an exterior domain?
#+LATEX: \begin{hidden}
No longer holds
#+LATEX: \end{hidden}

What if \(u = 1\)? Do you see any practical uses of this?
#+LATEX: \begin{hidden}
\(S\) term
disappears, \(- D 1\) is an indicator function for the domain \(D\). Indicator
functions can be useful, for example to set representations to zero where
they're invalid.
#+LATEX: \end{hidden}

*** Mean Value Theorem

#+LATEX: \begin{theorem}[Mean Value Theorem {\footnotesize [Kress LIE 2nd ed. Thm 6.7]}]
If \(\Delta u = 0\),
\(\ds u (x) = \overline{\int }_{B (x, r)} u (y) dy = \overline{\int }_{\partial
    B (x, r)} u (y) dy \)
#+LATEX: \end{theorem}

Define \(\overline{\int }\)?
#+LATEX: \begin{hidden}
\[\abs{\Omega } \assign \int _\Omega
   1 dx, \qquad \overline{\int }_\Omega  f (x) dx = \frac{1}{\abs{\Omega }}
   \int _\Omega  f (x) dx. \]
#+LATEX: \end{hidden}

Trace back to Green's Formula (say, in 2D):
#+LATEX: \begin{hidden}
\[u (x) = (S
   (\hat {n} \cdot \nabla u) - Du) (x) = \frac{1}{2 \pi } \log (r)
   \underbrace{\int _{\partial B} \hat {n} \cdot \nabla u}_0 - \frac{1}{2 \pi r}
   \int _{\partial B} u. \]
#+LATEX: \end{hidden}

*** Maximum Principle

#+LATEX: \begin{theorem}[Maximum Principle {\footnotesize [Kress LIE 2nd ed. 6.9]}]

    If \(\triangle u = 0\) on compact set \(\bar {\Omega }\):

    \(u\) attains its maximum on the boundary.
#+LATEX: \end{theorem}

Suppose it were to attain its maximum somewhere inside an open
set...
#+LATEX: \begin{hidden}
Then we'd be able to get the value there by averaging over
a neighborhood. \(\rightarrow\) some points there have to be as high or higher.

So boundaries are special.
#+LATEX: \end{hidden}

What do our /constructed/ harmonic functions (layer potentials) do there?
#+LATEX: \begin{hidden}
Good question \(\rightarrow\) next slide.
#+LATEX: \end{hidden}

*** Green's Formula at Infinity: Statement

\(\Omega \subseteq \mathbb{R}^n\) bounded, \(C^1\), connected boundary, \(\triangle  u = 0\) in \(\mathbb R^n\setminus\Omega\), \(u\) bounded

#+LATEX: \begin{theorem}[Green's Formula in the exterior {\footnotesize [Kress LIE 3rd ed. Thm 6.11]}]
\[(S_{\partial \Omega } (\hat {n} \cdot \nabla u) - D_{\partial \Omega } u)
   (x) + \tmcolor{red}{\tmop{PV}} u_\infty  = u (x) \]
for some constant \(u_\infty\). /Only/ for \(n = 2\),
\[u_\infty  = \frac{1}{2 \pi r}  \int _{|y| = r} u (y) ds_y . \]
#+LATEX: \end{theorem}

Realize the power of this statement:
#+LATEX: \begin{hidden}
/Every/ bounded harmonic function is representable as...
#+LATEX: \end{hidden}

*** Green's Formula at Infinity: Proof (1/4)

We will focus on \(\mathbb R^3\). WLOG assume \(0\in\Omega\). Let \(M=\norm{u}_{L^\infty(\mathbb R^n\setminus \bar\Omega)}\).

First, show \(\norm{\nabla u}\le 6M/\norm x\) for \(x\ge R_0\).
#+LATEX: \begin{hidden}[5cm]
Choose \(R_0\) so that \(B(0, R_0/2)^c \cap \Omega =\emptyset\) and assume \(\norm{x}\ge R_0\).

Since \(\partial_i u\) is also harmonic, we may apply the mean value theorem and Gauss's theorem:
\begin{equation*}
  \nabla u(x)
  = \frac 1{(4/3)\pi r^3} \int_{B(x,r)} \nabla u(y)dy
  = -\frac 3{4\pi r^3} \int_{\partial B(x,r)} \hat n(y) u(y)dy,
\end{equation*}
where \(\hat n(y)\) is the unit normal to \(\partial B(x,r)\) towards
the interior of the ball (hence the sign flip).
The second equality follows from Gauss's theorem by applying it to \(\vec v:= \vec e_i u\), where 
\(\vec e_i\) is the \(i\)th unit vector.

Choosing \(r=\norm{x}/2\) yields that
\[\norm{\nabla u(x)} \le \frac{3M}r=\frac {6M}{\norm{x}}.\]
#+LATEX: \end{hidden}
    
*** Green's Formula at Infinity: Proof (2/4)
    
Let \(x\in \mathbb R^3\setminus \bar\Omega\).
Let \(r\) be such that \(\bar \Omega \subset B(x,r)\). Apply Green's formula on /bounded/ domains to \(B(x,r)\setminus \bar \Omega\):
\begin{equation*}
   (S_{\partial \Omega } (\partial_n u) - D_{\partial \Omega } u) (x)
    + (S_{\partial B(x,r)} (\partial_n u) - D_{\partial B(x,r)} u) (x) = u(x).
\end{equation*}

Show \(S_{\partial B(x,r)} (\partial_n u) \to 0\) as \(r\to\infty\):
#+LATEX: \begin{hidden}[3cm]
Consider
\[0=\int_{\partial (B(x,r)\cap \bar \Omega^c)} (\partial_n u)(y) =\int_{\partial B(x,r) } (\partial_n u)(y) -\int_{\partial \Omega} (\partial_n u)(y) .\]

(The minus sign in the last term comes from the sign flip in the normal.) So
\[S_{\partial B(x,r)} (\partial_n u)
= \frac1{4\pi r} \int_{\partial B(x,r)} (\partial_n u)(y)
= \frac1{4\pi r} \int_{\partial \Omega} (\partial_n u)(y)\to 0.\]
#+LATEX: \end{hidden}

*** Green's Formula at Infinity: Proof (3/4)
It remains to bound the term
\[D_{\partial B(x,r)} u) (x) =\frac{4\pi}{r^2} \int_{\partial B (x,r)} u(y) dS_y.\]

Can we transplant that ball to the origin in some sense?
#+LATEX: \begin{hidden}[4cm]
\[u(x+y)-u(y) = \nabla u((1-\theta)y+\theta x)\cdot x\]
for some \(\theta \in [0,1]\), so that if \(y\) is sufficiently large,
\[\abs{u(x+y)-u(y)}\le \frac {6M\norm{x}} {\norm{y}-\norm{x}}.\]
So
\[\left|\frac{4\pi}{r^2} \int_{\partial B (x,r)} u(y) dS_y
-\frac{4\pi}{r^2} \int_{\partial B (0,r)} u(y) dS_y\right|\le \frac Cr.\]
#+LATEX: \end{hidden}

*** Green's Formula at Infinity: Proof (4/4)
    
Observe
\[\left|\frac{4\pi}{r^2} \int_{\partial B (0,r)} u(y) dS_y\right|\le 4\pi M.\]
Consider the sequence
\[\mu_n:=\frac{4\pi}{r_n^2} \int_{\partial B (0,r_n)} u(y) dS_y.\]
Because of its boundedness and sequential compactness of the bounding interval, 
out of a sequence of radii \(r_n\), we can pick a subsequence so that \((\mu_{n(k)})\)
converges. Call the limit \(u_\infty\).
    
*** Green's Formula at Infinity: Impact

Can we use this to bound \(u\) as \(x \rightarrow \infty\)?

Consider the behavior of the kernel as \(r \to \infty\). Focus on 3D for simplicity. (But 2D holds also.)
#+LATEX: \begin{hidden}
\[u (x) = u_\infty  + O \left ( \frac{1}{|x|} \right ) \]

(2D uses mean-0 property of \(\partial_n u\).)
#+LATEX: \end{hidden}

How about \(u\)'s derivatives?
#+LATEX: \begin{hidden}
\[\nabla u (x) = O \left (\frac{1}{|x|^{n - 1}} \right ) \]
#+LATEX: \end{hidden}

** Jump Relations
*** Jump relations:

#+BEGIN_CENTER
\begin{tikzpicture}
    \draw [->] (0,-1.5) -- (0,4.5) ;
    \draw [->] (-0.5,0) -- (4.5,0) node [right] {$r$} ;
    \draw [->] (-0.5,3) -- (4.5,3) node [right] {$r$} ;

    \draw [color=blue,domain=0:1] plot  (\x,1+3) ;
    \draw [color=blue,domain=1:4] plot  (\x,{1/\x+3})
    node [right] {$S\mu$} ;
    \fill [color=blue] (1,3+1) circle (0.05)
    node [above right] {$\mu$};
    \draw [dashed] (1,-1.5) -- (1,4.5) node [pos=0.5,right] {$\Gamma$};
    \draw [color=blue,domain=4:1] plot  (\x,{-1/\x^2})
    circle (0.05) node [below right] {$S'\mu$} ;
    \draw [color=blue,domain=0:1] plot  (\x,-0.025) circle (0.05) ;
    \fill [color=blue] (1,-0.5) circle (0.05);
\end{tikzpicture}
#+END_CENTER

*** Jump Relations: Mathematical Statement

Let \([X] = X_+ - X_-\). (Normal points towards ``+''=``exterior''.)

#+LATEX: \begin{theorem}[Jump Relations {\footnotesize {[Kress LIE 2nd ed. Thm. 6.14, 6.17,6.18]}}]
\vspace*{-3ex}
\begin{align*}
  [S \sigma ] & = 0\\
  \lim _{x \to x_0 \pm } (S' \sigma ) = \left ( S' \mp \frac{1}{2} I \right )
    (\sigma ) (x_0) \qquad \Rightarrow \qquad [S' \sigma ] & = - \sigma \\
  \lim _{x \to x_0 \pm } (D \sigma ) = \left ( D \pm \frac{1}{2} I \right )
    (\sigma ) (x_0) \qquad \Rightarrow \qquad [D \sigma ] & = \sigma \\
  [D' \sigma ] & = 0
\end{align*}
#+LATEX: \end{theorem}

Truth in advertising: Assumptions on \(\Gamma ?\)
#+LATEX: \begin{hidden}[1cm]
Needs to be \(C^2\), i.e. twice continuously differentiable.
#+LATEX: \end{hidden}

*** Jump Relations: Proof Sketch for SLP

Sketch the proof for the single layer.
#+LATEX: \begin{hidden}[6cm]

- Use same cut-off function approach as in proof for weakly singular
  \(\Rightarrow\) compact

- Single layer potential is uniform limit of continuous functions.
#+LATEX: \end{hidden}

*** Jump Relations: Proof Sketch for DLP

Sketch proof for the double layer.
#+LATEX: \begin{hidden}[6cm]

- Represent tgt point \(x\) near boundary as
  \[x = z + h \hat {n} (z) \]
  where \(z \in \Gamma\).

- DLP becomes
  \[D \sigma (x) = \sigma (z) D 1 (x) + D [\sigma - \sigma (z)] . \]
- Argument of \(D [\sigma - \sigma (z)]\) disappears as \(h \rightarrow 0\);
  exists as improper integral. Remains to mop up limit behavior.
#+LATEX: \end{hidden}

* Boundary Value Problems
  :PROPERTIES:
  :RELATE_TREE_SECTION_NAME: bvps
  :RELATE_TREE_SECTION_OPENED: true
  :END:

** Laplace

*** Boundary Value Problems: Overview


\begin{tabular}{l|p{6.0cm}|p{6.0cm}}
  & \tmtextbf{Dirichlet} & \tmtextbf{Neumann}\\
  \hline
    \tmtextbf{Int.} & \(\lim _{x \to \partial \Omega -} u (x) = g\)

    {\plusball } unique & \(\lim _{x \to \partial \Omega -}  \hat {n} \cdot \nabla u     (x) = g\)

    {\okball }may differ by constant\\
  \hline
    \tmtextbf{Ext.} & \(\lim _{x \to \partial \Omega +} u (x) = g\)

    {\footnotesize \(u (x) =   \begin{cases}     O (1) & 2 D\\
  o (1) & 3 D   \end{cases}\) as \(|x| \to \infty\) }

    {\plusball }unique & \(\lim _{x \to \partial \Omega +}  \hat {n} \cdot \nabla u     (x) = g\)

    {\footnotesize \(u (x) = o (1)\) as \(|x| \to \infty\) }

    {\plusball }unique
\end{tabular}

with \(g \in C (\partial \Omega )\).

What does \(f (x) = O (1)\) mean? (and \(f (x) = o (1)\)?)
#+LATEX: \begin{hidden}
\[f (x) =
   O (g (x)) \Leftrightarrow \frac{f (x)}{g (x)} \leqslant C, \qquad f (x) = o
   (g (x)) \Leftrightarrow \frac{f (x)}{g (x)} \rightarrow 0. \]
#+LATEX: \end{hidden}

*** Uniqueness Proofs

Dirichlet uniqueness: why?
#+LATEX: \begin{hidden}[1cm]
(Hint: Maximum principle)
#+LATEX: \end{hidden}

Neumann uniqueness: why?
#+LATEX: \begin{hidden}[4cm]
Suppose two solutions exist and
difference \(\tilde {u} = u_1 - u_2\) is not constant. Then \(\nabla \tilde {u} \neq 0\) somewhere. Then:
\[\int _\Omega  u \triangle u + \nabla u \cdot \nabla v = \int _{\partial
   \Omega } u (\hat {n} \cdot \nabla v) ds \]
gives:
\[0 < \int _\Omega  \abs{\nabla \tilde {u}}^2 = \int _{\partial \Omega }
   \tilde {u}  (\underbrace{\hat {n} \cdot \nabla \tilde {u}}_0) ds = 0. \]
#+LATEX: \end{hidden}

*** Uniqueness: Remaining Points

Truth in advertising: Missing assumptions on \(\Omega\)?
#+LATEX: \begin{hidden}
Above works cleanly if boundary is \(C^2\), i.e. twice continuously differentiable.
#+LATEX: \end{hidden}

What's a DtN map?
#+LATEX: \begin{hidden}
Given Dirichlet data, find Neumann data. Possible!
#+LATEX: \end{hidden}

*Next mission:* Find IE representations for each.

*** Uniqueness of Integral Equation Solutions

#+LATEX: \begin{theorem}[Nullspaces {\footnotesize [Kress LIE 2nd ed. Thm 6.20]}]

- \(N (I / 2 - D) = N (I / 2 - S') = \{0\}\)

- \(N (I / 2 + D) = \mathrm{span} \{1\}\), \(N (I / 2 + S') =         \mathrm{span} \{\psi \}\),

        where \(\int \psi \ne 0\).

#+LATEX: \end{theorem}

*** IE Uniqueness: Proofs (1/3)

Show \(N (I / 2 - D) = \{0 \}.\)
#+LATEX: \begin{hidden}[6cm]

- Suppose \(\varphi / 2 - D \varphi = 0\). To show: \(\varphi = 0\).
- \(u (x) \assign D \varphi (x)\) is harmonic off \(\partial \Omega\), \(u^-     = D \varphi - \varphi / 2 = 0\).
- Because of interior Dirichlet uniqueness, \(u |_\Omega  = 0     \).
- \((\partial _n u)^+ = 0\) by the jump relations.
- \(u\) has the right decay at \(\infty\), so solves ext. Neumann problem
  (unique)
- \(u = 0\) everywhere.
- \(\varphi = u^+ - u^- = 0\).
#+LATEX: \end{hidden}

*** IE Uniqueness: Proofs (2/3)
Show \(N (I / 2 - S') = \{0 \}.\)
#+LATEX: \begin{hidden}
\(I / 2 - S' = I / 2 - D^\ast\), Fredholm alternative.
#+LATEX: \end{hidden}

*** IE Uniqueness: Proofs (3/3)
Show \(N (I / 2 + D) = \tmop{span} \{1 \}\).
#+LATEX: \begin{hidden}[4cm]
- Suppose \(\varphi / 2 + D \varphi = 0\). To show: \(\varphi\) constant
- \(u (x) \assign D \varphi (x)\) is harmonic off \(\partial \Omega\), \(u^+     = D \varphi + \varphi / 2 = 0\).
- Has right decay, so exterior Dirichlet uniqueness says \(u     |_{\bar {\Omega }^c} = 0 \).
- Jump relations for \(\partial _n u\) yield \((\partial _n u)^- = 0\).
- Interior Neumann `uniqueness' says \(u = \tmop{const}\) in \(\Omega\).
- Jump relations say \(\varphi = \tmop{const}\) on \(\partial \Omega\).
- ``\(\supseteq\)'': Can use Green's thm to show that D1+1/2=0.
#+LATEX: \end{hidden}

What conditions on the RHS do we get for int. Neumann and ext. Dirichlet?
#+LATEX: \begin{hidden}
\((I - A) (X) = N (I - A^\ast )^\perp\)
#+LATEX: \end{hidden}

\(\to\) ``Clean'' Existence for 3 out of 4.

*** Patching up Exterior Dirichlet

Problem: \(N (I / 2 + S') = \{\psi \}\)... do not know \(\psi\). Use different kernel:

\[\hat {n} \cdot \nabla _y G (x, y) \qquad \to \qquad \hat {n} \cdot \nabla _y G
   (x, y) + \frac{1}{|x|^{n - 2}} \]
Note: Singularity only at origin! (assumed \(\in \Omega\))

- 2D behavior? 3D behavior?
- Still a solution of the PDE? Compact?
- Jump condition? Exterior limit? Deduce \(u = 0\) on exterior.
  - Consider \(\partial_n G = O(1/r^{n-1})\).
- \(|x|^{n - 2} u (x) =\)? as \(|x|\to\infty\)?
- Thus \(\int \phi = 0\). Contribution of the second term?
- \(\phi / 2 + D \phi = 0\), i.e. \(\phi \in N (I / 2 + D) =\)?
- Existence/uniqueness?

\(\to\) Existence for 4 out of 4.

*** Domains with Corners

#+BEGIN_CENTER
  #+ATTR_LATEX: :height 5cm
  [[./media/drop-crop.pdf]]
#+END_CENTER

What's the problem? /(Hint: Jump condition for constant density)/

*** Domains with Corners (II)

At corner \(x_0\): (2D)
\[\lim _{x \to x_0 \pm } = \int _{\partial \Omega } \hat {n} \cdot \nabla _y G (x,
   y) \phi (y) ds_y \pm \frac{1}{2} \tmcolor{red}{\frac{\langle \text{opening
   angle on \(\pm\) side} \rangle }{\pi }} \phi \]
\(\to\) non-continuous behavior of potential on \(\Gamma\) at \(x_0\)

What space have we been living in? How do we fix this mess?

#+LATEX: \begin{hidden}
- Continuous functions
- \(I\) \(+\) Bounded (Neumann) \(+\) Compact (Fredholm)
- Use \(L^2\) theory

  (point behavior ``invisible'')
#+LATEX: \end{hidden}

Numerically: Needs consideration, can drive up cost through refinement.

** Helmholtz

*** Where does Helmholtz come from?

Derive the Helmholtz equation from the wave equation \(\partial _t^2 U = c^2 \triangle U, \). *Q:* What is \(c\)?
#+LATEX: \begin{hidden}[6cm]
($c$: Sound speed) Ansatz: \(U (x, t) = u (x) e^{- i \omega t}\). Plug in:
\begin{eqnarray*}
  u (x) \partial _t^2 [e^{- i \omega t}] & = & c^2 e^{- i \omega t} \triangle u (x)\\
  u (x) (- i \omega )^2 e^{- i \omega t} & = & c^2 e^{- i \omega t} \triangle u (x) \\
  - u (x) \omega ^2 & = & c^2 \triangle u (x)\\
  0 & = & c^2 \triangle u (x) + \omega ^2 u (x)\\
  & = & \triangle u (x) + \left ( \frac{\omega }{c} \right )^2 u (x)\\
  0 & = & \triangle u (x) + k^2 u (x)
\end{eqnarray*}
where \(k = \omega / c\) is called the /wave number/.
#+LATEX: \end{hidden}

*** Helmholtz vs. Yukawa
**** Helmholtz
     :PROPERTIES:
     :BEAMER_col: 0.5
     :END:
*Helmholtz Equation*

- \(\triangle u+k^2u(x)=0\)
- Indefinite operator
- Oscillatory solution
- Difficult to solve, especially for large \(k\)

**** Yukawa
     :PROPERTIES:
     :BEAMER_col: 0.5
     :END:
*Yukawa Equation*

- \(-\triangle u+k^2u(x)=0\)
- Positive definite operator
- Smooth solutions
- `Screened Coulomb' interaction
- Generally quite simple to solve

*** The prototypical Helmholtz BVP: A Scattering Problem

#+BEGIN_CENTER

    \begin{tikzpicture}
      \begin{scope}[scale=0.1,rotate=90,xshift=-40cm,yshift=-45cm]
      \filldraw [thick,fill=black!20]
      (48.28571,46.09336).. controls (48.42857,36.37908) and (38.62250,34.76570)..
      (36.00000,39.52193).. controls (33.50150,44.05327) and (27.12078,41.06037)..
      (25.71429,46.66479).. controls (24.50694,51.47567) and (29.71429,62.66479)..
      (36.00000,64.95050) coordinate (surf).. controls (42.28571,67.23622) and (42.28571,62.09336)..
      (42.00000,60.09336).. controls (41.71429,58.09336) and (36.63481,53.17284)..
      (38.85714,50.95050).. controls (44.28572,45.52193) and (48.14286,55.80765)..
      (48.28571,46.09336) -- cycle;
      \node at (40,45) {$\Omega$};
      \node at (45,65) {$\mathbb R^n\setminus \Omega$};
      \draw [thick,<-] (surf) -- ++(3,10) node [anchor=south] {$\Gamma$};
      \end{scope}

    \begin{scope}[rotate=10,yshift=-2.5cm,xshift=-0.5cm]
      \foreach \i in {1,2,3,4}
        \draw [thick] (-1.5,-\i*0.25) -- +(3,0) ;

      \draw [thick,->] (-1.0,-1.25) -- ++(0,1.25)
      node [pos=0.5,right=1mm,fill=white,opacity=0.8] {$u^{\text{inc}}$};
    \end{scope}

    \draw [thick,decorate,decoration={expanding waves}]
      (-.2,-1.5) -- ++(0.25,-1.15) ;
    \draw [thick,->]
      (-.2,-1.5) -- ++(0.25,-1.15)
      node [pos=0.5,right=0.1cm,fill=white,opacity=0.8]
      {$u$};
    \end{tikzpicture}
#+END_CENTER

Ansatz:
\[u^{\tmop{tot}} = u + u^{\tmop{inc}} \]
Solve for scattered field \(u\).

*** Helmholtz: Some Physics

Physical quantities:

- Velocity potential: \(U (x, t) = u (x) e^{- i \omega t}\)

  (fix phase by e.g. taking real part)

- Velocity: \(v = (1 / \rho _0) \nabla U\)

- Pressure: \(p = - \partial _t U = i \omega ue^{- i \omega t}\)

  - Equation of state: \(p = f (\rho )\)

What's \(\rho _0\)?
#+LATEX: \begin{hidden}
The wave equation is a linearization of (nonlinear) Euler, and \(\rho
_0\) is the `equilibrium' density about which we've linearized.
#+LATEX: \end{hidden}

What happens to a pressure BC as \(\omega \to 0\)?
#+LATEX: \begin{hidden}[1cm]
It disappears!
#+LATEX: \end{hidden}

*** Helmholtz: Boundary Conditions

Interfaces between media: What's continuous?
#+LATEX: \begin{hidden}[1cm]
Normal velocity, pressure.
#+LATEX: \end{hidden}

- *Sound-soft*: Scatterer ``gives''

  - Pressure remains constant in time

  - \(u = f\) \(\to\) Dirichlet

- *Sound-hard*: Scatterer ``does not give''

  - Pressure varies, same on both sides of interface

  - \(\hat {n} \cdot \nabla u = 0\) \(\to\) Neumann

- *Impedance*: Some pressure translates into motion

  - Scatterer ``resists''

  - \(\hat {n} \cdot \nabla u + ik \lambda u = 0\) \(\to\) Robin \((\lambda >         0)\)

- *Sommerfeld* radiation condition: allow only outgoing waves (\(n\)-dim)
  \[r^{\frac{n - 1}{2}}  \left ( \frac{\partial }{\partial r} - ik \right ) u
       (x) \to 0 \qquad (r \to \infty ) \]

Many interesting BCs \(\to\) many IEs! :)

*** Unchanged from Laplace

#+LATEX: \begin{theorem}[Green's Formula {\scriptsize [Colton/Kress IAEST Thm 2.1]}]

    If \(\triangle u + k^2 u = 0\), then
    \[(S (\hat {n} \cdot \nabla u) - Du) (x) =
\begin{cases}
  u (x) & x \in D\\
  \frac{u (x)}{2} & x \in \partial D\\
  0 & x \not\in D
\end{cases} \]
#+LATEX: \end{theorem}

\begin{align*}
  [Su] & = 0\\
  \lim _{x \to x_0 \pm } (S' u) = \left ( S' \mp \frac{1}{2} I \right ) (u) (x_0)
    \qquad \Rightarrow \qquad [S' u] & = - u\\
  \lim _{x \to x_0 \pm } (Du) = \left ( D \pm \frac{1}{2} I \right ) (u) (x_0)
    \qquad \Rightarrow \qquad [Du] & = u\\
  [D'u] & = 0
\end{align*}

*** Unchanged from Laplace

Why is singular behavior (esp. jump conditions) unchanged?
#+LATEX: \begin{hidden}
\(e^{ikr} = 1 + O (r)\) as \(r\to 0\)
#+LATEX: \end{hidden}

Why does Green's formula survive?

#+LATEX: \begin{hidden}
Remember Green's theorem:
\[\int _\Omega  u \triangle v - v \triangle u = \int _{\partial \Omega } u
   (\hat {n} \cdot \nabla v) - v (\hat {n} \cdot \nabla u) ds \]

#+LATEX: \end{hidden}

*** Resonances

\(- \triangle\) on a bounded (interior) domain with homogeneous
Dirichlet/Neumann BCs has countably many real, positive eigenvalues.

What does that have to with Helmholtz?
#+LATEX: \begin{hidden}
\[- \triangle u = \lambda
   u \]
\[\triangle u + k^2 u = 0 \]
#+LATEX: \end{hidden}

Why could it cause grief?
#+LATEX: \begin{hidden}
Non-uniqueness/nullspaces.
#+LATEX: \end{hidden}

*** Helmholtz: Boundary Value Problems

Find \(u \in C (\bar {D})\) with \(\triangle u + k^2 = 0\) such that


\begin{tabular}{l|p{6.0cm}|p{6.0cm}}
  & \tmtextbf{Dirichlet} & \tmtextbf{Neumann}\\
  \hline
    \tmtextbf{Int.} & \(\lim _{x \to \partial D -} u (x) = g\)

    {\okball }unique (\(-\)resonances) & \(\lim _{x \to \partial D -}  \hat {n} \cdot      \nabla u (x) = g\)

    {\okball }unique (\(-\)resonances)\\
  \hline
    \tmtextbf{Ext.} & \(\lim _{x \to \partial D +} u (x) = g\)

    Sommerfeld

    {\plusball }unique & \(\lim _{x \to \partial D +}  \hat {n} \cdot \nabla u (x) =     g\)

    Sommerfeld

    {\plusball }unique
\end{tabular}

with \(g \in C (\partial D)\).\medskip

Find layer potential representations for each.
#+LATEX: \begin{hidden}
First idea: Same as Dirichlet. But: (see next slide).
#+LATEX: \end{hidden}

*** Patching up resonances

*Issue:* Ext. IE inherits non-uniqueness from `adjoint' int. BVP\medskip

*Fix:* Tweak representation [Brakhage/Werner `65, ...]

(also called the /CFIE/ or /combined field integral equation/)
\[u = D \phi - i \alpha S \phi \]
(\(\alpha\): tuning knob \(\to\) \(1\) is fine, \(\sim k\) better for large
\(k\))\medskip

*** Patching up resonances: CFIE (1/3)

#+LATEX: \begin{hidden}[6cm]
For simplicity, we'll choose the the scaling
parameter \(\alpha = 1\), so that
\[u = D \varphi + iS \varphi . \]
The exterior Dirichlet BC yields the integral equation (by way of the jump
relations for \(S\) and \(D\)):
\[\frac{\varphi }{2} + D \varphi - iS \varphi = g. \]
Suppose \(\varphi / 2 + D \varphi - iS \varphi = 0\). We want to show \(\varphi = 0\).

#+LATEX: \end{hidden}

*** Patching up resonances: CFIE (2/3)

#+LATEX: \begin{hidden}[6cm]

From the IE, we conclude that \(\lim _+ u = 0\). Using exterior uniqueness, we
conclude that \(u = 0\) in the entire exterior, thus \(\lim _+ \hat {n} \cdot  \nabla u = 0\) also. The jump relations for the double and single layer then
give us

\begin{eqnarray*}
  0 - (\hat {n} \cdot \nabla u)^- = [\hat {n} \cdot \nabla u] & = & [\hat {n}
    \cdot \nabla (D \varphi - iS \varphi )] = - [iS' \varphi ] = i \varphi \\
  0 - u^- = u^+ - u^- = [u] & = & [D \varphi - iS \varphi ] = [D \varphi ] =
    \varphi
\end{eqnarray*}
#+LATEX: \end{hidden}

*** Patching up resonances: CFIE (3/3)
#+LATEX: \begin{hidden}[6cm]

Equating right the right hand sides, we get
\[- i (\hat {n} \cdot \nabla u)^- = u^- . \]
Green's first theorem \(\int u\triangle v + \nabla u\cdot\nabla v= \int_\partial u\partial_n v\) yields
\begin{align*}
  \underbrace{\int _\Omega  - k^2 | u |^2 + | \nabla u |^2}_{\in \mathbb R}
  = \int _\Omega  u \triangle \bar {u} + | \nabla u |^2
  &= \int _{\partial \Omega } u^-
   \overline{(\hat {n} \cdot \nabla u)^-} \mathd s\\
  &= - i \int _{\partial \Omega } | u^- |^2 \mathd s.
\end{align*}
Taking the imaginary part yields
\(\ds\int _{\partial \Omega } | u^- |^2 \mathd s = 0. \)

Using \(u^+ = u^- = 0\) and the jump relation for the double layer, we obtain
\(\varphi = 0\) as desired.
#+LATEX: \end{hidden}

*** Helmholtz Uniqueness
Uniqueness for remaining IEs similar:

#+LATEX: \begin{hidden}[6cm]

- Set RHS of IE to 0.
- Use uniqueness to get zero limit on one side.
- Use jump condition to get zero limit on other side.
- Go to ``other'' jump condition to get zero limit on other side.
- Use jump condition to show density \(= 0\).

\(\Rightarrow\) Existence for all four BVPs.
#+LATEX: \end{hidden}

** Calderón identities

*** A word about $D'$
Show that \(D'\) is self-adjoint. 
#+LATEX: {\scriptsize [Kress LIE 3rd ed. Sec 7.6]}
#+LATEX: \begin{hidden}[6cm]

- To show: \((D' \varphi , \psi ) = (\varphi , D' \psi )\)

- Introduce: \(u = D \varphi , \quad v = D \psi\)

- Green's second thm (here, in part thanks to Sommerfeld):
  \[\int _{\partial \Omega } (\hat {n} \cdot \nabla u) v = \int _{\partial
       \Omega } u (\hat {n} \cdot \nabla v) \]
- Then:
  \begin{eqnarray*}
    (D' \varphi , \psi )
    & = & (\hat {n} \cdot \nabla u, [v])\\
    & = & (u^+, \hat {n} \cdot \nabla v^+) - (u^-, \hat {n} \cdot \nabla v^-)\quad\text{(split jump)}\\
    & = & (u^+ - u^-, \hat {n} \cdot \nabla v)\\
    & = & (\varphi , D' \psi )
  \end{eqnarray*}
#+LATEX: \end{hidden}

*** Towards Calderón

Show that \((S \varphi , D' \psi ) = ((S' + I / 2) \varphi , (D - I / 2) \psi )\).
#+LATEX: \begin{hidden}
Let \(w:=S\phi\) and \(v=D\psi\).
\[(S\phi, D'\psi)=(w,\partial_n v) =(\partial_n w,v)=((S' + I / 2) \varphi , (D - I / 2) \psi ).\]
#+LATEX: \end{hidden}

\((\varphi , SD' \psi )\)?
#+LATEX: \begin{hidden}[4cm]
\vspace*{-5mm}
\begin{eqnarray*}
  &  & (\varphi , SD' \psi )\\
  & = & (S \varphi , D' \psi )\\
  & = & ((S' + I / 2) \varphi , (D - I / 2) \psi )\\
  & = & (\varphi , (D + I / 2) (D - I / 2) \psi )\\
  & = & (\varphi , (D + I / 2) (D - I / 2) \psi )\\
  & = & (\varphi , (D^2 - I / 4) \psi )
\end{eqnarray*}
#+LATEX: \end{hidden}

*** Calderón Identities: Summary

#+LATEX: \begin{tcolorbox}

- \(SD' = D^2 - I / 4\)

- \(D' S = S^{\prime 2} - I / 4\)

#+LATEX: \end{tcolorbox}

Also valid for Laplace (jump relation same after all!)\medskip

Why do we care?
#+LATEX: \begin{hidden}[3cm]
\(\to\) Exterior Neumann IE has \(D'\).
But: Hypersingular is yucky.

Right-precondition with a single layer.

\(\to\) /Calderón preconditioning/
#+LATEX: \end{hidden}

* Back from Infinity: Discretization
  :PROPERTIES:
  :RELATE_TREE_SECTION_NAME: discretization
  :RELATE_TREE_SECTION_OPENED: true
  :END:

** Fundamentals: Meshes, Functions, and Approximation

*** Numerics: What do we need?

- Discretize curves and surfaces
  - Interpolation
  - Grid management
  - Adaptivity
- Discretize densities
- Discretize integral equations
  - Nyström, Collocation, Galerkin
- Compute integrals on them
  - ``Smooth'' quadrature
  - Singular quadrature
- Solve linear systems

*** Constructing Discrete Function Spaces

#+BEGIN_CENTER

  Floating point numbers (/Degrees of Freedom/ or /DoFs/) \(\leftrightarrow\) Functions
#+END_CENTER

Discretization relies on three things:

- Base/reference domain
- Basis of functions
- Meaning of DoFs

Related finite element concept: /Ciarlet triple/

\medskip
Discretization options for a curve?
#+LATEX: \begin{hidden}[3cm]
- Equispaced
- Fourier modes (actually different from equispaced?)
- Piecewise polynomials
#+LATEX: \end{hidden}

*** What do the DoFs mean?

Common DoF choices:

- Point values of function
- Point values of (directional?) derivatives
- Basis coefficients
- Moments

Often: useful to have both ``modes'', ``nodes'', jump back and forth

*** Why high order?

Order \(p\): Error bounded as \(| u_h - u | \le Ch^p \)

Thought experiment:

| First order                            | Fifth order                         |
|----------------------------------------+-------------------------------------|
| 1,000 DoFs \(\approx\) 1,000 triangles | 1,000 DoFs \(\approx\) 66 triangles |
| Error: 0.1                             | Error: 0.1                          |
| Error: 0.01 \(\rightarrow\) ?          | Error: 0.01 \(\rightarrow\) ?       |

Complete the table.
#+LATEX: \begin{hidden}

| First                                      | Fifth                               |
|--------------------------------------------+-------------------------------------|
| 100,000 DoFs \(\approx\) 100,000 triangles | 1,800 DoFs\(\approx\) 120 triangles |

#+LATEX: \end{hidden}

Remarks:

- Want \(p \ge 3\) available.
- *Assumption:* Solution sufficiently smooth
- Ideally: \(p\) chosen by user

*** What is an Unstructured Mesh?

#+BEGIN_CENTER
  #+ATTR_LATEX: :height 2cm
  [[./media/unstructured-mesh-sample.png]]
#+END_CENTER

**** Reasons for unstructured                                      :B_column:
     :PROPERTIES:
     :BEAMER_col: 0.5
     :END:

Why have an unstructured mesh?
#+LATEX: \begin{hidden}[4cm]
- Adaptable to many engineering problems
- Deal with topology
- Deal with solution non-smoothness
- Adaptivity in space
- Adaptivity in time
#+LATEX: \end{hidden}

**** Cost of going unstructured                                    :B_column:
     :PROPERTIES:
     :BEAMER_col: 0.5
     :END:

What is the trade-off in going unstructured?
#+LATEX: \begin{hidden}[3cm]
- Complexity: Data structures, algorithms, generation
- Where do meshes come from?
- What is a `reference element'?
#+LATEX: \end{hidden}

\demo{CAD software}

*** Fixed-order vs Spectral

\begin{tabular}{p{6.0cm}|p{6.0cm}}
  Fixed-order & Spectral\\
  \hline
    Number of DoFs \(n\)

    \(\sim\)

    Number of `elements'
    \[\text{Error} \sim \frac{1}{n^p} \]
    Examples?

  \begin{itemize}
    \item Piecewise Polynomials
  \end{itemize} & Number of DoFs \(n\)

    \(\sim\)

    Number of modes resolved
    \[\text{Error} \sim \frac{1}{C^n} \]
    Examples?

  \begin{itemize}
    \item Global Fourier

        \item Global Orth. Polynomials
  \end{itemize}
\end{tabular}

What assumptions are buried in each of these?
#+LATEX: \begin{hidden}
Smoothness:
Piecewise vs. Global
#+LATEX: \end{hidden}

*** Fixed-order vs Spectral

What should the DoFs be?
#+LATEX: \begin{hidden}[3cm]
Natural DoF match:

- Fixed-order: point values
- Spectral: modal coefficients
#+LATEX: \end{hidden}

What's the difficulty with purely modal discretizations?
#+LATEX: \begin{hidden}[3cm]
Nonlinearities are hard to express.

\(\rightarrow\) Use point values to compute those. (/Pseudospectral methods/)
#+LATEX: \end{hidden}

*** Vandermonde Matrices

\begin{equation*}
\begin{bmatrix}
  x_0^0 & x_0^1 & \cdots & x_0^n\\
  x_1^0 & x_1^1 & \cdots & x_1^n\\
  \vdots & \vdots & \ddots & \vdots \\
  x_n^0 & x_n^1 & \cdots & x_n^n
\end{bmatrix}
\begin{bmatrix}
  a_0\\a_1\\\vdots \\a_n
\end{bmatrix} = \text{?}
\end{equation*}

*** Generalized Vandermonde Matrices

\begin{equation*}
\begin{bmatrix}
  \phi _0 (x_0) & \phi _1 (x_0) & \cdots & \phi _n (x_0)\\
  \phi _0 (x_1) & \phi _1 (x_1) & \cdots & \phi _n (x_1)\\
  \vdots & \vdots & \ddots & \vdots \\
  \phi _0 (x_n) & \phi _1 (x_n) & \cdots & \phi _n (x_n)
\end{bmatrix}
\begin{bmatrix}
  a_0\\
  a_1\\
  \vdots \\
  a_n
\end{bmatrix} = \text{?}
\end{equation*}

*** Generalized Vandermonde Matrices

\[
\begin{bmatrix}
  \phi _0 (x_0) & \phi _1 (x_0) & \cdots & \phi _n (x_0)\\
  \phi _0 (x_1) & \phi _1 (x_1) & \cdots & \phi _n (x_1)\\
  \vdots & \vdots & \ddots & \vdots \\
  \phi _0 (x_n) & \phi _1 (x_n) & \cdots & \phi _n (x_n)
\end{bmatrix} \tmop{MODAL} \tmop{COEFFS} = \tmop{NODAL} \tmop{COEFFS}
\]

\smallskip
- Node placement? \demo{Interpolation node placement}
- Vandermonde conditioning? \demo{Vandermonde conditioning}
- What about multiple dimensions? 
  - \demo{Visualizing the 2D PKDO Basis}
  - \demo{2D Interpolation Nodes}

*** Common Operations

(Generalized) Vandermonde matrices simplify common operations:

- Modal \(\leftrightarrow\) Nodal (``Global interpolation'')
  - Filtering
  - Up-/Oversampling

- Point interpolation (Hint: solve using \(V^T\))
- Differentiation
- Indefinite Integration
- Inner product
- Definite integration

*** Unstructured Mesh

#+BEGIN_CENTER

  #+ATTR_LATEX: :height 3cm
  [[./media/unstructured-mesh-sample.png]]
#+END_CENTER

- Design a data structure to represent this
- Compute normal vectors
- Compute area
- Compute integral of a function
- How is the function represented?

\demo{Working with Unstructured Meshes}
  
** Integral Equation Discretizations
*** Integral Equation Discretizations: Overview

\[\phi (x) - \int _\Gamma  K (x, y) \phi (y) dy = f (y) \]

\begin{tabular}{p{6.0cm}|p{6.0cm}}
  Nystr{\"o}m & Projection\\
  \hline

  \begin{itemize}
    \item Approximate integral by quadrature:

        \(\int _\Gamma  f (y) dy \to \sum _{k = 1}^n \omega _k f (y_k)\)

        \item Evaluate quadrature'd IE at quadrature nodes, solve
  \end{itemize} &
  \begin{itemize}
    \item Consider residual:

        \(R \assign \phi - A \phi - f\)

        \item Pick projection \(P_n\) onto finite-dimensional subspace

        \(P_n \phi \assign \sum _{k = 1}^n \langle \phi , v_k \rangle w_k\) \(\quad \to          \quad\) DOFs \(\langle \phi , v_k \rangle\)

        \item Solve \(P_n R = 0\)
  \end{itemize}
\end{tabular}

*** Projection/Galerkin

- Equivalent to projection: Test IE with test functions
- Important in projection methods: /sub/-space (e.g. of \(C(\Gamma )\))

Name some generic discrete projection bases.
#+LATEX: \begin{hidden}
- Galerkin: \(v_k = w_k\) (Commonly: polynomials)
- Collocation:

  \(v_k = \delta (x_k)\), \(w_k (x_j) = \delta _{jk}\)
- Petrov-Galerkin: \(v_k \ne w_k\) (Commonly: polynomials)
#+LATEX: \end{hidden}

Collocation and Nyström: the same?
#+LATEX: \begin{hidden}
No--collocation demands that the integrals be computed /exactly/.
#+LATEX: \end{hidden}

Are projection methods implementable?
#+LATEX: \begin{hidden}
No, not usually--the integrals need to be computed /exactly/.
#+LATEX: \end{hidden}

** Integral Equation Discretizations: Nyström
*** Nyström Discretizations (1/4)

Nyström consists of two distinct steps:

1. Approximate integral by quadrature:

  \begin{equation}
    \varphi _n (x) - \sum _{k = 1}^n \omega _k K (x, y_k) \varphi _n (y_k) = f (x)
        \label{eq:nystrom-quad}
  \end{equation}
1. Evaluate quadrature'd IE at quadrature nodes, solve discrete system

  \begin{equation}
    \varphi ^{(n)}_j - \sum _{k = 1}^n \omega _k K (x_j, y_k) \varphi ^{(n)}_k = f
        (x_j) \label{eq:nystrom-nodes}
  \end{equation}
  with \(x_j = y_j\) and \(\varphi ^{(n)}_j = \varphi _n (x_j) = \varphi _n (y_j)\)

Is version \eqref{eq:nystrom-quad} solvable?
#+LATEX: \begin{hidden}
No--still deals with functions in \(x\). Infinitely many
`rows', but only \(n\) `columns'.
#+LATEX: \end{hidden}

*** Nyström Discretizations (2/4)

What's special about \eqref{eq:nystrom-nodes}?
#+LATEX: \begin{hidden}
Density only known at point values

No continuous density
#+LATEX: \end{hidden}

/Solution/ density also only known at point values. But: can get approximate continuous density. How?
#+LATEX: \begin{hidden}
\[\tilde {\varphi } (x) = f (x) -
   \sum _{k = 1}^n \omega _k K (x, y_k) \varphi ^{(n)}_k . \]
#+LATEX: \end{hidden}

Assuming the IE comes from a BVP. Do we also only get the BVP solution at discrete points?
#+LATEX: \begin{hidden}
No: Using the (now discrete) representation, we can still evaluate the BVP solution anywhere.
#+LATEX: \end{hidden}

*** Nyström Discretizations (3/4)

Does \eqref{eq:nystrom-quad} \(\Rightarrow\) \eqref{eq:nystrom-nodes} hold?
#+LATEX: \begin{hidden}
Sure--if it's true for a function \(\varphi\), it should be true for point values of that function.
#+LATEX: \end{hidden}

Does \eqref{eq:nystrom-nodes} \(\Rightarrow\) \eqref{eq:nystrom-quad} hold?
#+LATEX: \begin{hidden}[4cm]
Actually--it must! \(\varphi _k^{(n)}\) are
point values of the density (since we satisfied \eqref{eq:nystrom-nodes}!),
and so the `approximate' \(\tilde {\varphi }\) was not so approximate after
all--it must be the function that solves \eqref{eq:nystrom-quad}.
#+LATEX: \end{hidden}

*** Nyström Discretizations (4/4)

What good does that do us?
#+LATEX: \begin{hidden}[4cm]
Goal: say something about error. I.e.: does the method work at all?

Point: much easier to examine error between \eqref{eq:nystrom-quad} and the
IE (than \eqref{eq:nystrom-nodes} and the IE)

Can stay in function space, no need to mess with varying dimensionality.
#+LATEX: \end{hidden}

Does Nyström work for first-kind IEs?
#+LATEX: \begin{hidden}
No. Specifically because backing out the density relies on second-kind.
#+LATEX: \end{hidden}

*** Convergence for Nyström (1/2)

Increase number of quadrature points \(n\):

Get sequence \((A_n)\)

Want \(A_n \to A\) in some sense

What senses of convergence are there for sequences of functions
\(f_n\)?
#+LATEX: \begin{hidden}

- pointwise
- uniform (`in the \(\norm{\cdot }_\infty\) norm')
- (and a few more)
#+LATEX: \end{hidden}

What senses of convergence are there for sequences of operators \(A_n\)?
#+LATEX: \begin{hidden}

- functionwise (the analog to `pointwise')
- uniform (in the operator norm)
#+LATEX: \end{hidden}

*** Convergence for Nyström (2/2)

Will we get norm convergence \(\|A_n - A\|_\infty \to 0\) for Nyström?
{\scriptsize [Kress LIE 2nd ed. Thm. 12.8]}]
#+LATEX: \begin{hidden}[3cm]
*No:* Pick \(\psi _\epsilon  = 1\) everywhere except in
\(\epsilon\)-nbh of quad nodes, 0 there.

Show:

- \(\|A \phi \psi _\epsilon  - A \phi \|_\infty  \to 0\) (\(\epsilon \to      0\))

- \(\|A - A_n \|_\infty  \ge \|A\|_\infty\)
#+LATEX: \end{hidden}

Is functionwise convergence good enough?
#+LATEX: \begin{hidden}[3cm]
No, not at all. When we're solving \(A \varphi = b\), we want all possible densities to be
roughly `equally far along' in convergence.

So neither notion of convergence really `works' for Nyström.

\(\rightarrow\) Compactness to the rescue.
#+LATEX: \end{hidden}

*** Compactness-Based Convergence

\(X\) Banach space (think: of functions)

#+LATEX: \begin{theorem}[Not-quite-norm convergence {\scriptsize [Kress LIE 2nd ed. Cor 10.4]}]

    \(A_n : X \to X\) bounded linear operators,

    functionwise convergent to \(A : X \to X\)

    Then convergence is uniform on compact subsets \(U \subset X\), i.e.
    \[\sup _{\phi \in U} \|A_n \phi - A \phi \|\to 0 \qquad (n \to \infty ) \]
#+LATEX: \end{theorem}

How is this different from norm convergence?
#+LATEX: \begin{hidden}
/Only on compact subsets of \(X\)!/
#+LATEX: \end{hidden}

*** Collective Compactness

Set \(\mathcal{A}\) of operators \(A : X \to X\)

#+LATEX: \begin{definition}[Collectively compact]
    \(\mathcal{A}\) is called /collectively compact/ if and only if

    for \(U \subset X\) bounded, \(\mathcal{A} (U)\) is relatively compact.
#+LATEX: \end{definition}

What was relative compactness (=precompactness)?
#+LATEX: \begin{hidden}
Has a convergent
subsequence.

(that doesn't necessarily converge in the set.)
#+LATEX: \end{hidden}

*** Collective Compactness: Questions (1/2)
    
Is each operator in the set \(\mathcal{A}\) compact?
#+LATEX: \begin{hidden}
Yes.
#+LATEX: \end{hidden}

Is collective compactness the same as ``every operator in \(\mathcal A\) is compact''?
#+LATEX: \begin{hidden}
No.
#+LATEX: \end{hidden}

*** Collective Compactness: Questions (2/2)

When is a sequence collectively compact?
#+LATEX: \begin{hidden}
The definition applies
to sequences-viewed-as-sets as is.
#+LATEX: \end{hidden}

Is the limit operator of such a sequence compact?
#+LATEX: \begin{hidden}
Yes.
#+LATEX: \end{hidden}

How can we use the two together?
#+LATEX: \begin{hidden}
- We'll have a sequence of operators \(A_n\) that's collectively compact.
- Then we get norm convergence on the range of the operators \(A\).
#+LATEX: \end{hidden}

*** Making use of Collective Compactness

\(X\) Banach space, \(A_n : X \to X\), \((A_n)\) collectively compact, \(A_n \to A\)
functionwise.

#+LATEX: \begin{corollary}[Post-compact convergence {\scriptsize [Kress LIE 3rd ed. Cor 10.11]}]

- \(\|(A_n - A) A\|\to 0\)

- \(\|(A_n - A) A_n \|\to 0\)

    \((n \to \infty )\)
#+LATEX: \end{corollary}

*** Anselone's Theorem

\((I - A)^{- 1}\) exists, with \(A : X \to X\) compact, \((A_n) : X \to X\)
collectively compact and \(A_n \to A\) functionwise.

#+LATEX: \begin{theorem}[Nystr{\"o}m error estimate {\scriptsize [Kress LIE 3rd ed. Thm 10.12]}]

    For sufficiently large \(n\), \((I - A_n)\) is invertible and
    \[\|\phi _n - \phi \|\le C (\|(A_n - A) \phi \|+\|f_n - f\|) \]
#+LATEX: \end{theorem}

\[C = \frac{1 + \|(I - A)^{- 1} A_n \|}{1 - \|(I - A)^{- 1} (A_n - A) A_n \|}
\]
\(I + (I - A)^{- 1} A = ?\)
#+LATEX: \begin{hidden}
\((I - A)^{- 1}\). (Idea: What would
happen for fractions?)
#+LATEX: \end{hidden}

*** Anselone's Theorem: Proof (I)

Define approximate inverse \(B_n = I + (I - A)^{- 1} A_n .\)

\medskip
How good of an inverse is it?
\footnotesize
\begin{eqnarray*}
  \tmop{Id} & \approx ^? & B_n (I - A_n)\\
  & = & (I + (I - A)^{- 1} A_n) (I - A_n)\\
  & = & [I + (I - A)^{- 1} A_n] - [A_n + (I - A)^{- 1} A_n A_n]\\
  & = & [I + (I - A)^{- 1} A_n] - [{\color{blue}(I - A)^{- 1} (I - A)} A_n
    + (I - A)^{- 1} A_n A_n]\\
  & = & [I + (I - A)^{- 1} A_n] - [{\color{blue}(I - A)^{- 1} I} A_n -
    {\color{blue}(I - A)^{- 1} A} A_n + (I - A)^{- 1} A_n A_n]\\
  & = & I + (I - A)^{- 1} \tmop{AA}_n - (I - A)^{- 1} A_n A_n\\
  & = & I + \underbrace{(I - A)^{- 1} (A - A_n) A_n}_{- S_n} = I - S_n
\end{eqnarray*}

*** Anselone's Theorem: Proof (II)
Want \(S_n \rightarrow 0\) somehow. Prior result gives us \(\norm{(A - A_n) A_n} \rightarrow 0\).

#+LATEX: \begin{hidden}[7cm]
So \(\norm{S_n} \rightarrow 0\).

Using Neumann series:
\[\norm{(I - S_n)^{- 1}} \leqslant \frac{1}{1 - \norm{S_n}} \]
if \(\norm{S_n} < 1\). In particular: The inverse exists!

Long story short from earlier:
\[B_n (I - A_n) = I - S_n, \]
So \(I - A_n\) must also be invertible. Rearrange:
\[(I - A_n)^{- 1} = (I - S_n)^{- 1} B_n, \]
#+LATEX: \end{hidden}

*** Anselone's Theorem: Proof (III)
#+LATEX: \begin{hidden}[5cm]

Let \(\varphi\) be the exact density that solves \((I - A) \varphi = f\) and
\(\varphi _n\) the approximate density that solves \((I - A_n) \varphi _n = f_n\).
Then consider

\begin{eqnarray*}
  (I - A_n) (\varphi _n - \varphi ) & = & f_n - (I - A_n) \varphi \\
  & = & f_n - (I - A) \varphi + (A - A_n) \varphi \\
  & = & f_n - f + (A - A_n) \varphi
\end{eqnarray*}
Combining all this knowledge as
\begin{eqnarray*}
  \|\varphi _n - \varphi \|& \leqslant & \|(I - A_n)^{- 1} \|(\|f_n - f\|+
    \|(A_n - A) \varphi \|)  \\
  & \leqslant & \frac{\|B_n \|}{1 - \|S_n \|} (\|f_n - f\|+\|(A_n - A)
    \varphi \|)
\end{eqnarray*}
gives the desired estimate.
#+LATEX: \end{hidden}

*** Anselone: A Question
Nyström: /specific to \(I + \text{compact}\)./ Why?
#+LATEX: \begin{hidden}
Used identity to fish out density more than once.
#+LATEX: \end{hidden}

*** Nyström: Collective Compactness

We /assumed/ collective compactness. Do we have that? Assume
\begin{equation}
  \sum | \text{quad. weights for \(n\) points} | \le C \qquad \text{(independent
    of \(n\))} \label{eq:bounded-quad-weights}
\end{equation}
#+LATEX: \begin{hidden}[4cm]
To use Arzelà-Ascoli, we'll need to show uniform boundedness
(easy!) and equicontinuity of the sequence \((A_n \varphi )\) for a given density
\(\varphi\). To show the latter, consider
\begin{eqnarray*}
  &  & | (A_n \varphi ) (x_1) - (A_n \varphi ) (x_2) |   \\
  & = & \left | \sum _{i = 1}^n \omega _i (K (x_1, y_i) \varphi (y_i) - K (x_2,
    y_i) \varphi (y_i)) |  \right .\\
  & \leqslant & \sum _{i = 1}^n | \omega _i | \underbrace{(K (x_1, y_i) - K
    (x_2, y_i))}_{(\ast )} \norm{ \varphi }_\infty  .
\end{eqnarray*}
\((\ast )\) bounded because \(K\) lives on a compact domain.
#+LATEX: \end{hidden}

*** Nyström: Collective Compactness

#+LATEX: \begin{hidden}[5cm]
And
\[\sum _{i = 1}^n | \omega _i |, \]
is bounded because we assumed it is. Since the constant doesn't depend on \(n\):
Collectively compact.
#+LATEX: \end{hidden}

Also assumed functionwise uniform convergence, i.e. \(\|A_n \phi - A \phi \|\to 0\) for each \(\phi\).
#+LATEX: \begin{hidden}
Follows from equicontinuity of \((A_n \phi )\).

Assumption \eqref{eq:bounded-quad-weights} is important to make all this work!
#+LATEX: \end{hidden}

** Integral Equation Discretizations: Projection

*** Projection Method
    
\(X\) Banach space, \(U\subset X\) nontrivial subspace, \(A:X\to Y\) injective,
\(X_n\subset X\), \(Y_n\subset Y\), \(\operatorname{dim} X_n = n\), \(\operatorname{dim} Y_n = n\), \(P_n:?\to ?\)
- \(P\) is a projection \(\Leftrightarrow\) \(P|_U=\operatorname{Id}\) \(\Leftrightarrow\) \(P^2=P\)
- \(\norm P \ge 1\)
- Orthogonal projectors: \(\norm P = 1\)
- Interpolators ("collocation projection"): Also projections
- *Projection method:* \(P_nA \phi_n=P_n f\) \((\#)\)

Define convergence:
#+LATEX: \begin{hidden}
*Convergent* if there exists \(n_0\in \mathbb N_0\) so that for \(n\ge n_0\)
- for each \(f\in A(X)\) \((\#)\) has a unique solution \(\phi_n\)
- \(\phi_n\to\phi\), where \(A\phi =f\).

I.e. functionwise convergence.
#+LATEX: \end{hidden}
*** Assumptions on the Approximation Spaces
    
What's needed of \(X_n\) so that it can even approximate the solution?
#+LATEX: \begin{hidden}
*Denseness*

\[\inf_{\psi\in X_n} \norm{\psi-\phi} \to 0\quad (n\to \infty)\]
#+LATEX: \end{hidden}

*** Error Estimates for Projection :noexport:

\(X\) Banach space, \(A : X \to X\) injective, \(P_n : Y \to Y_n\)

#+LATEX: \begin{theorem}[Céa's Lemma {\scriptsize [Kress LIE 2nd ed. Thm 13.6]}]

Convergence of the projection method \(\Leftrightarrow\)

There exist \(n_0\) and \(M\) such that for \(n \ge n_0\)

1. \(P_n A : X_n \to Y_n\) are invertible,

1. \(\|(P_n A)^{- 1} P_n A\|\le M\). (/Uniform Boundedness/, /Stability/)

In this case,
\[\|\phi _n - \phi \|\le (1 + M) \inf _{\psi \in X_n} \|\phi - \psi \| \]
#+LATEX: \end{theorem}

*** Céa's Lemma: Proof :noexport:
Proof?
#+LATEX: \begin{hidden}[4cm]

\[\phi_n-\phi = ((P_n A)^{- 1} P_n A - I)\phi\]

\(\Psi\in X_n\): \((P_n A)^{- 1} P_n A \psi=\psi\)

\[\phi_n-\phi = ((P_n A)^{- 1} P_n A - I)(\phi-\psi)\]

Build error estimate.

(``\(\Leftarrow\):'' Uniform Boundedness Principle)
#+LATEX: \end{hidden}

Core message of the theorem?
#+LATEX: \begin{hidden}[1cm]
Projection doesn't destroy invertibility.
#+LATEX: \end{hidden}

*** Céa's Lemma: Remarks :noexport:

Note domain of invertibility for \(P_n A\).
#+LATEX: \begin{hidden}
It's just \(X_n\)! Cannot
be all of \(X\), because the projection is not (assumed to be) injective.
#+LATEX: \end{hidden}

Domain/range of \((P_n A)^{- 1} P_n A\)?
#+LATEX: \begin{hidden}
Domain: All of \(X\).

Range: Reachable solutions for \(P_n A \varphi = f\).
#+LATEX: \end{hidden}

Relationship to conditioning?
#+LATEX: \begin{hidden}
\(\|(P_n A)^{- 1} P_n A\|\leqslant  \norm{(P_n A)^{- 1} } \norm{P_n A} = \kappa\)
#+LATEX: \end{hidden}

*** Norm Convergence of Inverses

\(X\), \(Y\) Banach spaces, \(A:X\to Y\) bounded, \(A^{-1}\) bounded
#+LATEX: \begin{theorem}[Norm Convergence of Inverses {\scriptsize [Kress LIE 3rd ed. Thm. 10.1]}]
If \(\norm{A_n-A}\to 0\) as \(n\to\infty\). Then for sufficiently large \(n\), \(A_n^{-1}\)
exists and is bounded by
\[ \norm{A_n^{- 1}} \leqslant \frac{\norm{A^{- 1}}}{1 - \norm{A^{- 1} (A_n -
   A)}} . \]
For $A \varphi = f$ and $A_n \varphi_n = f_n$, we have the estimate
\[ \norm{\varphi_n - \varphi} \leqslant \frac{\norm{A^{- 1}}}{1 - \norm{A^{-
   1} (A_n - A)}} \left[ \norm{(A_n - A) \varphi} + \norm{f_n - f} \right] .
\]
#+LATEX: \end{theorem}
*** Norm Convergence of Inverses: Proof

Prove the result:
#+LATEX: \begin{hidden}[5cm]
Note: $I - A^{- 1} (A_n - A) = A^{- 1} A_n$.

\medskip
Neumann series: If $\norm{A^{- 1} (A_n - A)} < 1$, then $[I - A^{- 1} (A_n -
A)]^{- 1}$ exists and
\[ \norm{[I - A^{- 1} (A_n - A)]^{- 1}} \leqslant \frac{1}{1 - \norm{A^{- 1}
   (A_n - A)}} . \]
But $[I - A^{- 1} (A_n - A)]^{- 1} A^{- 1} = A_n$, hence the bound.

\medskip
For the error estimate, consider $A_n (\varphi_n - \varphi) = f_n - f + (A - A_n) \varphi$.
#+LATEX: \end{hidden}

*** Projection Methods for Second Kind

Write out the projected version of the second-kind equation $\varphi - A
\varphi = f$:
#+LATEX: \begin{hidden}[4cm]
\[ P_n \varphi_n - P_n A \varphi_n = P_n f \]
Valid, but necessarily non-unique. Better (but distinct!):
\[ \varphi_n - P_n A \varphi_n = P_n f. \]
Each solution $\varphi_n \in X$ of this equation is automatically in $X_n$

\medskip
$\rightarrow$ better chance of uniqueness.

\medskip
(Error estimate connecting the two below!)

#+LATEX: \end{hidden}

*** Error Estimate for Second Kind Projection

$X$ Banach, $A : X \rightarrow X$ compact, $I - A$ injective

#+LATEX: \begin{theorem}[Second Kind Projection Estimate {\scriptsize [Kress LIE 3rd ed. Thm. 13.10]}]

Assume $\norm{P_n A - A} \rightarrow 0$ ($n \rightarrow \infty$). Then for
sufficiently large $n$,
\[ \varphi_n - P_n A \varphi_n = P_n f \]
is uniquely solvable for all $f \in X$, and we have $\norm{\varphi_n -
\varphi} \leqslant M \norm{P_n \varphi - \varphi}$ for $M$ a constant
depending on $A$.

#+LATEX: \end{theorem}

*** Error Estimate for Second Kind Projection: Proof

Prove the result:
#+LATEX: \begin{hidden}[5cm]
Riesz' theorem: $(I - A)$ invertible.

\medskip
Norm convergence of inverses: $(I - P_n A)^{- 1}$ exists and uniformly bounded
(if $n$ large enough).

\medskip
Consider $P_n$ applied to the continuous IE:
\begin{eqnarray*}
  P_n (\varphi - A \varphi) & = & P_n f\\
  \Leftrightarrow \quad \varphi - P_n A \varphi & = & P_n f + \varphi - P_n
  \varphi
\end{eqnarray*}
Subtract the latter from the projection method:
\[(I - P_n A) (\varphi_n - \varphi) = P_n \varphi - \varphi.\]
That and the uniform boundedness gives the error estimate.
#+LATEX: \end{hidden}

*** Perturbations of Projection Methods for Second Kind

In actual numerical use, we're not solving
\[ \varphi_n - P_n A \varphi_n = P_n f \]
but
\[ \tilde{\varphi}_n - P_n A_n \tilde{\varphi}_n = P_n f_n, \]
where

- $A_n$ approximates $A$,
- $f_n$ approximates $f$.

*** Perturbations of Projection Methods for Second Kind: Estimate
    
$X$ Banach, $A : X \rightarrow X$ compact, $I - A$ injective

#+LATEX: \begin{theorem}[SK Projection Perturbation {\scriptsize [Kress LIE 3rd ed. Cor. 13.11]}]

Assume that functionwise $P_n A_n - P_n A
\rightarrow 0$ and $\norm{P_n A_n - P_n A} \rightarrow 0$ ($n \rightarrow
\infty$). Then for sufficiently large $n$
\(\tilde{\varphi}_n - P_n A_n \tilde{\varphi}_n = P_n f_n \)
is uniquely solvable and for some positive constant $M$,
\[ \norm{\tilde{\varphi}_n - \varphi} \leqslant M \left( \norm{P_n \varphi -
   \varphi} + \norm{(P_n A_n - P_n A) \varphi_n} + \norm{P_n (f_n - f)}
   \right) . \]
#+LATEX: \end{theorem}

#+LATEX: \begin{hidden}[3cm]
Norm convergence of inverses: Existence and uniform boundedness of $(I - P_n
A_n)^{- 1}$ (from $(I - P_n A)^{- 1}$).

Error estimate from there:
\[ \norm{\tilde{\varphi}_n - \varphi_n} \leqslant C \left( \norm{(P_n A_n -
   P_n A) \varphi_n} + \norm{P_n (f_n - f)} \right) . \]
Use earlier estimate and uniform boundedness principle.
#+LATEX: \end{hidden}

*** Decisions, Decisions: Nyström or Galerkin? :noexport:

Quote Kress LIE, 2nd ed., p. 244 (Sec. 14.1):

#+LATEX: \begin{tcolorbox}
  \small
  [...] the Nyström method is generically stable whereas the collocation
  and Galerkin methods may suffer from instabilities due to a poor choice of
  basis for the approximating subspace.
#+LATEX: \end{tcolorbox}

Quote Kress LIE, 2nd ed., p. 244 (Sec. 13.5):

#+LATEX: \begin{tcolorbox}
  \small
  In principle, for the Galerkin method for equations of the second kind the
  same remarks as for the collocation method apply. As long as numerical
  quadratures are available, in general, the Galerkin method cannot compete in
  efficiency with the Nyström method.

  Compared with the collocation method, it is less efficient, since its matrix
  elements require double integrations.
#+LATEX: \end{tcolorbox}

Need good quadratures to use Nyström.

Remaining advantage of Galerkin: Can be made not to break for non-second-kind.

*** Iterative Methods and Corners [Bremer et al. `11]
#+BEGIN_CENTER
  #+ATTR_LATEX: :height 2cm
  [[./media/drop-crop.pdf]]
#+END_CENTER

*Problem:* Singular behavior at corner points. Density may blow up.

Can the density be convergent in the \(\|\cdot \|_\infty\) sense?

Conditioning of the discrete system?

GMRES will flail and break, because it sees \(\ell ^2 \sim l^\infty  \sim  L^\infty\) convergence.

Make GMRES `see' \(L^2\) convergence by redefining density DOFs:
\[\overline{\tmmathbf{\sigma }}_h \assign
\begin{bmatrix}
  \sqrt{\omega _1} \sigma (x_1)\\
  \vdots \\
  \sqrt{\omega _n} \sigma (x_n)
\end{bmatrix} = \sqrt{\tmmathbf{\omega }}  \tmmathbf{\sigma }_h \]
So \(\overline{\tmmathbf{\sigma }}_h \cdot \overline{\tmmathbf{\sigma }}_h =\)?

Also fixes system conditioning! Why?

* Computing Integrals: Approaches to Quadrature
  :PROPERTIES:
  :RELATE_TREE_SECTION_NAME: quadrature
  :RELATE_TREE_SECTION_OPENED: true
  :END:
** A Bag of Quadrature Tricks
*** `Off-the-shelf' ways to compute integrals

How do I compute an integral of a nasty singular kernel?

Symbolic integration
#+LATEX: \begin{hidden}
Good when it works.
#+LATEX: \end{hidden}

Why not Gaussian?
#+LATEX: \begin{hidden}[4cm]
Error estimate:
\[\abs{\int f - \sum f (x_i) \omega _i} \leqslant C \norm{f^{(p)}} h^p \]
\(\norm{f^{(p)}}\) blows up!
#+LATEX: \end{hidden}
*** Singular and Near-Singular Quadrature

Numerically distinct scenarios:

- Near-Singular quadrature
  - Integrand nonsingular
  - But may locally require lots of
  - Adaptive quadrature works, but...
- Singular quadrature
  - Integrand singular
  - Conventional quadrature fails

*** Kussmaul-Martensen quadrature

#+LATEX: \begin{theorem}[A special integral {\scriptsize [Kress LIE Lemma 8.21]}]

\begin{equation*}
  \frac{1}{2 \pi }  \int _0^{2 \pi } \log \left ( 4 \sin ^2  \frac{t}{2}
       \right ) e^{imt} dt =
  \begin{cases}
    0 & m = 0,\\
    - \frac{1}{|m|} & m = \pm 1, \pm 2 \ldots .
  \end{cases}
\end{equation*}
#+LATEX: \end{theorem}

Why is that exciting?

\demo{Kussmaul-Martensen quadrature}

*** Singularity Subtraction

\begin{gather*}
  \int \langle \text{Thing \(X\) you would like to integrate} \rangle \qquad \\
  = \int \langle \text{Thing \(Y\) you \tmem{can} integrate} \rangle \\
  \qquad + \int \langle \text{Difference \(X - Y\) which is easy to integrate
    (numerically)} \rangle
\end{gather*}

Give a typical application.
#+LATEX: \begin{hidden}
Helmholtz: \(H^{(1)}_0 (x) = \log (x) + \text{smooth}\)
#+LATEX: \end{hidden}

Drawbacks?
#+LATEX: \begin{hidden}
Two integrals to compute.
#+LATEX: \end{hidden}

*** High-Order Corrected Trapezoidal Quadrature

- Conditions for new nodes, weights

  (\(\to\) linear algebraic system, dep. on \(n\))

  to integrate
  \[\langle \text{smooth} \rangle \cdot \langle \text{singular} \rangle +
       \langle \text{smooth} \rangle \]
- Allowed singularities: \(|x|^\lambda\) (for \(| \lambda | < 1\) ), \(\log      |x|\)

- Generic nodes and weights for \(\log\) singularity

- Nodes and weights copy-and-pasteable from paper

[Kapur, Rokhlin `97]\medskip

Alpert `99 conceptually similar:
#+LATEX: \begin{hidden}

- Hybrid Gauss-Trapezoidal

- Positive weights

- Somewhat more accurate (empirically) than K-R

- Similar allowed singularities (\(\lambda > - 1\))

- Copy-paste weights
#+LATEX: \end{hidden}

*** Generalized Gaussian

- ``Gaussian'':

  - Integrates \(2 n\) functions exactly with \(n\) nodes

  - Positive weights

- Clarify assumptions on system of functions (``Chebyshev system'') for
  which Gaussian quadratures exist

- When do (left/right) singular vectors of integral operators give rise
  to Chebyshev systems?

  - In many practical cases!

- Find nodes/weights by Newton's method

  - With special starting point

- Very accurate

- Nodes and weights for download

[Yarvin/Rokhlin `98]

*** Singularity cancellation: Polar coordinate transform

\begin{gather*}
  \int \int _{\partial \Omega } K (\tmmathbf{x}, \tmmathbf{y}) \phi (y) ds_y\\
  =\\
  \int _0^R \int _{\tmmathbf{x} + \tmmathbf{r} \in \partial \Omega \cap \partial
    B (\mathbf{x}, r)} K (\tmmathbf{x}, \tmmathbf{x} + \tmmathbf{r}) \phi
    (\mathbf{x} + \mathbf{r}) d \langle \text{angles} \rangle \hspace{0.17em} r
    \hspace{0.17em} dr\\
  =\\
  \int _0^R \int _{\tmmathbf{x} + \tmmathbf{r} \in \partial \Omega \cap \partial
    B (\mathbf{x}, r)} \frac{K_{\text{less singular}}  (\tmmathbf{x},
    \tmmathbf{x} + \tmmathbf{r})}{r} \phi (\tmmathbf{x} + \tmmathbf{r}) d
    \langle \text{angles} \rangle \hspace{0.17em} r \hspace{0.17em} dr
\end{gather*}

where \(K_{\text{less singular}} = K \cdot r\).

*** Quadrature on Triangles

#+BEGIN_CENTER

    \begin{tikzpicture}
      \coordinate (a) at (-1.5, -1.5);
      \coordinate (b) at (1.5, -1.5);
      \coordinate (c) at (-1.5, 1.5);
      \coordinate (sing) at (-1, 0);

      \draw [thick] (a) -- (b) -- (c) --cycle;

      \fill [red] (sing) circle (2pt);
      \uncover<2->{%
        \draw [dotted, thick] (a)--(sing);
        \draw [dotted, thick] (b)--(sing);
        \draw [dotted, thick] (c)--(sing);
      }
    \end{tikzpicture}
#+END_CENTER

*Problem:* Singularity can sit /anywhere/ in triangle

\(\to\) need /lots/ of quadrature rules (one per target)

*** Kernel regularization

Singularity makes integration troublesome: /Get rid of it!/
\[\frac{\cdots }{\sqrt{(x - y)^2}} \quad \to \quad \frac{\cdots }{\sqrt{(x -
   y)^2 + \epsilon ^2}} \]
Use Richardson extrapolation to recover limit as \(\epsilon \to 0\).

(May also use geometric motivation: limit along line towards singular point.)

Primary drawbacks:

- Low-order accurate

- Need to make \(\epsilon\) smaller (i.e. kernel more singular) to get
  better accuracy

Can take many forms--for example:

- Convolve integrand to smooth it

  (\(\to\) remove/weaken singularity)

- Extrapolate towards no smoothing

Related: [Beale/Lai `01]

*** Acceleration and Quadature 

How can singular quadrature and FMM acceleration be made compatible?
#+LATEX: \begin{hidden}[4cm]
- FMM is a point-to-point algorithm: requires point discretization
  - Kussmaul-Martensen and FMM?
- If singular quadrature applies to all targets:
  - Simply feed \(\sigma(x_j)\cdot w_j\) to FMM as `charges'
- If singular quadrature applies only near singularity:
  - Exclude 'near sources' from FMM processing for each target
  - Inconvenient when 'near sources' have no relationship to FMM
  - Desperate: Carry out full FMM, subtract 'near source' contributions
#+LATEX: \end{hidden}

*** FMMs and other Layer Potentials
How does an FMM evaluate a double layer? 
#+LATEX: \begin{hidden}
Expand double layer kernel into `regular' expansions.
#+LATEX: \end{hidden}

How does an FMM evaluate \(S'\)?
#+LATEX: \begin{hidden}
Take derivatives \(\partial_x\) and \(\partial_y\) of the local expansion at the end.
#+LATEX: \end{hidden}

What effect does this have on accuracy?
#+LATEX: \begin{hidden}
Loses an FMM order.
#+LATEX: \end{hidden}

** Quadrature by expansion (`QBX')
*** Layer Potential Evaluation: Some Intuition
#+BEGIN_CENTER
#+ATTR_LATEX: :width 9cm
[[./media/trapz-underlying-error-nsrc50-crop.pdf]]
#+END_CENTER
*** QBX: Idea

#+BEGIN_CENTER
\begin{tikzpicture}
\node (pic) {\includegraphics[height=7cm]{./media/locexp-ball-order3-crop.pdf}};

%\uncover<+>{}
{
    \draw [ultra thick,<-] (pic.south west) ++(2.25,1.95) -- ++(1,-2)
    node [anchor=north] {Curve $\Gamma$};
    \draw [ultra thick,<-] (pic.south east) ++(-2.85,2.15) -- ++(0,-2.2)
    node [anchor=north] (quad-node-label) {Source quad. nodes $x'$};
    \draw [ultra thick,<-] (pic.south east) ++(-3.85,2) -- (quad-node-label);
    \draw [ultra thick,<-] (pic) ++(-1.2,-1.75) -- ++(-2,1)
    node [thick,anchor=east,fill=white] {Target point $x$};
}
{
    \draw [ultra thick,<-] (pic) ++(-1.2,0.5) -- ++(-2,1)
    node [thick,anchor=east,fill=white] {Expansion center};
    \node at ($(pic)+(-1.2,1.75)$)
    [fill=white,opacity=0.75,text width=2.5cm]
    {Potential from expansion};
    \node at ($(pic)+(1.4,2.85)$)
    [fill=white,opacity=0.75,text width=1.5cm]
    {``Naive'' potential};
}
\end{tikzpicture}
#+END_CENTER
*** QBX: An Experiment

\begin{center}
\begin{tabular}{p{0.45\textwidth}p{0.45\textwidth}}
\uncover<+->{%
    \tikz \node[rotate=90]{$p=3$, $N=80$};
    \includegraphics[width=0.38\textwidth]{./media/locexp-ball-order3-crop.pdf}
}
&
\uncover<+->{%
    \tikz \node[rotate=90]{$p=6$, $N=80$};
    \includegraphics[width=0.38\textwidth]{./media/locexp-ball-order6-crop.pdf}
} \\
\uncover<+->{%
    \tikz \node[rotate=90]{$p=12$, $N=80$};
    \includegraphics[width=0.38\textwidth]{./media/locexp-ball-order12-bad-crop.pdf}
}
&
\uncover<+->{%
    \tikz \node[rotate=90]{$p=12$, $N=240$};
    \includegraphics[width=0.38\textwidth]{./media/locexp-ball-order12-good-crop.pdf}
}
\end{tabular}
\end{center}
*** QBX: Notation, Basics

\Large Graf's addition theorem

\bigskip
#+BEGIN_CENTER
\begin{tikzpicture}[scale=1.3]
  \coordinate (c) at (0,0) ;
  \path (c) ++(45+75:2.25) coordinate (s);
  \path (c) ++(45+15:1.55) coordinate (t);
  \path (c) ++(45+15:2) coordinate (t-to-curve);


  \path (s) ++(-1.5,0) coordinate (curve-before);
  \path (t) ++(1,-0.3) coordinate (curve-after);
  \draw [thick]
    (curve-before)
    ..controls +(30:0.7) and +(180-45:0.7) ..
    (s)
    node [pos=0.2,anchor=north] {$\Gamma$}
    ..controls +(-45:0.7) and +(45+90+15:1.25) ..
    (t-to-curve)
    ..controls +(45+270+15:0.3) and +(160:0.3) ..
    (curve-after) ;

  \fill (c) circle (1pt);
  \fill (t) circle (1pt);
  \fill (s) circle (1pt);

  \draw [dotted] ++(-20:1.55) arc (-20:45+105:1.55);

  \node at (85:1.55) [fill=white] {$\rho$};
  \draw (c) -- (t) ;
  \draw (c) -- (s) ;
  \node at (c) [anchor=north west] {$c$};
  \node at (s) [anchor=south west] {$x'$};
  \node at (t) [anchor=west,fill=white,xshift=0.5mm,inner sep=1mm] {$x$};
  \draw [dashed,->] (c) ++(-1, 0) -- ++(3.5,0);

  \draw (c) ++(0.5,0) arc (0:45+75:0.5);
  \path (c) ++(45*0.8+75*0.8:0.5)
    node [anchor=south] {$\theta'$};

  \draw (c) ++(0.75,0) arc (0:45+15:0.75) ;
  \path (c) ++(45/2+15/2:0.75)
    node [anchor=west] {$\theta$};

  \draw [very thin] (t) -- (t-to-curve);
  \draw [very thin]
    let
      \p1 = ($ 0.1*(t-to-curve) - 0.1*(c) $),
      \p2 = (-\y1,\x1)
    in
    ($(t-to-curve)!0.1!(c)$) -- ++(\p2) -- ++(\p1) ;

\end{tikzpicture}
#+END_CENTER
\pause
\[
  H^{(1)}_0(k|x-x'|) = \sum_{l=-\infty}^\infty H^{(1)}_l(k|x'-c|) e^{i l \theta'}
  J_l(k |x-c|) e^{-i l \theta}
\]
#+BEGIN_EXPORT latex
\uncover<+->{
  \begin{tikzpicture} [overlay]
    \node [below left=7mm of current page.north east,
    draw,drop shadow,fill=white,inner sep=5mm,thick]
      {
        Requires: $|x-c| < |x'-c|$ (``local expansion'')
      } ;
  \end{tikzpicture}
}
#+END_EXPORT

*** QBX: Formulation, Discretization

Compute layer potential on the disk as
\[
  S_k\sigma(x) = \sum_{l=-\alt<2->{p}{\infty}}^{\alt<2->{p}{\infty}} \alpha_l J_l(k \rho) e^{-i l \theta}
\]
with
\[
  \alpha_l = \frac{i}{4} \,
  \only<2->{T_N\bigg(}
  \int_\Gamma H^{(1)}_l(k|x'-c|) e^{i l \theta'} \sigma(x') \intd x'
  \only<2->{\bigg)}
  \quad
  (l = -\infty, \ldots, \infty)
\]
$S\sigma$ is a smooth function \emph{up to $\Gamma$}.

*** Quadrature by Expansion (QBX)

#+BEGIN_CENTER
#+ATTR_LATEX: :height 4.5cm
[[./media/locexp-ball-order3-crop.pdf]]
#+END_CENTER

\begin{equation*}
  \text{Error}
  \le
  \Bigg(
    C
    \underbrace{
      r^{p+1}
    }_{\text{Truncation error}}
    +
    C
    \underbrace{
       \left( \frac{h}{r} \right)^{q}
    }_{\text{Quadrature error}}
  \Bigg)
  \|\sigma\|
\end{equation*}

[K, Barnett, Greengard, O'Neil JCP `13]
*** Achieving high order
    
\begin{equation*}
\text{Error} \le \Bigg(C \underbrace{r^{p+1}}_{\text{Truncation error}}
+ C \underbrace{\left( \frac{h}{r} \right)^{q}}_{\text{Quadrature error}} \Bigg) \|\sigma\|
\end{equation*}

Two approaches:

- \emph{Asymptotically convergent}: \(r=\sqrt h\)
  - \plusball Error \(\to 0\) as \(h\to 0\)
  - \minusball Low order: \(h^{(p+1)/2}\)
- /Convergent with controlled precision/: $r=5h$
  - \minusball Error \(\not\to\) 0 as \(h\to 0\)
  - \plusball High order: \(h^{p+1}\) to controlled precision \(\epsilon:=(1/5)^q\)
        
*** Other layer potentials
Can't just do single-layer potentials:
\[\alpha^D_l = \frac{i}{4} \,
  \int_\Gamma \frac{\partial}{\partial \hat n_{x'}} H^{(1)}_l(k|x'-c|) e^{i l \theta'} \mu(x') \intd x'. \]
Even easier for target derivatives ($S'$ et al.): *Take derivative of local expansion.*

*Analysis says:* Will lose an order.

*Slight issue:* QBX computes one-sided limits.

Fortunately: Jump relations are known--e.g.
\[
  (PV) D^* \mu(x)|_\Gamma  = \lim_{x^\pm \rightarrow x}  D\mu(x^\pm) \mp \frac{1}{2} \mu(x).
\]

/Alternative:/ Two-sided average \(\to\) Preferred because of conditioning
  
*** Understanding Truncation Behavior

Let $\Gamma=\partial\Omega^-$ be piecewise $C^2$ with no inward
facing cusps.  Let $\Psi$ be the exterior Riemann map  that maps
the exterior $\Omega^+$ onto the exterior of the unit disk.

\begin{theorem}[A basis of QBX-exact densities]
  A function on the interior $f : \Omega^- \to \mathbb{R}$ is a harmonic polynomial of degree $n$ if and only if
  $f$ has the representation $f = D \varphi$ and the associated double-layer
  density function $\varphi$ takes the form
  %
  \begin{equation*}
    \varphi(z) = \sum_{k = 0}^n \lambda_k \cos(k \theta(z) + \mu_k), \quad
    z \in \Gamma
  \end{equation*}
  %
  for some set of real coefficients $\lambda_k$, $\mu_k$, where
  $\theta(w)=\arg \Psi(w)$ is the \emph{boundary correspondence}.
\end{theorem}
\bigskip
[Wala, K `18]

*** QBX and Conformal Mapping

#+BEGIN_EXPORT latex
\begin{algorithmic}
  \REQUIRE{A smooth Jordan boundary $\Gamma$, with $0$ in the interior.}
  \REQUIRE{A boundary sign $s$: $+1$ for exterior, $-1$ for interior.}
  \ENSURE{Computes the boundary correspondence $\theta$.}
  \STATE{\textsc{Stage 1}}
  \STATE{
    Solve the following integral equation for the density $\sigma$, for all $\zeta \in \Gamma$:
    \[
    \begin{dcases}
      \zeta = \left( \mathcal{D} - \frac{1}{2} \right) \sigma(\zeta) & \text{ if } s = +1 \\
      \overline{\zeta^{-1}} = \left( \mathcal{D} + \int + \frac{1}{2} \right) \sigma(\zeta) & \text{ if } s = -1. \\
    \end{dcases}
    \]}
  \STATE{\textsc{Stage 2}}
  \STATE{Let $\displaystyle \tilde{\sigma}(\zeta) = \sigma(\zeta) + \frac{s}{2 \pi i} \int_\Gamma \frac{\sigma(y)}{y} \, dy$
  ($\zeta\in\Gamma$).}
  \STATE{\textsc{Stage 3}}
  \STATE{Let $\displaystyle \theta(\zeta) =  \arg \left(-s \frac{\tilde{\sigma}(\zeta)}{|\tilde{\sigma}(\zeta)|} \right)$
  ($\zeta\in\Gamma$).}
\end{algorithmic}
#+END_EXPORT

** QBX Acceleration
*** Local QBX: Viewing QBX as a Local Correction

What happens if one attempts to use QBX quadrature as a 'local correction'?
#+LATEX: \begin{hidden}[6cm]
#+LATEX: \end{hidden}

*** QBX + FMM : A straightforward coupling

#+BEGIN_CENTER
#+BEGIN_EXPORT latex
\begin{tikzpicture}[scale=0.5]
  \draw [thick] (-2,1) coordinate (boxll) rectangle ++(7,7) ++(-3.5,-3.5)
    coordinate (boxcenter) ;

  \coordinate (qbxcenter) at ($ (boxcenter) + (-0.8,-2.1)$);

  \fill (boxcenter) circle (2pt);

  \def\qbxrad{1.9}

  \def\endpoint{5.4,-1.1}
  \def\basepoint{1.9,-0.1}
  \draw [thick]  plot[smooth, tension=1] coordinates {(-2,-2) (\basepoint) (\endpoint)};
  \coordinate (basepoint) at (\basepoint) ;
  \coordinate (endpoint) at (\endpoint) ;

  %\uncover<2->{
    \draw [dashed]
      let
        \p1 = ($(boxcenter)-(boxll)$),
        \n2 = {veclen(\x1,\y1)}
      in
        (boxcenter) circle (\n2)
        coordinate (circexplabel) at ($ (boxcenter) + (30:\n2) $);
  %}
  %\uncover<3->{
    \draw [thick,->,shorten >=1mm, shorten <=1mm] (boxcenter) -- (qbxcenter);
    \fill (qbxcenter) circle (2pt);
  %}
  %\uncover<4->{
    \fill [opacity=0.2] (qbxcenter) circle (\qbxrad);
    \draw (qbxcenter) circle (\qbxrad)
      coordinate (qbxexplabel) at ($(qbxcenter) + (140:\qbxrad) $);

    \node [right=3mm of circexplabel, text width=2.5cm]
      {Box local expansion};
    \draw [<-,shorten <=3mm] (qbxcenter) -- ++(-4,-1)
      node [anchor=east] {QBX center};

    \draw [<-,shorten <=1mm] (qbxexplabel) -- ++(-1,1)
      node [anchor=east,text width=2cm]
      {QBX expansion};
  %}

  \node [above=3mm of endpoint] {$\Gamma$};
\end{tikzpicture}
#+END_EXPORT
#+END_CENTER

*** Accuracy vs FMM/QBX orders: Straightforward (2D)

#+BEGIN_EXPORT latex

{
  \sisetup{detect-weight,mode=text,output-exponent-marker=\text{e}}
  \small
  \begin{tabular}{cccccc}
    \toprule
    $(1/2)^{\pfmm+1}$ & $\pfmm$ & $\pqbx=3$ & $\pqbx=5$ & $\pqbx=7$ & $\pqbx=9$\\
    \midrule
    0 & (direct) & \num{4.35e-06} & \num{6.21e-07} & \num{1.05e-07} & \num{5.71e-08}\\
    \num{6e-02} & 3 & \num{2.55e-02} & \num{2.96e-02} & \num{4.07e-02} & \num{5.77e-02}\\
    \num{2e-02} & 5 & \num{6.94e-03} & \num{1.61e-02} & \num{2.29e-02} & \num{3.10e-02}\\
    \num{5e-04} & 10 & \num{4.95e-04} & \num{1.75e-03} & \num{5.80e-03} & \num{9.48e-03}\\
    \num{2e-05} & 15 & \num{1.58e-05} & \num{1.85e-04} & \num{6.40e-04} & \num{3.17e-03}\\
    \num{5e-07} & 20 & \converged{\num{4.35e-06}} & \num{1.31e-05} & \num{8.99e-05} & \num{5.01e-04}\\
    \bottomrule
  \end{tabular}
}
#+END_EXPORT

\bigskip
$\ell^\infty$ error in Green's formula $\mathcal S(\partial_n
u)-\mathcal D(u)=u/2$, scaled by $1/\|u\|_\infty$, for the
65-armed starfish
$\gamma_{65}$, using the conventional QBX FMM algorithm.

$3250$ Gauss-Legendre panels, with $33$ nodes per panel.
*** Recap: Local Expansions of Potentials

\begin{center}
  \begin{tikzpicture}
    \fill (0,0) coordinate(source) circle (2pt) node [anchor=east] {$s$};
    \draw (3,-1) coordinate(center) circle (2cm)  node [anchor=west] {$c$};
    \fill (center) circle (2pt);
    \fill ($ (center) + (0,2) $) coordinate (target)  circle (2pt) node [anchor=south] {$t$};
    \draw [thick,->,shorten <=3mm, shorten >=3mm,purple] (source) -- (center);
    \draw [thick,->,shorten <=3mm, shorten >=3mm,cyan] (center) -- (target);
  \end{tikzpicture}
\end{center}
\[
  \text{Truncation Error}
  \sim
  \left(\frac{\text{\color{cyan}furthest target}}{\text{\color{purple}closest source}}\right)^{p+1}
\]

*** QBX + FMM: Sources of Inaccuracy

#+BEGIN_CENTER
\input{./media/list2.pgf}
#+END_CENTER

*** Possible Expansion Sequences

- Source $\to$ Multipole($p$) $\to$ QBX-Local($q$)
- Source $\to$ Local($p$) $\to$ QBX-Local($q$)
- Source $\to$ Multipole($p$) $\to$ Local($p$) $\to$ QBX-Local($q$)

*** Translation chains for QBX
#+BEGIN_CENTER
\input{./media/list2-less-bump.pgf}%
#+END_CENTER

*** Translation chains for QBX
#+BEGIN_CENTER
\input{./media/list2-bump.pgf}%
#+END_CENTER

*** Expansions of Expansions?

\begin{center}
  \begin{tikzpicture}
    \fill (0,0) coordinate(source) circle (2pt) node [anchor=east] {$s$};
    \draw (3,-1) coordinate(center) circle (2cm)  node [anchor=west] {$c$};
    \fill (center) circle (2pt);
    \fill ($ (center) + (0,2) $) coordinate (target)  circle (2pt) node [anchor=south] {$t$};
    \draw [thick,->,shorten <=3mm, shorten >=3mm,purple] (source) -- (center);
    \draw [thick,->,shorten <=3mm, shorten >=3mm,cyan] (center) -- (target);
  \end{tikzpicture}
\end{center}
\[
  \text{Truncation Error}
  \sim
  \left(\frac{\text{\color{cyan}furthest target}}{\text{\color{purple}closest source}}\right)^{p+1}
\]

This holds for point evaluations of a single expansion.

**Question:** Can we generalize it to hold when forming /expansions of expansions/?

*** Example: Local($p$) $\to$ Local($q$) Truncation Error (2D Lap.)

\small
\begin{lemma}
%[Truncating a mediating local to $p$-th order on a $q$-th order local]%
\label{lem:l2qbxl}
Let $c, r > 0$. Suppose that a single unit strength charge is placed at $z_0$,
with $|z_0| \geq (c + 1)r$.
%Consider the closed disk $\overline B(0,r)$ of radius $r$
%centered at the origin.
Suppose that $y, z \in \overline B(0,r)$.  If $|z| < r$ and $|y -
z| \leq r - |z|$, the potential $\phi$ due to the charge is described by a power
series
%
$ \phi(y) = \sum_{l=0}^{\infty} \beta_l {(y-z)}^l$.
%
Fix the intermediate local order $p \ge 0$.  For $n \geq 0$, let
% $\tilde{\beta}_n$ be the $n$-th coefficient of a local expansion centered at $z$
% obtained by translating a $p$-th order local expansion of $\phi$ centered at the
% origin:
%
\[
\tilde{\beta}_n = \frac{1}{n!} \frac{d^n}{dz^n}
\left(\sum_{k=0}^{p} \frac{\phi^{(k)}(0)}{k!} z^k \right).
\]
%
Fix the local expansion order $q \geq 0$. Define $\alpha = 1/(1 + c)$. Then
%
\[
\left| \sum_{k=0}^q \beta_k{(y-z)}^k
- \sum_{k=0}^q \tilde{\beta}_k {(y-z)}^k \right|
\leq \left(\frac{q+1}{p+1}\right)
\left( \frac{\alpha^{p+1}}{1 - \alpha} \right).
\]
\end{lemma}
[Wala, K `18a -- \arxiv{1801.04070}]
#+BEGIN_EXPORT latex
\uncover<+->{}
\uncover<+->{%
  \begin{tikzpicture} [overlay]
    \node [above left=7mm of current page.south east,
    draw,drop shadow,fill=white,inner sep=5mm,thick]
      {%
        \includegraphics[width=5cm]{./media/l2qbxl-crop.pdf}
      } ;
  \end{tikzpicture}
}
\uncover<+->{%
  \begin{tikzpicture} [overlay]
    \node [above right=10mm of current page.south west,
    draw,drop shadow,fill=white,inner sep=5mm,thick,
    text width=0.6\textwidth]
      {%
        Slightly more subtle, but essentially confirms
        \[
          \text{Truncation Error}
          \sim
          \left(\frac{\text{\color{cyan}furthest target}}{\text{\color{purple}closest source}}\right)^{p+1}.
        \]
      } ;
  \end{tikzpicture}
}
#+END_EXPORT

*** A Glimpse of Expansion Technology

**** Description
     :PROPERTIES:
     :BEAMER_col: 0.6
     :END:
- M/L expansions typically work by *separation of variables*
  - In angular + radial coordinates
- Basis for capturing the angular dependency in 3D?
- Known: Expanded potential solves PDE
- So: Expansion fully specified if known on surface of sphere
  - (Interior Dirichlet BVP, e.g.)
  - Radial dependency: find ODE, straightforward to evaluate

**** Ymn pic
     :PROPERTIES:
     :BEAMER_col: 0.4
     :END:

#+ATTR_LATEX: :width \textwidth
[[./media/spherical-harmonic.png]]

*** Expansions on the Surface of a Sphere

- Generalizing to $n$ dimensions: (we care about $d=2,3$)

  \(\mathbb{S}^{d-1} = \{\bm{x} \in \mathbb{R}^d: \norm{\bm{x}} = 1 \}\)

- A polynomial $p: \mathbb{R}^d \to \mathbb{C}$ is \emph{homogeneous} of degree $k$ if
  $p$ if $p$ satisfies $p(r \bm{x}) = r^k p(\bm{x})$ for all $\bm{x} \in \mathbb{R}^d$.
- *Space of spherical harmonics \(\mathbb{Y}_n^d\):* restrictions to the unit sphere
  $\mathbb{S}^{d-1}$ of the harmonic (\(\triangle p=0\)), homogeneous polynomials of degree $n$.
# TODO: Define F-L series
- *Fourier-Laplace series:*
  \begin{equation*}
    \fourier{p} f(\bm{\xi}) = \sum_{n=0}^p \proj{n} f (\bm{\xi}),
    \quad \bm{\xi} \in \mathbb{S}^{d - 1},
  \end{equation*}
  where \(\proj{n}[\cdot]\) is an orthogonal projection onto \(\mathbb{Y}_n^d\).
*** Convergence of Fourier-Laplace Series

\begin{proposition}[Norm of the Fourier-Laplace partial sum]%
  \label{prop:fourier-laplace-norm}
  Let $f \in C(\mathbb{S}^{d-1})$. Then a constant $\lebesgueconst{n}{d} > 0$
  exists such that
  %
  \[
    \bignorm{\fourier{p} f}_\infty
    \leq \lebesgueconst{p}{d} \bignorm{f}_\infty,
  \]
  %
  where, in dimensions $d = 2$ and $d = 3$,
  %
  \begin{align*}
     \lebesgueconst{p}{2} &= \frac{4}{\pi^2} \log{p} + O(1), \\
     \lebesgueconst{p}{3} &= 2 \sqrt{\frac{2p}{\pi}} + o(\sqrt{p}),
  \end{align*}
  %
  asymptotically as $p \to \infty$.
\end{proposition}

[Rivlin `69], [Gronwall 1911]
*** Expansions of Expansions: M2QBXL

#+BEGIN_CENTER
\begin{tikzpicture}[scale=1.25]
  % B(0,R) (disk containing local expansion)
  \node [disk, minimum size=3cm](Gamma) at (0,0) {};
  \node [below right] at (Gamma.south east) {$\overline {B(\bm{0},R)}$};
  \draw [fill] (Gamma) circle (1pt) node [right] {$\bm{0}$};
  \draw [<->] (Gamma.center) -- (Gamma.north) node [right, midway] {$R$};

  % local expansion inside B(0,R)
  \path (Gamma) ++ (-45:0.9cm) node [disk, color=qbxcolor, minimum size=1cm](lexp) {};
  \path (lexp) ++ (20:0.5cm) node (y) {};
  \draw [fill] (lexp) circle (1pt) node [below] {$\bm{c'}$};
  \draw [fill] (y) circle (1pt) node [right] {$\bm{t}$};

  % B(c,r) (multipole disk)
  \path (Gamma) ++ (-4cm, 0) node [disk, color=srccolor, minimum size=1.5cm](gamma) {};
  \node [below right] at (gamma.south east) {$\overline {B(\bm{c}, r)}$};
  \path (gamma) ++(155:0.75cm) coordinate (source);
  \draw [fill] (gamma) circle (1pt) node [below] {$\bm{c}$};
  \draw [fill] (source) circle (1pt) node [left] {$\bm{s}$};

  \draw [color=srccolor]
      (source)
      edge [bend left, dashed, ->]
      node [midway, above, yshift=0.5cm] {$\mpole{\bm{c}}{p}[\pot{\bm{s}}]$}
      (gamma.center);

  \draw [<->] (gamma.center) -- (Gamma) node [below, midway] {$\rho$};
  \draw [<->] (gamma.center) -- (gamma.north)
      node [right, midway] {$r$};

  \draw [color=qbxcolor]
      (gamma.center)
      edge [bend left, dashed, ->]
      node [midway, above, xshift=-0.8cm]
      {$\local{\bm{c'}}{q}[\mpole{\bm{c}}{p}[\pot{\bm{s}}]]$}
      (lexp.center);

  \draw (source)
      edge [bend right=65, dashed, ->]
      node [midway, below] {$\local{\bm{c'}}{q}[\pot{\bm{s}}]$}
      (lexp.center);
\end{tikzpicture}
#+END_CENTER

*** Analyzing M2QBXL

\begin{lemma}[Source $\to$ Multipole($p$) $\to$ Local($q$)]
  Let $R > 0$ and $\rho > r > 0$.  Consider a closed ball of radius $r$ centered
  at $\bm{c}$, with $\norm{\bm{c}} = R + \rho$, containing a unit-strength
  source $\bm{s}$.  Also, let a ball of radius $R$ centered at the origin
  contain points $t$ and $\bm{c'}$ satisfying $\norm{\bm{c}} \leq R$ and
  $\norm{\bm{t} - \bm{c'}} \leq R - \norm{\bm{c'}}$.

  \medskip
  Then, in the situation of the previous slide:
  \begin{equation*}%
    \label{eqn:m2qbxl}
    \left|
    \local{\bm{c'}}{q}[\pot{\bm{s}}](\bm{t})
    - \local{\bm{c'}}{q}[\mpole{\bm{c}}{p}[\pot{\bm{s}}]](\bm{t}) \right|
    \leq
    \lebesgueconst{q}{d} \bignorm{
      \left. \left( \pot{\bm{s}} - \mpole{\bm{c}}{p}[\pot{\bm{s}}] \right) \right|_{\overline{B(\bm{0}, R)}}
      }_\infty.
  \end{equation*}
\end{lemma}

\medskip
[Wala-K `19---in prep.]

*** Translation Chains for QBX

Rigorous truncation error bounds for local expansions for scenarios
QBX locals /near/ box locals:

#+BEGIN_CENTER
\input{./media/list2-zero-stickout.pgf}
#+END_CENTER

*** Targets with Extent: Target Confinement Regions

\begin{tikzpicture}[scale=1.3,baseline={(0,0)}]
  \def\sout{0.25}
  \def\soutrad{1.414+\sout*1.414}
  \def\pr{0.55}

  \draw (-1,-1) rectangle (1,1);
  \draw [dashed] (0,0) circle (\soutrad);

  \node [anchor=south] at (0,-1) {Box};
  \node [anchor=north] at (0,-1-\sout) {Target Confinement Region };
  \draw [|<->|] (0,0) -- (1+\sout,1+\sout) node [pos=0.5, anchor=west] {$(1+t_f)R$};
  \draw [|<->|] (0,0) -- (-1,0) node [pos=0.5, anchor=north] {$R$};

  \coordinate (pc) at (-0.9, 1.1);
  \fill [red] (pc) circle (1pt);
  \draw [red] (pc) circle (\pr);
  \draw [red,|<->|] (pc) -- ++(0,-\pr) node [pos=0.5,anchor=east] {$r$};

  \node [anchor=north] at (0,-2.25) {QBX center `\emph{not in}' box};
\end{tikzpicture}
\hfill
\begin{tikzpicture}[scale=1.3,baseline={(0,0)}]
  \def\sout{0.25}
  \def\soutrad{1.414+\sout*1.414}
  \def\pr{0.55}

  \draw (-1,-1) rectangle (1,1);
  \draw [dashed] (0, 0) circle (\soutrad);

  \node [anchor=south] at (0,-1) {Box};
  \node [anchor=north] at (0,-1-\sout) {Target Confinement region };
  \draw [|<->|] (0,0) -- (1+\sout,1+\sout) node [pos=0.5, anchor=west] {$(1+t_f)R$};
  \draw [|<->|] (0,0) -- (-1,0) node [pos=0.5, anchor=north] {$R$};

  \coordinate (pc) at (-0.45, 0.7);
  \fill [green] (pc) circle (1pt);
  \draw [green] (pc) circle (\pr);
  \draw [green,|<->|] (pc) -- ++(0,-\pr) node [pos=0.5,anchor=east] {$r$};

  \node [anchor=north] at (0,-2.25) {QBX center `\emph{in}' box};
\end{tikzpicture}

*** M2L Convergence Factor with 2-Away, TCF (3D)
#+BEGIN_CENTER
\begin{tikzpicture}[scale=0.9, z={(-0.09,-0.09)}]

  \def\tcf{0.5}

  \DrawCube[cubecolor]{0}{0}{0}{1}
  \DrawCube[cubecolor]{-6}{0}{0}{1}

  % intervening cubes
  \begin{scope}[canvas is xz plane at y=-1]
    \draw[cubecolor, dotted] (-1,1) -- +(-4,0);
    \draw[cubecolor, dotted] (-1,-1) -- +(-4,0);
  \end{scope}

  \begin{scope}[canvas is xz plane at y=1]
    \draw[cubecolor, dotted] (-1,1) -- +(-4,0);
    \draw[cubecolor, dotted] (-1,-1) -- +(-4,0);
  \end{scope}

  \begin{scope}[canvas is yz plane at x=-3]
    \draw[cubecolor, dotted] (-1,-1) rectangle (1,1);
  \end{scope}

  \DrawSphere[localcolor]{0}{0}{0}{sqrt 3*(1+\tcf)}
  \DrawSphere[srccolor]{-6}{0}{0}{sqrt 3}

  \draw[<->, length]
    (0,0) --
    (xyz cs: x=-1-\tcf, y=1+\tcf, z=1+\tcf)
    node [right, xshift=0.2cm] {$\sqrt{3} (1 + t_f) r$};

  \draw[<->, length]
    (0,0) --
    (xyz cs: y=-1)
    node [right, midway] {$r$};

  \coordinate[mark coordinate] (t) at (0, 0);
  \coordinate[mark coordinate] (s) at (xyz cs: x=-6);

  \draw[<->, length] (t) -- (s) node[below, midway, xshift=-0.9cm] {$6r$};

  \draw[<->, length] (s) -- +(xyz cs: x=1, y=1, z=1)
    node [left, midway] {$\sqrt{3}r$};
\end{tikzpicture}

3D, $t_f=0.9$: Conv. factor $\approx 0.77$
#+END_CENTER

*** GIGAQBX Fast Algorithm: End-to-End Accuracy (2D/3D)

\begin{theorem}[GIGAQBX~FMM for Laplace (2D/3D)]%
  Let the center $\bm{c}$ be owned by the box $b$ and let $\bm{t}$ be a target
  associated with the center $\bm{c}$.  Assuming that $0 \leq t_f \leq
  6/\sqrt{d} - 2$, and defining the constants
  %
  \[
    \omega =  \frac{\sqrt{d}(1+t_f)}{6 - \sqrt{d}},
    \quad
    A = \sum_{i=1}^{N_S} \left| w_i \right|,
  \]
  %
  and letting $D$ be the minimum box width in the tree, the (absolute)
  acceleration error in the GIGAQBX FMM is bounded as follows:
  %
  {\tiny
  \begin{equation*}
    \label{eqn:gigaqbx-bound}
    \norm{
      \local{\bm{c}}{q}[\ptpot](\bm{t})
      -
      G_{\bm{c}}^{p,q}[\ptpot](\bm{t})
    }
    \leq
    %
    \begin{cases}
      A \lebesgueconst{q}{2}
      \max
      \left(
      \frac{1}{1-\frac{\sqrt{2}}{3}} \left(\frac{\sqrt{2}}{3}\right)^{p+1},
      \frac{1+\lebesgueconst{p}{2}}{1-\omega} \omega^{p+1}
      \right),
      & d = 2, \\
      \frac{A \lebesgueconst{q}{3}}{D}
      \max
      \left(
      \frac{1}{3 - \sqrt{3}} \left( \frac{\sqrt{3}}{3} \right)^{p+1},
      \frac{1 + \lebesgueconst{p}{3}}{6 - 2\sqrt{3} - \sqrt{3}t_f} \omega^{p+1}
      \right),
      & d = 3.
    \end{cases}
  \end{equation*}
  }
\end{theorem}

\medskip
[Wala-K `19---in prep.]

#+BEGIN_EXPORT latex
\uncover<+->{}
\uncover<+->{%
  \begin{tikzpicture} [overlay]
    \node [above left=7mm of current page.south east,
    draw,drop shadow,fill=white,inner sep=5mm,thick,
    text width=0.5\textwidth]
      {%
        ``GIGAQBX'':
        \begin{itemize}
        \item Consider \textbf{sized targets} (QBX expansions)
        \item Introduce a \textbf{Target Confinement Rule}
        \item Some M2P and P2L must be direct
        \item \textbf{Targets in Non-Leaf Boxes}
        \item \textbf{Two-Box Separation}
        \end{itemize}
      } ;
  \end{tikzpicture}
}
#+END_EXPORT

*** Interaction Lists

#+BEGIN_CENTER
#+ATTR_LATEX: :height 7cm
[[./media/vol-interaction-lists.pdf]]
#+END_CENTER
*** Complexity (3D, Point-and-Shoot)

\def\EtoEcost{\pfmm^3}
\def\PtoEcost{\pfmm^2}
\def\PtoPcost{\pqbx^2}

| Modeled Operation Count                             | What                  |
|-----------------------------------------------------+-----------------------|
| $NL$                                                | Build tree            |
| $N_S \PtoEcost + N_B \EtoEcost$                     | Form M, Upward pass   |
| $(27 (N_C + N_S) \nmax + N_C M_C) \PtoPcost$        | List 1: P2QBXL        |
| $875 N_B \EtoEcost$                                 | List 2: M2L           |
| $N_C M_C q^2 + 124L N_S \nmax \PtoPcost$            | List 3: P2QBXL+M2QBXL |
| $375 N_B \nmax \PtoEcost + 250 N_C \nmax \PtoPcost$ | List 4: P2QBXL+P2L    |
| $8 N_B \EtoEcost$                                   | Downward              |
| $N_C \EtoEcost$                                     | L2QBXL                |
| $N_T \PtoPcost$                                     | QBXL2P                |

*** Complexity (3D)

\begin{theorem}%
  Assume that $\pfmm = O(\lvert \log \epsilon \rvert)$, and that $\pqbx
  \le \pfmm$. For a fixed value of $\nmax$,
  using a level-restricted octree and with $t_f < \sqrt{3} - 1$,
  the cost in modeled flops of the evaluation stage of the
  GIGAQBX~FMM is
  \[
    O((N_C + N_S + N_B) \lvert \log \epsilon \rvert^3
    + N_C M_C \lvert \log \epsilon \rvert^2
    + N_T \lvert \log \epsilon \rvert^2).
  \]
  Assuming that the particle distribution satisfies $N_B = O(N)$ and $M_C =
  O(1)$, the worst-case modeled cost using a level-restricted octree
  and~$t_f < \sqrt{3} - 1$ is linear~in~$N$.
\end{theorem}

[Wala-K `18]

*** Curve Interaction Lists

#+BEGIN_CENTER
#+ATTR_LATEX: :height 7cm
[[./media/curve-interaction-lists.pdf]]
#+END_CENTER

** Reducing Complexity through better Expansions
*** Spherical Harmonic Expansions: Notation
**** Notation
     :PROPERTIES:
     :BEAMER_col: 0.5
     :END:
- $s$: source point
- $t$: target point
- $c$: expansion center
- $a = t - c$
- $b = s - c$
- $\gamma$: angle between $a$ and $b$
- $p$: expansion order

**** Figure
     :PROPERTIES:
     :BEAMER_col: 0.5
     :END:
\begin{tikzpicture}[z={(-0.09,-0.09)},scale=1.5]
  \DrawSphere[qbxcolor]{0}{0}{0}{1}
  \coordinate[mark coordinate] (c) at (0,0,0);
  \coordinate[mark coordinate] (s) at (-1.5,-0.75,1);
  \def\tc{1/sqrt 3}
  \coordinate[mark coordinate] (t) at (-\tc,\tc,\tc);
  \draw[<-, label] (s) -- (c) node[below,midway] {$b$};
  \draw[->, label] (c) -- (t) node[above,midway,xshift=2pt] {$a$};
  \draw[label] (c) node[right] {$c$};
  \draw[label] (s) node[left] {$s$};
  \draw[label] (t) node[left] {$t$};
  % s = np.array((-1.5,-0.75,1))
  % tc = 1/3**0.5
  % t = np.array((-tc,tc,tc))
  % y = (s - t.dot(s)*t) / la.norm(s - t.dot(s)*t)
  % theta = 360/(2*3.1416) * np.arccos(t.dot(s) / la.norm(s))
  \begin{scope}[canvas is plane={O(0,0,0)x(-\tc,\tc,\tc)y(-0.549,-0.798,0.249)}]
    \draw[label] (0.4, 0) arc (0:58.83:0.4) node[midway, left] {$\gamma$};
  \end{scope}
\end{tikzpicture}
*** Spherical Harmonic Expansions: Notation

Expansion of Laplace potential in 3D:

\[
  \frac{{(4\pi)}^{-1}}{\norm{a-b}} =
  \sum_{n=0}^\infty \frac{1}{2n + 1}
  \frac{\norm{a}^n}{\norm{b}^{n+1}} \sum_{m=-n}^n Y_n^m(\theta_a, \phi_a)
  Y_n^{-m}(\theta_b, \phi_b)
\]

Valid for $|a| < |b|$.

\bigskip
*Total cost:* $O({(p+1)}^2(N + M))$ (for $M$ targets, $N$ sources)
*** Spherical Harmonic Expansions: An Identity

By /Legendre addition theorem/

\begin{equation*}
  P_n(\cos \gamma) =
  \frac{1}{2n + 1}
  \sum_{m=-n}^n Y_n^m(\theta_a, \phi_a) Y_n^{-m}(\theta_b, \phi_b)
\end{equation*}

$P_n$ are Legendre polynomials

Results in line expansion (or `target-specific expansion'):
\begin{equation*}
  \frac{{(4\pi)}^{-1}}{\norm{a-b}} =
  \sum_{n=0}^\infty \frac{\norm{a}^n}{\norm{b}^{n+1}} P_n(\cos \gamma)
\end{equation*}

\bigskip
\textbf{Total cost:} $O((p+1)NM)$

\bigskip
First use in `local' QBX: [Siegel, Tornberg '17]

*Downside:* Sources/targets no longer separated.

*** Details

- *QBX [K et al `13]:* Unifies toolset for quad. and accel.
- *QBX FMM [Rachh et al `16]:* Geometry proc., first fast alg.
- *Truncation Result [Wala, K `18]:* Exact density basis
- *GIGAQBX 2D [Wala, K `18]:* Guaranteed-Accuracy Accel.
- *GIGAQBX 3D [Wala, K `18]:* \(\ell^2\) TC, improved geom. proc.
- *GIGAQBX-TS [Wala, K `19]:* Reduce accel. cost
- *Fourier-Laplace bounds [Wala, K `19--in prep.]:*

  2D/3D analysis

** Results: Layer Potentials
*** Layer Potentials: Accuracy (2D GIGAQBX)

# warning: functional blank line
#+BEGIN_EXPORT latex

{%
  \sisetup{detect-weight,mode=text,output-exponent-marker=\text{e}}
  \small
  \begin{tabular}{cccccc}
    \toprule
    $(1/2)^{\pfmm+1}$ & $\pfmm$ & $\pqbx=3$ & $\pqbx=5$ & $\pqbx=7$ & $\pqbx=9$\\
    \midrule
    0 & (direct) & \num{4.35e-06} & \num{6.21e-07} & \num{1.05e-07} & \num{5.71e-08}\\
    \num{6e-02} & 3 & \num{5.16e-03} & \num{6.35e-03} & \num{6.33e-03} & \num{6.34e-03}\\
    \num{2e-02} & 5 & \num{3.83e-04} & \num{5.95e-04} & \num{5.95e-04} & \num{5.93e-04}\\
    \num{5e-04} & 10 & \converged{\num{4.35e-06}} & \num{4.82e-06} & \num{6.94e-06} & \num{9.30e-06}\\
    \num{2e-05} & 15 & \converged{\num{4.35e-06}} & \converged{\num{6.21e-07}} & \converged{\num{1.05e-07}} & \num{1.76e-07}\\
    \num{5e-07} & 20 & \converged{\num{4.35e-06}} & \converged{\num{6.21e-07}} & \converged{\num{1.05e-07}} & \converged{\num{5.71e-08}}\\
    \bottomrule
  \end{tabular}
}
#+END_EXPORT

$\ell^\infty$ error in Green's formula $\mathcal S(\partial_n
u)-\mathcal D(u)=u/2$, scaled by $1/\|u\|_\infty$, for the
65-armed starfish
$\gamma_{65}$, using the GIGAQBX FMM algorithm.

$3250$ Gauss-Legendre panels, with $33$ nodes per panel.

*** Layer Potentials: Accuracy (2D Straightforward)

# warning: functional blank line
#+BEGIN_EXPORT latex

{
  \sisetup{detect-weight,mode=text,output-exponent-marker=\text{e}}
  \small
  \begin{tabular}{cccccc}
    \toprule
    $(1/2)^{\pfmm+1}$ & $\pfmm$ & $\pqbx=3$ & $\pqbx=5$ & $\pqbx=7$ & $\pqbx=9$\\
    \midrule
    0 & (direct) & \num{4.35e-06} & \num{6.21e-07} & \num{1.05e-07} & \num{5.71e-08}\\
    \num{6e-02} & 3 & \num{2.55e-02} & \num{2.96e-02} & \num{4.07e-02} & \num{5.77e-02}\\
    \num{2e-02} & 5 & \num{6.94e-03} & \num{1.61e-02} & \num{2.29e-02} & \num{3.10e-02}\\
    \num{5e-04} & 10 & \num{4.95e-04} & \num{1.75e-03} & \num{5.80e-03} & \num{9.48e-03}\\
    \num{2e-05} & 15 & \num{1.58e-05} & \num{1.85e-04} & \num{6.40e-04} & \num{3.17e-03}\\
    \num{5e-07} & 20 & \converged{\num{4.35e-06}} & \num{1.31e-05} & \num{8.99e-05} & \num{5.01e-04}\\
    \bottomrule
  \end{tabular}
}
#+END_EXPORT

$\ell^\infty$ error in Green's formula $\mathcal S(\partial_n
u)-\mathcal D(u)=u/2$, scaled by $1/\|u\|_\infty$, for the
65-armed starfish
$\gamma_{65}$, using the conventional QBX FMM algorithm.

$3250$ Gauss-Legendre panels, with $33$ nodes per panel.
*** Layer Potentials: Accuracy in 3D

#+BEGIN_EXPORT latex
\begin{center}

  \def\nmaxgigaqbx{512}
  \def\urchintestarms{8}
  \def\urchinquadorder{8}
  \def\urchinovsmp{5}
  \def\urchintotalquadorder{32}
  \def\urchingreentestnodesperelement{295}
  \def\urchingreentestnelementsone{48500}
  \def\urchingreentestnelementstwo{277712}
  \def\expnmax{512}

  \sisetup{%
    table-format = 1.2e-1,
    output-exponent-marker = \text{e},
    table-number-alignment = center,
    table-sign-exponent = true,
    table-column-width=4.5em,
    scientific-notation = true,
    round-mode = places,
    round-precision = 2,
    detect-weight = true,
    mode = text,
    table-parse-only=true,
  }
  \small
  \begin{tabular}{ScSSSS}
    \toprule
    {$(3/4)^{\pfmm+1}$} & {$\pfmm$} & {$\pqbx=3$} & {$\pqbx=5$} & {$\pqbx=7$} & {$\pqbx=9$}\\
    \midrule
    0.31640625           &     3 & 0.008291143698645285 & 0.00968404877505063 & 0.009151145525972935 & 0.00917659527996213 \\
    0.177978515625       &    5 & 0.0014306317807280255 & 0.0026739663967821794 & 0.0028549448708101214 & 0.0027803566630102924 \\
    0.04223513603210449  &     10 & \converged{6.076861932032935e-05} & 6.438110189318733e-05 & 0.0001270450424234368 & 0.0001466052625046138 \\
    0.010022595757618546 &   15 & \converged{6.0760217036864854e-05} & \converged{6.3807673565196985e-06} & 3.237558247597872e-06 & 7.07096870849642e-06 \\
    0.002378408954200495 &  20 & \converged{6.076021987995081e-05} & \converged{6.3807144934427785e-06} & 1.413902385243144e-06 & 2.509328730510307e-07 \\
    \bottomrule
  \end{tabular}

  \bigskip
  $\ell^\infty$ error in Green's formula $\mathcal S(\partial_n
  u)-\mathcal D(u)=u/2$, scaled by $1/\|u\|_\infty$, for the
  $\urchintestarms$-armed `urchin' geometry
  $\gamma_{\urchintestarms}$.

  Stage 1: $\urchingreentestnelementsone$ triangles,
  stage 2: $\urchingreentestnelementstwo$ triangles, with
  $\urchingreentestnodesperelement$ nodes per
  triangle.
\end{center}
\uncover<+->{}
\uncover<+->{%
  \begin{tikzpicture} [overlay]
    \node [below left=7mm of current page.north east,
    draw,drop shadow,fill=white,inner sep=5mm,thick,
    text width=0.5\textwidth]
      {%
        \includegraphics[width=\textwidth]{./media/urchin-8-sm.png}

        `Urchin' geometry $\gamma_8$, based on 8th order spherical
        harmonics
      } ;
  \end{tikzpicture}
}
#+END_EXPORT
*** Layer Potentials: (Somewhat) Complex Geometry

#+ATTR_LATEX: :width \textwidth
[[./media/plane-density.png]]

*** Cost Scaling: 3D GIGAQBX FMM

#+BEGIN_CENTER
#+ATTR_LATEX: :width \textwidth
[[./media/complexity-gigaqbx-3d.pdf]]

\bigskip\footnotesize
Modeled operation counts for the GIGAQBX FMM for $S\mu$.

\def\nmaxgigaqbx{512}
$\nmax = \nmaxgigaqbx$ and $t_f = 0.9$. Geometries:
$\gamma_2,\gamma_4,\dots,\gamma_{10}$.
#+END_CENTER

*** ``Balancing'' an FMM

#+BEGIN_CENTER
\input{./media/complexity-nmax-urchin-study.pgf}
#+END_CENTER

*** Line/Target-Specific Expansions: Cost Impact
**** Data
     :PROPERTIES:
     :BEAMER_col: 0.5
     :END:

- Operator: Single layer
- Orders: QBX: 9, FMM: 20 (9~digits)
- Points: 19M $\to$ 2.1M

**** Figure
     :PROPERTIES:
     :BEAMER_col: 0.5
     :END:

#+ATTR_LATEX: :height 2.5cm
[[./media/donuts.png]]
**** Clear
     :PROPERTIES:
     :BEAMER_env: ignoreheading
     :END:

\input{./media/timing-donut.pgf}

#+BEGIN_EXPORT latex
\uncover<+->{}
\uncover<+->{%
  \begin{tikzpicture} [overlay]
    \node [above left=7mm of current page.south east,
    draw,drop shadow,fill=white,inner sep=5mm,thick,
    text width=0.7\textwidth]
      {%
       $\nmax$: $96 \to 928$, $\nmpole$: $40 \to 380$

       Speedup: 3.3$\times$ [Wala-K `19]
      } ;
  \end{tikzpicture}
}
#+END_EXPORT
** Results: Poisson
*** Poisson: 3D, CAD Geometry

#+ATTR_LATEX: :height 7cm
[[./media/betterplane-poisson.png]]
Volume degree: 7 $\cdot$ Boundary degree: 6 $\cdot$ QBX order: 3

* Going General: More PDEs
  :PROPERTIES:
  :RELATE_TREE_SECTION_NAME: general
  :RELATE_TREE_SECTION_OPENED: true
  :END:

*** TODO Conformal mapping
*** Inhomogeneous Problems

*Example:* Poisson
\[\triangle u = f, \qquad u = g \text{ on \(\partial \Omega\)} . \]
Steps:

1. Solve the PDE (without the boundary condition) using the free-space
  Green's function \(G\):
  \[\tilde {u} = G \ast f, \]
  where `\(\ast\)' represents convolution.

1. Solve
  \[\triangle \hat {u} = 0, \qquad \hat {u} = g - \tilde {u} \text{ on \(\partial         \Omega\)} \]
  using a boundary integral equation.

1. Add
  \[u = \tilde {u} + \hat {u}, \]
  which solves the Poisson problem.

*** Eigenvalue Problems

*Example:* Solve
\[\triangle u = \lambda u. \]
Two options:

- Volume linear eigenvalue problem with Laplace kernel

- Surface nonlinear eigenvalue problem with Helmholtz kernel

*** Maxwell's equations

*Example:* Solve a scattering problem from a perfect electric
conductor.

Use /Vector Potential/ \(\vec{A}\) to represent magnetic field:
\[\vec{H} = \vec{\nabla } \times \vec{A}, \]
where
\[\triangle \vec{A} + k^2 \vec{A} = \vec{0} . \]
Since \(\vec{A}\) solves vector Helmholtz, simply represent as
\[\vec{A} (x) = S_k \vec{J}_s, \]
where \(\overrightarrow{J_{}}_s\) (physically) amounts to a surface
/current density/.

*** Maxwell's: Towards the MFIE
Then use

- the /continuity condition/
  \[\vec{n} \times [\vec{H}_{\tmop{tot}}] = \vec{J}_s, \]
- the /extinction theorem/ for perfect electrical conductors:
  \[\vec{H}_{\tmop{tot}}^- = \vec{0} \]
  inside the scatterer.

- the jump conditions

together to obtain the /Magnetic Field Integral Equation/ (/MFIE/):
\[\vec{n} \times \vec{H}_{\tmop{inc}}^+ = \frac{J_s}{2} - \vec{n} \times
   (\tmop{PV}) \vec{\nabla } \times S_k \vec{J}_s . \]

*** Stokes flow

(see project presentation)
